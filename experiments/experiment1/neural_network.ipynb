{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brent/anaconda/envs/stocks/lib/python3.6/site-packages/fix_yahoo_finance/__init__.py:43: DeprecationWarning: \n",
      "    Auto-overriding of pandas_datareader's get_data_yahoo() is deprecated and will be removed in future versions.\n",
      "    Use pdr_override() to explicitly override it.\n",
      "  DeprecationWarning)\n",
      "/Users/Brent/anaconda/envs/stocks/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.2.1, GPU: Not found\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas_datareader.data as pdr\n",
    "import datetime\n",
    "import numpy as np\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "# seems to be a problem with pandas_datareader\n",
    "# use this fix to get data from Yahoo! Finance\n",
    "import fix_yahoo_finance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "# import tensorflow and print info\n",
    "import tensorflow as tf\n",
    "gpu_info = \"Not found\" if not tf.test.gpu_device_name() else tf.test.gpu_device_name() \n",
    "print(\"Tensorflow version %s, GPU: %s\" % (tf.__version__,gpu_info))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_unit_tests(testClass):\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(testClass)\n",
    "    unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Create a class that downloads the stock data and sets up the input/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CreateDataSet:\n",
    "    \n",
    "    def __init__(self,symbol):\n",
    "        self.symbol = symbol\n",
    "        self.number_of_days = None\n",
    "        self.train_start_date = None\n",
    "        self.train_end_date = None\n",
    "        self.test_start_date = None\n",
    "        self.test_end_date = None\n",
    "        self.validation_start_date = None\n",
    "        self.validation_end_date = None\n",
    "        self.yahoo_data = None\n",
    "        self.pandas_df = None\n",
    "        self.training_df = None\n",
    "        self.test_df = None\n",
    "        self.validation_df = None\n",
    "        self.training_x = None\n",
    "        self.training_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.validation_x = None\n",
    "        self.validation_y = None\n",
    "        self.batch_size = None\n",
    "        self.__data_downloaded = False\n",
    "\n",
    "        \n",
    "    def set_training_dates(self,start_date,end_date):\n",
    "        self.__check_date_string_format(start_date)\n",
    "        self.__check_date_string_format(end_date)\n",
    "        self.__check_start_date_before_end_date(start_date,end_date)\n",
    "        self.train_start_date = start_date\n",
    "        self.train_end_date = end_date\n",
    "        \n",
    "    def set_test_dates(self,start_date,end_date):\n",
    "        self.__check_date_string_format(start_date)\n",
    "        self.__check_date_string_format(end_date)\n",
    "        self.__check_start_date_before_end_date(start_date,end_date)\n",
    "        self.test_start_date = start_date\n",
    "        self.test_end_date = end_date\n",
    "        \n",
    "    def set_validation_dates(self,start_date,end_date):\n",
    "        self.__check_date_string_format(start_date)\n",
    "        self.__check_date_string_format(end_date)\n",
    "        self.__check_start_date_before_end_date(start_date,end_date)\n",
    "        self.validation_start_date = start_date\n",
    "        self.validation_end_date = end_date\n",
    "        \n",
    "    def set_batch_size(self,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def download_data(self):\n",
    "        assert self.train_start_date != None, \"Training start date not set\"\n",
    "        assert self.train_end_date != None, \"Training end date not set\"\n",
    "        assert self.test_start_date != None, \"Test start date not set\"\n",
    "        assert self.test_end_date != None, \"Test end date not set\"\n",
    "        assert self.validation_start_date != None, \"Validation start date not set\"\n",
    "        assert self.validation_end_date != None, \"Validation end date not set\"\n",
    "        \n",
    "        start_date, end_date = self.__calculate_download_start_end_dates()\n",
    "\n",
    "        self.yahoo_data = self.__download_data_yahoo(self.symbol,start_date,end_date)\n",
    "        \n",
    "        assert self.yahoo_data.shape != (0,0), \"No data was downloaded\"\n",
    "        \n",
    "        self.__data_downloaded = True\n",
    "        \n",
    "    def create_dataset(self,number_of_days=10):\n",
    "        \n",
    "        self.number_of_days = number_of_days\n",
    "        \n",
    "        if not self.__data_downloaded:\n",
    "            self.download_data()\n",
    "        \n",
    "        prices = self.yahoo_data['Adj Close']\n",
    "        \n",
    "        # fill in missing data\n",
    "        prices.fillna(method='ffill', inplace=True)\n",
    "        prices.fillna(method='bfill', inplace=True)\n",
    "        \n",
    "        # scale features to make neural network easier to train\n",
    "        scaled_prices = self.__scale_data(prices)\n",
    "        \n",
    "        # input features named X0, X1, X2, ...\n",
    "        # output feature named Y (this is what we are predicting, tomorrows stock price)\n",
    "        x_features = [ 'X%d' % (i) for i in range(self.number_of_days)]\n",
    "        y_feature = ['Y']\n",
    "        features = x_features + y_feature\n",
    "        \n",
    "        df = pd.DataFrame(index=prices.index,columns=features)\n",
    "        df['Y'] = scaled_prices.shift(-1)\n",
    "        \n",
    "        for i in range(self.number_of_days):\n",
    "            x_feature = \"X%d\" % (i)\n",
    "            df[x_feature] = scaled_prices.shift(i)\n",
    "            \n",
    "        # trim the dataset to only use dates with enough data\n",
    "        #\n",
    "        # example: if number_of_days=10, then first date with enough\n",
    "        # information will be day 10 (i.e. array index 9)\n",
    "        #\n",
    "        # last date will be next to last day in dataset since predicting tommorrows price\n",
    "        df = df[self.number_of_days-1:-1]\n",
    "        \n",
    "        # there shouldn't be any NaN values\n",
    "        assert df.isnull().values.any() == False, \"Dataset contains unexpected NaN values\"\n",
    "                    \n",
    "        self.pandas_df = df\n",
    "        \n",
    "        # split dataset into training/test/validation sets\n",
    "        \n",
    "        # data as pandas dataframes\n",
    "        self.training_df = df[self.train_start_date:self.train_end_date]\n",
    "        self.test_df = df[self.test_start_date:self.test_end_date]\n",
    "        self.validation_df = df[self.validation_start_date:self.validation_end_date]\n",
    "        \n",
    "        # data as numpy arrays\n",
    "        self.training_x = self.training_df[x_features].values\n",
    "        self.training_y = self.training_df[y_feature].values\n",
    "        self.test_x = self.test_df[x_features].values\n",
    "        self.test_y = self.test_df[y_feature].values\n",
    "        self.validation_x = self.validation_df[x_features].values\n",
    "        self.validation_y = self.validation_df[y_feature].values\n",
    "        self.x_features = x_features\n",
    "        \n",
    "    # function used with neural networks\n",
    "    # training happens on a batch instead of all the data\n",
    "    # this function is called multiple times using an iterator to get the next batch\n",
    "    def get_batches(self):\n",
    "        assert self.batch_size != None\n",
    "        \n",
    "        n_batches = len(self.training_x) // self.batch_size\n",
    "        assert n_batches > 0, \"Batch size %d is too big (resulted in 0 batches)\" % (self.batch_size)\n",
    "        \n",
    "        x = self.training_x[:n_batches*self.batch_size]\n",
    "        y = self.training_y[:n_batches*self.batch_size]\n",
    "        \n",
    "        for ii in range(0, len(x), self.batch_size):\n",
    "            yield x[ii:ii+self.batch_size], y[ii:ii+self.batch_size]\n",
    "        \n",
    "    def __download_data_yahoo(self,symbol,start_date,end_date):\n",
    "        data = pdr.get_data_yahoo(symbol, start=start_date, end=end_date)\n",
    "        return data\n",
    "    \n",
    "    def __str_to_datetime(self,date_str):\n",
    "        return datetime.datetime.strptime(date_str,\"%Y-%m-%d\")\n",
    "    \n",
    "    def __check_date_string_format(self,date_str):\n",
    "        assert datetime.datetime.strptime(date_str,\"%Y-%m-%d\"), \"Dates must be a string in the format YYYY-MM-DD\"\n",
    "        \n",
    "    def __check_start_date_before_end_date(self,start_date,end_date):\n",
    "        start = self.__str_to_datetime(start_date)\n",
    "        end = self.__str_to_datetime(end_date)\n",
    "        assert start < end, \"Start date must be before the end date\"\n",
    "        \n",
    "    def __calculate_download_start_end_dates(self):\n",
    "        train_start = self.__str_to_datetime(self.train_start_date)\n",
    "        test_start = self.__str_to_datetime(self.test_start_date)\n",
    "        validation_start = self.__str_to_datetime(self.validation_start_date)\n",
    "        train_end = self.__str_to_datetime(self.train_end_date)\n",
    "        test_end = self.__str_to_datetime(self.test_end_date)\n",
    "        validation_end = self.__str_to_datetime(self.validation_end_date)\n",
    "        \n",
    "        start_datetime = min(train_start,test_start,validation_start)\n",
    "        end_datetime = max(train_end,test_end,validation_end)\n",
    "        \n",
    "        start_date = start_datetime.strftime(\"%Y-%m-%d\")\n",
    "        end_date = end_datetime.strftime(\"%Y-%m-%d\")\n",
    "          \n",
    "        return start_date,end_date\n",
    "    \n",
    "    def __scale_data(self,data):\n",
    "        mean, std = data.mean(), data.std()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "        scaled_data = (data - mean) / std\n",
    "        return scaled_data\n",
    "    \n",
    "    def unscale_data(self,scaled_data):\n",
    "        return scaled_data * self.std + self.mean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 3.422s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Test the create data set class\n",
    "class CreateDataSetTest(unittest.TestCase):\n",
    "        \n",
    "    def test_set_dates(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-01\",end_date=\"2011-01-01\")\n",
    "        dataset.set_test_dates(start_date=\"2010-01-01\",end_date=\"2011-01-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2010-01-01\",end_date=\"2011-01-01\")\n",
    "        \n",
    "    def test_set_dates_not_str(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=datetime.datetime(2010,1,1),end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"2010-01-01\",end_date=datetime.datetime(2011,1,1))\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=datetime.datetime(2010,1,1),end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"2010-01-01\",end_date=datetime.datetime(2011,1,1))\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=datetime.datetime(2010,1,1),end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"2010-01-01\",end_date=datetime.datetime(2011,1,1))\n",
    "            \n",
    "    def test_set_dates_bad_str_format(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"01-01-2011\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"2010-01-01\",end_date=\"01-01-2011\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"01-01-2011\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"2010-01-01\",end_date=\"01-01-2011\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"01-01-2011\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"2010-01-01\",end_date=\"01-01-2011\")\n",
    "            \n",
    "    def test_set_dates_start_date_after_end_date(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"2011-01-02\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"2011-01-02\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"2011-01-02\",end_date=\"2011-01-01\")\n",
    "            \n",
    "    def test_download_start_end_dates_1(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.download_data()\n",
    "        \n",
    "        # (start_date,end_date) should be (training_start,validation_end)\n",
    "        self.__verify_start_end_date(dataset.yahoo_data,expected_start=\"2010-01-04\",expected_end=\"2012-06-01\")\n",
    "        \n",
    "    def test_download_start_end_dates_2(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_test_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_training_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.download_data()\n",
    "        \n",
    "        # (start_date,end_date) should be (test_start,training_end)\n",
    "        self.__verify_start_end_date(dataset.yahoo_data,expected_start=\"2010-01-04\",expected_end=\"2012-06-01\")\n",
    "        \n",
    "    def test_download_start_end_dates_3(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_validation_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_training_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.download_data()\n",
    "        \n",
    "        # (start_date,end_date) should be (validation_start,test_end)\n",
    "        self.__verify_start_end_date(dataset.yahoo_data,expected_start=\"2010-01-04\",expected_end=\"2012-06-01\")\n",
    "        \n",
    "    def __verify_start_end_date(self,dataframe,expected_start,expected_end):\n",
    "        start_date, end_date = dataframe.index[0], dataframe.index[-1]\n",
    "        start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        self.assertEqual(start_date,expected_start)\n",
    "        self.assertEqual(end_date,expected_end)\n",
    "        \n",
    "    def test_create_dataset(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.create_dataset()\n",
    "        \n",
    "        # check for 10 x features and 1 y feature\n",
    "        self.assertEqual(dataset.training_x.shape[1],10)\n",
    "        self.assertEqual(dataset.training_y.shape[1],1)\n",
    "        self.assertEqual(dataset.test_x.shape[1],10)\n",
    "        self.assertEqual(dataset.test_y.shape[1],1)\n",
    "        self.assertEqual(dataset.validation_x.shape[1],10)\n",
    "        self.assertEqual(dataset.validation_y.shape[1],1)\n",
    "        \n",
    "        # check dates\n",
    "        self.__check_data_in_date_range(dataset.training_df,start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        self.__check_data_in_date_range(dataset.test_df,start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        self.__check_data_in_date_range(dataset.validation_df,start_date=\"2012-01-04\",end_date=\"2012-06-01\")\n",
    "        \n",
    "    def __check_data_in_date_range(self,df,start_date,end_date):\n",
    "        dates = df.index\n",
    "        start_timestamp = pd.to_datetime(start_date)        \n",
    "        end_timestamp = pd.to_datetime(end_date)\n",
    "        \n",
    "        for date in dates:\n",
    "            self.assertTrue(date >= start_timestamp)\n",
    "            self.assertTrue(date <= end_timestamp)\n",
    "            \n",
    "    def test_get_batches(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.create_dataset()\n",
    "        \n",
    "        dataset.set_batch_size(32)\n",
    "        for x_batch, y_batch in dataset.get_batches():\n",
    "            self.assertEqual(x_batch.shape,(32,10))\n",
    "            self.assertEqual(y_batch.shape,(32,1))\n",
    "            \n",
    "    \n",
    "        \n",
    "run_unit_tests(CreateDataSetTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your model\n",
    "Setup whatever model you plan to use for price prediction. i.e. neural network, linear regression, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.learning_rate = None\n",
    "        self.number_of_inputs = None\n",
    "        self.keep_prob = None\n",
    "        self.epochs = None\n",
    "        self.dataset = None\n",
    "        self.optimizer = None\n",
    "        self.cost = None\n",
    "        self.graph = None\n",
    "        self.output = None\n",
    "        self.run_until_loss_achieved = False\n",
    "        self.max_epochs = 0\n",
    "        self.average_last_n_samples = 0\n",
    "        self.loss_threshold = 0\n",
    "        self.hidden_layers = []\n",
    "        \n",
    "    def add_inputs(self,number_of_inputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        \n",
    "    def add_hidden_layer(self,number_of_nodes,dropout=False):\n",
    "        self.hidden_layers.append((number_of_nodes,dropout))\n",
    "            \n",
    "    def set_learning_rate(self,learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def set_keep_probability(self,keep_prob):\n",
    "        self.keep_probability = keep_prob\n",
    "        \n",
    "    def set_epochs(self,epochs):\n",
    "        self.run_until_loss_achieved = False\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    # instead of running for a specific number of epochs, \n",
    "    # keep training until the loss value is acceptable\n",
    "    def stop_when_average_validation_loss_below(self,loss_threshold,last_n_samples=5,max_epochs=100000):\n",
    "        self.run_until_loss_achieved = True\n",
    "        self.loss_threshold = loss_threshold\n",
    "        self.max_epochs = max_epochs\n",
    "        self.average_last_n_samples = last_n_samples\n",
    "        \n",
    "    def set_dataset(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "        \n",
    "            inputs = tf.placeholder(tf.float32,[None,self.number_of_inputs],name=\"inputs\")\n",
    "            targets = tf.placeholder(tf.float32,[None,1],name=\"targets\")\n",
    "            keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "        \n",
    "            # add hidden layers\n",
    "            layer = inputs\n",
    "            for i in range(len(self.hidden_layers)):\n",
    "                num_nodes, dropout = self.hidden_layers[i]\n",
    "\n",
    "                layer = tf.layers.dense(layer,num_nodes,activation=tf.nn.relu)\n",
    "            \n",
    "                if dropout:\n",
    "                    layer = tf.nn.dropout(layer,keep_prob)\n",
    "            \n",
    "            \n",
    "            # last layer predict the price\n",
    "            output = tf.layers.dense(layer,1,activation=None,name=\"output\")\n",
    "        \n",
    "            # setup loss function and optimizer\n",
    "            loss = tf.nn.l2_loss(targets-output)\n",
    "            cost = tf.reduce_mean(loss)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(cost)\n",
    "        \n",
    "            # save variables to be used later\n",
    "            self.inputs = inputs\n",
    "            self.targets = targets\n",
    "            self.keep_prob = keep_prob\n",
    "            self.cost = cost\n",
    "            self.optimizer = optimizer\n",
    "            self.output = output\n",
    "        \n",
    "    def train(self,print_updates=True):\n",
    "        \n",
    "        self.__reset_stats()\n",
    "        num_epochs = self.__calculate_number_of_epochs()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                \n",
    "                batch_i = 0\n",
    "                for batch_x, batch_y in self.dataset.get_batches():\n",
    "                    \n",
    "                    sess.run(self.optimizer,feed_dict={self.inputs:batch_x, self.targets:batch_y, self.keep_prob:self.keep_probability})\n",
    "                    train_loss, val_loss = self.__save_stats(sess,batch_x,batch_y)\n",
    "                    \n",
    "                    if print_updates:\n",
    "                        #print(\"Epoch %d Batch %d:  Training Loss: %0.4f, Validation Loss %0.4f\" % (epoch,batch_i,train_loss,val_loss))\n",
    "                        sys.stdout.write(\"\\rEpoch %d Batch %d:  Training Loss: %0.4f, Validation Loss %0.4f\" % (epoch,batch_i,train_loss,val_loss))\n",
    "                        sys.stdout.flush()\n",
    "                        \n",
    "                    batch_i += 1\n",
    "                    \n",
    "                if self.__terminate_training():\n",
    "                    break\n",
    "                    \n",
    "            # save the number of epochs run\n",
    "            self.number_of_epochs_run = epoch\n",
    "             \n",
    "            if print_updates:\n",
    "                print(\"\")\n",
    "            \n",
    "            # run final model on the test set\n",
    "            test_loss = self.__calculate_test_loss(sess)\n",
    "            print(\"Training Complete.  Test Set Loss: %0.4f\" % (test_loss))\n",
    "            self.test_loss = test_loss\n",
    "            \n",
    "            # save the model\n",
    "            save_path = saver.save(sess, \"checkpoints/nn.ckpt\")\n",
    "    \n",
    "    def predict(self,input_x):\n",
    "        \n",
    "        x = np.array([input_x])\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "            \n",
    "            output_y = sess.run(self.output, feed_dict={self.inputs:x,self.keep_prob:1})\n",
    "            \n",
    "        return output_y\n",
    "    \n",
    "    def predict_sequence(self,initial_input_x,x_features,length):\n",
    "        \n",
    "        x = np.array([initial_input_x])\n",
    "        sequence = []\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "            \n",
    "            for i in range(length):\n",
    "                \n",
    "                # predict one data sample\n",
    "                y = sess.run(self.output, feed_dict={self.inputs:x,self.keep_prob:1})[0][0]\n",
    "                sequence.append(y)\n",
    "                \n",
    "                # prepare data for the next sample\n",
    "                x = self.__prepare_next_input(x,x_features,y)\n",
    "                \n",
    "        return np.array(sequence)\n",
    "            \n",
    "    def plot_losses(self,title=\"\",start_index=0):\n",
    "        x = range(len(self.train_losses))\n",
    "        plt.plot(x[start_index:],self.train_losses[start_index:], label='Training loss')\n",
    "        plt.plot(x[start_index:],self.validation_losses[start_index:], label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        _ = plt.ylim()\n",
    "                    \n",
    "    def __reset_stats(self):\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "        \n",
    "    def __calculate_number_of_epochs(self):\n",
    "        if self.run_until_loss_achieved:\n",
    "            return self.max_epochs\n",
    "        \n",
    "        return self.epochs\n",
    "        \n",
    "    def __calculate_train_loss(self,session,batch_x,batch_y):\n",
    "        train_loss = session.run(self.cost, feed_dict={self.inputs:batch_x,self.targets:batch_y,self.keep_prob:1})\n",
    "        return train_loss\n",
    "    \n",
    "    def __calculate_validation_loss(self,session):\n",
    "        validation_loss = session.run(self.cost, feed_dict={self.inputs:self.dataset.validation_x,self.targets:self.dataset.validation_y,self.keep_prob:1})\n",
    "        return validation_loss\n",
    "    \n",
    "    def __calculate_test_loss(self,session):\n",
    "        test_loss = session.run(self.cost, feed_dict={self.inputs:self.dataset.test_x,self.targets:self.dataset.test_y,self.keep_prob:1})\n",
    "        return test_loss\n",
    "                       \n",
    "    def __save_stats(self,session,batch_x,batch_y):\n",
    "        train_loss = self.__calculate_train_loss(session,batch_x,batch_y)\n",
    "        validation_loss = self.__calculate_validation_loss(session)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.validation_losses.append(validation_loss)\n",
    "        return train_loss, validation_loss\n",
    "    \n",
    "    def __prepare_next_input(self,current_x,x_feature_map,predicted_y):\n",
    "        \n",
    "        new_x = np.zeros(current_x.shape,dtype=current_x.dtype)\n",
    "        \n",
    "        num_features = len(x_feature_map)\n",
    "        \n",
    "        # check out the feature map to ensure the names are as expected\n",
    "        # i.e. X0, X1, X2, ...\n",
    "        # and create an index lookup\n",
    "        lookup_index = dict()\n",
    "        for i in range(num_features):\n",
    "            feature = \"X%d\" % (i)\n",
    "            assert feature in x_feature_map\n",
    "            lookup_index[feature] = i\n",
    "            \n",
    "        # shift all inputs by 1 day except for the last feature\n",
    "        for i in range(-1,num_features-2,-1):\n",
    "            old_feature = \"X%d\" % (i)\n",
    "            new_feature = \"X%d\" % (i+1)\n",
    "            \n",
    "            print(\"%s = %s\" % (new_feature,old_feature))\n",
    "            \n",
    "            old_i = lookup_index[old_feature]\n",
    "            new_i = lookup_index[new_feature]\n",
    "            \n",
    "            new_x[new_i] = current_x[old_i]\n",
    "        \n",
    "        x0_index = lookup_index[\"X0\"]\n",
    "        new_x[x0_index] = predicted_y\n",
    "        \n",
    "        return new_x\n",
    "    \n",
    "    def __terminate_training(self):\n",
    "        if self.run_until_loss_achieved == False:\n",
    "            return False\n",
    "        \n",
    "        last_n_losses = self.validation_losses[-self.average_last_n_samples:]\n",
    "        average_validation_loss = np.mean(last_n_losses)\n",
    "        \n",
    "        return average_validation_loss < self.loss_threshold  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Create a class to help try out different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    \n",
    "    def __init__(self,initial_seed=None):\n",
    "        if initial_seed == None:\n",
    "            self.set_seed(0)\n",
    "        else:\n",
    "            self.set_seed(initial_seed)\n",
    "\n",
    "        self.params = dict()\n",
    "        self.values = dict()\n",
    "        \n",
    "    def choose_params(self):\n",
    "        \n",
    "        self.__set_random_seed()\n",
    "        \n",
    "        self.values = dict()\n",
    "        \n",
    "        for param in self.params:\n",
    "            \n",
    "            param_info = self.params[param]\n",
    "            param_type = param_info[0]\n",
    "            \n",
    "            if param_type == \"scalar\":\n",
    "                _, value = param_info\n",
    "                self.values[param] = value\n",
    "                \n",
    "            elif param_type == \"random_choice\":\n",
    "                _, choices = param_info\n",
    "                value = np.random.choice(choices)\n",
    "                self.values[param] = value\n",
    "                \n",
    "            elif param_type == \"random_float\":\n",
    "                _, start, end = param_info\n",
    "                value = np.random.uniform(start,end)                \n",
    "                self.values[param] = value\n",
    "                \n",
    "            elif param_type == \"random_int\":\n",
    "                _, start, end = param_info\n",
    "                value = np.random.randint(start,end+1) \n",
    "                self.values[param] = value\n",
    "                \n",
    "            else:\n",
    "                assert False, \"Unexpected parameter type %s\" % (param_type)\n",
    "    \n",
    "    def set_seed(self,seed):\n",
    "        self.seed = seed\n",
    "        self.increment_seed = False\n",
    "    \n",
    "    def scalar(self,param_name,value):\n",
    "        self.params[param_name] = (\"scalar\",value)\n",
    "        \n",
    "    def random_choice(self,param_name,choices):\n",
    "        self.params[param_name] = (\"random_choice\",choices)\n",
    "    \n",
    "    def random_float(self,param_name,start_value,end_value):\n",
    "        self.params[param_name] = (\"random_float\",start_value,end_value)\n",
    "    \n",
    "    def random_int(self,param_name,start_value,end_value):\n",
    "        self.params[param_name] = (\"random_int\",start_value,end_value)\n",
    "        \n",
    "    def print_chosen_values(self,seed=None):\n",
    "\n",
    "        # save current seed\n",
    "        if seed != None:\n",
    "            saved_seed = self.seed\n",
    "            self.set_seed(seed)\n",
    "            self.choose_params()\n",
    "            \n",
    "        # print the values\n",
    "        parameters = sorted(self.values.keys())\n",
    "        \n",
    "        for param in parameters:\n",
    "            print(\"%s: %s\" % (param,self.values[param]))\n",
    "        print(\"\")\n",
    "        \n",
    "        # restore seed\n",
    "        if seed != None:\n",
    "            self.set_seed(saved_seed)\n",
    "            self.choose_params()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __set_random_seed(self):        \n",
    "        if self.increment_seed:\n",
    "            self.seed += 1\n",
    "        self.increment_seed = True\n",
    "            \n",
    "        np.random.seed(self.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for displaying hyperparameter search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_results(number,results):\n",
    "    values = results.values()\n",
    "    train_loss_sorted = sorted(values,key=lambda x:x[1])\n",
    "    validation_loss_sorted = sorted(values,key=lambda x:x[2])\n",
    "    test_loss_sorted = sorted(values,key=lambda x:x[3])\n",
    "    \n",
    "    top_train_loss = train_loss_sorted[:number]\n",
    "    top_validation_loss = validation_loss_sorted[:number]\n",
    "    top_test_loss = test_loss_sorted[:number]\n",
    "    \n",
    "    return top_train_loss, top_validation_loss, top_test_loss\n",
    "\n",
    "def print_loss_results(top_loss,title):\n",
    "    print(\"\")\n",
    "    print(title)\n",
    "    for i in range(len(top_loss)):\n",
    "        seed, train_loss, val_loss, test_loss, epochs = top_loss[i]\n",
    "        print(\"Seed %d (%d epochs): Train=%0.4f, Validation=%0.4f, Test=%0.4f\" % (seed, epochs, train_loss, val_loss, test_loss))\n",
    "        \n",
    "def print_parameters(params,seed,title=\"\"):\n",
    "    print(title)\n",
    "    params.print_chosen_values(seed)\n",
    "    \n",
    "def smooth_loss(losses,last_num_points=5):\n",
    "    data = np.array(losses[-last_num_points:])\n",
    "    return data.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Neural Networks\n",
    "Choose different hyperparameters and run them in the neural network.<br>\n",
    "Keep track of the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brent/anaconda/envs/stocks/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete.  Test Set Loss: 177.2452\n",
      "Training Complete.  Test Set Loss: 0.5930\n"
     ]
    }
   ],
   "source": [
    "smooth = True   # Ensure that the model we pick has a consistent loss instead of randomly being a low value\n",
    "\n",
    "# setup the dataset\n",
    "dataset = CreateDataSet('SPY')\n",
    "dataset.set_training_dates(start_date=\"2008-01-01\",end_date=\"2010-01-01\")\n",
    "dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\") \n",
    "dataset.set_batch_size(32)\n",
    "\n",
    "# setup the parameters to vary\n",
    "params = Parameters()\n",
    "params.random_choice('learning_rate',[0.1,0.01,0.001,0.0001])\n",
    "params.scalar('keep_prob',0.5)\n",
    "params.random_choice('hidden_layers',[1,2,3])\n",
    "params.random_int('number_of_days',2,100)\n",
    "params.random_int('number_of_nodes',20,100)\n",
    "\n",
    "# setup and run the neural networks\n",
    "results = dict()\n",
    "\n",
    "for i in range(2):\n",
    "     \n",
    "    # choose the parameters\n",
    "    params.set_seed(i)\n",
    "    params.choose_params()\n",
    "    \n",
    "    number_of_days = params.values['number_of_days']\n",
    "    learning_rate = params.values['learning_rate']\n",
    "    keep_prob = params.values['keep_prob']\n",
    "    num_nodes_1st_layer = params.values['number_of_nodes']\n",
    "    num_layers = params.values['hidden_layers']\n",
    "    \n",
    "    # create the dataset\n",
    "    dataset.create_dataset(number_of_days=number_of_days)\n",
    "    \n",
    "    # build the neural network\n",
    "    nn = NeuralNetwork()\n",
    "    nn.set_learning_rate(learning_rate)\n",
    "    nn.set_keep_probability(keep_prob)\n",
    "    nn.stop_when_average_validation_loss_below(1.0)\n",
    "    nn.set_dataset(dataset)\n",
    "    \n",
    "    nn.add_inputs(number_of_inputs=number_of_days)\n",
    "    \n",
    "    num_nodes = num_nodes_1st_layer\n",
    "    for layer in range(num_layers):\n",
    "        nn.add_hidden_layer(number_of_nodes=num_nodes)\n",
    "        num_nodes = num_nodes // 2\n",
    "    \n",
    "    nn.build_model() \n",
    "    \n",
    "    # train the model\n",
    "    nn.train(print_updates=False)\n",
    "    \n",
    "    # save results\n",
    "    # seed number, train loss, validation loss, test loss\n",
    "    if smooth:\n",
    "        smooth_train_loss = smooth_loss(nn.train_losses)\n",
    "        smooth_val_loss = smooth_loss(nn.validation_losses)\n",
    "        \n",
    "        results[i] = (i,smooth_train_loss, smooth_val_loss, nn.test_loss,nn.self.number_of_epochs_run)\n",
    "    else:\n",
    "        results[i] = (i,nn.train_losses[-1], nn.validation_losses[-1], nn.test_loss, self.number_of_epochs_run)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the top results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Training Loss:\n",
      "Seed 1: Train=0.4623, Validation=0.9705, Test=0.5930\n",
      "Seed 0: Train=4.1681, Validation=248.3544, Test=177.2452\n",
      "\n",
      "Top 5 Validation Loss:\n",
      "Seed 1: Train=0.4623, Validation=0.9705, Test=0.5930\n",
      "Seed 0: Train=4.1681, Validation=248.3544, Test=177.2452\n",
      "\n",
      "Top 5 Test Loss:\n",
      "Seed 1: Train=0.4623, Validation=0.9705, Test=0.5930\n",
      "Seed 0: Train=4.1681, Validation=248.3544, Test=177.2452\n"
     ]
    }
   ],
   "source": [
    "top_train, top_val, top_test = top_results(5,results)\n",
    "print_loss_results(top_train,\"Top 5 Training Loss:\")\n",
    "print_loss_results(top_val,\"Top 5 Validation Loss:\")\n",
    "print_loss_results(top_test,\"Top 5 Test Loss:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Training Loss Parameters\n",
      "hidden_layers: 1\n",
      "keep_prob: 0.5\n",
      "learning_rate: 0.01\n",
      "number_of_days: 74\n",
      "number_of_nodes: 29\n",
      "\n",
      "Top Validation Loss Parameters\n",
      "hidden_layers: 1\n",
      "keep_prob: 0.5\n",
      "learning_rate: 0.01\n",
      "number_of_days: 74\n",
      "number_of_nodes: 29\n",
      "\n",
      "Top Test Loss Parameters\n",
      "hidden_layers: 1\n",
      "keep_prob: 0.5\n",
      "learning_rate: 0.01\n",
      "number_of_days: 74\n",
      "number_of_nodes: 29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print out the top results hyperparemeters\n",
    "print_parameters(params,seed=top_train[0][0],title=\"Top Training Loss Parameters\")\n",
    "print_parameters(params,seed=top_val[0][0],title=\"Top Validation Loss Parameters\")\n",
    "print_parameters(params,seed=top_test[0][0],title=\"Top Test Loss Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = CreateDataSet('SPY')\n",
    "dataset.set_training_dates(start_date=\"2008-01-01\",end_date=\"2010-01-01\")\n",
    "dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\") \n",
    "dataset.set_batch_size(32)\n",
    "dataset.create_dataset()\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.set_learning_rate(.0001)\n",
    "nn.set_keep_probability(0.5)\n",
    "nn.set_epochs(10000)\n",
    "nn.set_dataset(dataset)\n",
    "\n",
    "nn.add_inputs(number_of_inputs=10)\n",
    "nn.add_hidden_layer(number_of_nodes=20)\n",
    "nn.build_model() \n",
    "\n",
    "nn.train(print_updates=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "nn.plot_losses(title=\"Overall Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show losses in better detail\n",
    "\n",
    "# set starting point where train loss goes below 5 for at least 10 samples\n",
    "def find_starting_point(losses,loss_target=5,consecutive_values=10):\n",
    "    start = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for i,loss in enumerate(losses):\n",
    "        start = i\n",
    "        if loss < loss_target:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "        \n",
    "        if counter >= consecutive_values:\n",
    "            break\n",
    "    return start\n",
    "    \n",
    "start_index = find_starting_point(nn.validation_losses,loss_target=5)\n",
    "nn.plot_losses(title=\"Below 5 Loss\",start_index=start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.plot_losses(title=\"Last 200\",start_index=-200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the first data point (use to show that prediction working)\n",
    "x = dataset.test_x[0]\n",
    "y = nn.predict(x)\n",
    "target_y = dataset.test_y[0]\n",
    "\n",
    "y_price = dataset.unscale_data(y)\n",
    "target_y_price = dataset.unscale_data(target_y)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"Prediction: %0.4f (Price $%0.4f)\" % (y,y_price))\n",
    "print(\"Target    : %0.4f (Price $%0.4f)\" % (target_y,target_y_price))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the price over the test set starting from the initial test data point\n",
    "x = dataset.test_x[0]\n",
    "num_predictions = len(dataset.test_x)\n",
    "\n",
    "y = nn.predict_sequence(x,dataset.x_features,length=num_predictions)\n",
    "\n",
    "y_prices = dataset.unscale_data(y)\n",
    "test_y_prices = dataset.unscale_data(dataset.test_y)\n",
    "\n",
    "# plot the prices\n",
    "plt.plot(test_y_prices, label='Actual Price')\n",
    "plt.plot(y_prices, label='Predicted Price')\n",
    "plt.legend()\n",
    "plt.title(\"Test Data Prices Actual vs Predicted\")\n",
    "_ = plt.ylim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the price over the training set starting from the initial training data point\n",
    "x = dataset.training_x[0]\n",
    "num_predictions = len(dataset.training_x)\n",
    "\n",
    "y = nn.predict_sequence(x,dataset.x_features,length=num_predictions)\n",
    "\n",
    "y_prices = dataset.unscale_data(y)\n",
    "test_y_prices = dataset.unscale_data(dataset.training_y)\n",
    "\n",
    "# plot the prices\n",
    "plt.plot(test_y_prices, label='Actual Price')\n",
    "plt.plot(y_prices, label='Predicted Price')\n",
    "plt.legend()\n",
    "plt.title(\"Training Data Prices Actual vs Predicted\")\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
