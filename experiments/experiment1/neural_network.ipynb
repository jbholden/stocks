{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.2.1, GPU: Not found\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader.data as pdr\n",
    "import datetime\n",
    "import numpy as np\n",
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# seems to be a problem with pandas_datareader\n",
    "# use this fix to get data from Yahoo! Finance\n",
    "import fix_yahoo_finance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "# import tensorflow and print info\n",
    "import tensorflow as tf\n",
    "gpu_info = \"Not found\" if not tf.test.gpu_device_name() else tf.test.gpu_device_name() \n",
    "print(\"Tensorflow version %s, GPU: %s\" % (tf.__version__,gpu_info))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_unit_tests(testClass):\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(testClass)\n",
    "    unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Create a class that downloads the stock data and sets up the input/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CreateDataSet:\n",
    "    \n",
    "    def __init__(self,symbol,number_of_days=10):\n",
    "        self.symbol = symbol\n",
    "        self.number_of_days = number_of_days\n",
    "        self.train_start_date = None\n",
    "        self.train_end_date = None\n",
    "        self.test_start_date = None\n",
    "        self.test_end_date = None\n",
    "        self.validation_start_date = None\n",
    "        self.validation_end_date = None\n",
    "        self.yahoo_data = None\n",
    "        self.pandas_df = None\n",
    "        self.training_df = None\n",
    "        self.test_df = None\n",
    "        self.validation_df = None\n",
    "        self.training_x = None\n",
    "        self.training_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.validation_x = None\n",
    "        self.validation_y = None\n",
    "        self.batch_size = None\n",
    "\n",
    "        \n",
    "    def set_training_dates(self,start_date,end_date):\n",
    "        self.__check_date_string_format(start_date)\n",
    "        self.__check_date_string_format(end_date)\n",
    "        self.__check_start_date_before_end_date(start_date,end_date)\n",
    "        self.train_start_date = start_date\n",
    "        self.train_end_date = end_date\n",
    "        \n",
    "    def set_test_dates(self,start_date,end_date):\n",
    "        self.__check_date_string_format(start_date)\n",
    "        self.__check_date_string_format(end_date)\n",
    "        self.__check_start_date_before_end_date(start_date,end_date)\n",
    "        self.test_start_date = start_date\n",
    "        self.test_end_date = end_date\n",
    "        \n",
    "    def set_validation_dates(self,start_date,end_date):\n",
    "        self.__check_date_string_format(start_date)\n",
    "        self.__check_date_string_format(end_date)\n",
    "        self.__check_start_date_before_end_date(start_date,end_date)\n",
    "        self.validation_start_date = start_date\n",
    "        self.validation_end_date = end_date\n",
    "        \n",
    "    def set_batch_size(self,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def download_data(self):\n",
    "        assert self.train_start_date != None, \"Training start date not set\"\n",
    "        assert self.train_end_date != None, \"Training end date not set\"\n",
    "        assert self.test_start_date != None, \"Test start date not set\"\n",
    "        assert self.test_end_date != None, \"Test end date not set\"\n",
    "        assert self.validation_start_date != None, \"Validation start date not set\"\n",
    "        assert self.validation_end_date != None, \"Validation end date not set\"\n",
    "        \n",
    "        start_date, end_date = self.__calculate_download_start_end_dates()\n",
    "\n",
    "        self.yahoo_data = self.__download_data_yahoo(self.symbol,start_date,end_date)\n",
    "        \n",
    "        assert self.yahoo_data.shape != (0,0), \"No data was downloaded\"\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        \n",
    "        if self.yahoo_data == None:\n",
    "            self.download_data()\n",
    "        \n",
    "        prices = self.yahoo_data['Adj Close']\n",
    "        \n",
    "        # fill in missing data\n",
    "        prices.fillna(method='ffill', inplace=True)\n",
    "        prices.fillna(method='bfill', inplace=True)\n",
    "        \n",
    "        # scale features to make neural network easier to train\n",
    "        scaled_prices = self.__scale_data(prices)\n",
    "        \n",
    "        # input features named X0, X1, X2, ...\n",
    "        # output feature named Y (this is what we are predicting, tomorrows stock price)\n",
    "        x_features = [ 'X%d' % (i) for i in range(self.number_of_days)]\n",
    "        y_feature = ['Y']\n",
    "        features = x_features + y_feature\n",
    "        \n",
    "        df = pd.DataFrame(index=prices.index,columns=features)\n",
    "        df['Y'] = scaled_prices.shift(-1)\n",
    "        \n",
    "        for i in range(self.number_of_days):\n",
    "            x_feature = \"X%d\" % (i)\n",
    "            df[x_feature] = scaled_prices.shift(i)\n",
    "            \n",
    "        # trim the dataset to only use dates with enough data\n",
    "        #\n",
    "        # example: if number_of_days=10, then first date with enough\n",
    "        # information will be day 10 (i.e. array index 9)\n",
    "        #\n",
    "        # last date will be next to last day in dataset since predicting tommorrows price\n",
    "        df = df[self.number_of_days-1:-1]\n",
    "        \n",
    "        # there shouldn't be any NaN values\n",
    "        assert df.isnull().values.any() == False, \"Dataset contains unexpected NaN values\"\n",
    "                    \n",
    "        self.pandas_df = df\n",
    "        \n",
    "        # split dataset into training/test/validation sets\n",
    "        \n",
    "        # data as pandas dataframes\n",
    "        self.training_df = df[self.train_start_date:self.train_end_date]\n",
    "        self.test_df = df[self.test_start_date:self.test_end_date]\n",
    "        self.validation_df = df[self.validation_start_date:self.validation_end_date]\n",
    "        \n",
    "        # data as numpy arrays\n",
    "        self.training_x = self.training_df[x_features].values\n",
    "        self.training_y = self.training_df[y_feature].values\n",
    "        self.test_x = self.test_df[x_features].values\n",
    "        self.test_y = self.test_df[y_feature].values\n",
    "        self.validation_x = self.validation_df[x_features].values\n",
    "        self.validation_y = self.validation_df[y_feature].values\n",
    "        \n",
    "    # function used with neural networks\n",
    "    # training happens on a batch instead of all the data\n",
    "    # this function is called multiple times using an iterator to get the next batch\n",
    "    def get_batches(self):\n",
    "        assert self.batch_size != None\n",
    "        \n",
    "        n_batches = len(self.training_x) // self.batch_size\n",
    "        assert n_batches > 0, \"Batch size %d is too big (resulted in 0 batches)\" % (self.batch_size)\n",
    "        \n",
    "        x = self.training_x[:n_batches*self.batch_size]\n",
    "        y = self.training_y[:n_batches*self.batch_size]\n",
    "        \n",
    "        for ii in range(0, len(x), self.batch_size):\n",
    "            yield x[ii:ii+self.batch_size], y[ii:ii+self.batch_size]\n",
    "        \n",
    "    def __download_data_yahoo(self,symbol,start_date,end_date):\n",
    "        data = pdr.get_data_yahoo(symbol, start=start_date, end=end_date)\n",
    "        return data\n",
    "    \n",
    "    def __str_to_datetime(self,date_str):\n",
    "        return datetime.datetime.strptime(date_str,\"%Y-%m-%d\")\n",
    "    \n",
    "    def __check_date_string_format(self,date_str):\n",
    "        assert datetime.datetime.strptime(date_str,\"%Y-%m-%d\"), \"Dates must be a string in the format YYYY-MM-DD\"\n",
    "        \n",
    "    def __check_start_date_before_end_date(self,start_date,end_date):\n",
    "        start = self.__str_to_datetime(start_date)\n",
    "        end = self.__str_to_datetime(end_date)\n",
    "        assert start < end, \"Start date must be before the end date\"\n",
    "        \n",
    "    def __calculate_download_start_end_dates(self):\n",
    "        train_start = self.__str_to_datetime(self.train_start_date)\n",
    "        test_start = self.__str_to_datetime(self.test_start_date)\n",
    "        validation_start = self.__str_to_datetime(self.validation_start_date)\n",
    "        train_end = self.__str_to_datetime(self.train_end_date)\n",
    "        test_end = self.__str_to_datetime(self.test_end_date)\n",
    "        validation_end = self.__str_to_datetime(self.validation_end_date)\n",
    "        \n",
    "        start_datetime = min(train_start,test_start,validation_start)\n",
    "        end_datetime = max(train_end,test_end,validation_end)\n",
    "        \n",
    "        start_date = start_datetime.strftime(\"%Y-%m-%d\")\n",
    "        end_date = end_datetime.strftime(\"%Y-%m-%d\")\n",
    "          \n",
    "        return start_date,end_date\n",
    "    \n",
    "    def __scale_data(self,data):\n",
    "        mean, std = data.mean(), data.std()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "        scaled_data = (data - mean) / std\n",
    "        return scaled_data\n",
    "    \n",
    "    def unscale_data(self,scaled_data):\n",
    "        return scaled_data * self.std + self.mean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 3.318s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Test the create data set class\n",
    "class CreateDataSetTest(unittest.TestCase):\n",
    "        \n",
    "    def test_set_dates(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-01\",end_date=\"2011-01-01\")\n",
    "        dataset.set_test_dates(start_date=\"2010-01-01\",end_date=\"2011-01-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2010-01-01\",end_date=\"2011-01-01\")\n",
    "        \n",
    "    def test_set_dates_not_str(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=datetime.datetime(2010,1,1),end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"2010-01-01\",end_date=datetime.datetime(2011,1,1))\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=datetime.datetime(2010,1,1),end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"2010-01-01\",end_date=datetime.datetime(2011,1,1))\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=datetime.datetime(2010,1,1),end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"2010-01-01\",end_date=datetime.datetime(2011,1,1))\n",
    "            \n",
    "    def test_set_dates_bad_str_format(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"01-01-2011\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"2010-01-01\",end_date=\"01-01-2011\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"01-01-2011\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"2010-01-01\",end_date=\"01-01-2011\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"01-01-2011\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"2010-01-01\",end_date=\"01-01-2011\")\n",
    "            \n",
    "    def test_set_dates_start_date_after_end_date(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_training_dates(start_date=\"2011-01-02\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_test_dates(start_date=\"2011-01-02\",end_date=\"2011-01-01\")\n",
    "            \n",
    "        with self.assertRaises(Exception):\n",
    "            dataset.set_validation_dates(start_date=\"2011-01-02\",end_date=\"2011-01-01\")\n",
    "            \n",
    "    def test_download_start_end_dates_1(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.download_data()\n",
    "        \n",
    "        # (start_date,end_date) should be (training_start,validation_end)\n",
    "        self.__verify_start_end_date(dataset.yahoo_data,expected_start=\"2010-01-04\",expected_end=\"2012-06-01\")\n",
    "        \n",
    "    def test_download_start_end_dates_2(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_test_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_training_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.download_data()\n",
    "        \n",
    "        # (start_date,end_date) should be (test_start,training_end)\n",
    "        self.__verify_start_end_date(dataset.yahoo_data,expected_start=\"2010-01-04\",expected_end=\"2012-06-01\")\n",
    "        \n",
    "    def test_download_start_end_dates_3(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_validation_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_training_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.download_data()\n",
    "        \n",
    "        # (start_date,end_date) should be (validation_start,test_end)\n",
    "        self.__verify_start_end_date(dataset.yahoo_data,expected_start=\"2010-01-04\",expected_end=\"2012-06-01\")\n",
    "        \n",
    "    def __verify_start_end_date(self,dataframe,expected_start,expected_end):\n",
    "        start_date, end_date = dataframe.index[0], dataframe.index[-1]\n",
    "        start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        self.assertEqual(start_date,expected_start)\n",
    "        self.assertEqual(end_date,expected_end)\n",
    "        \n",
    "    def test_create_dataset(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.create_dataset()\n",
    "        \n",
    "        # check for 10 x features and 1 y feature\n",
    "        self.assertEqual(dataset.training_x.shape[1],10)\n",
    "        self.assertEqual(dataset.training_y.shape[1],1)\n",
    "        self.assertEqual(dataset.test_x.shape[1],10)\n",
    "        self.assertEqual(dataset.test_y.shape[1],1)\n",
    "        self.assertEqual(dataset.validation_x.shape[1],10)\n",
    "        self.assertEqual(dataset.validation_y.shape[1],1)\n",
    "        \n",
    "        # check dates\n",
    "        self.__check_data_in_date_range(dataset.training_df,start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        self.__check_data_in_date_range(dataset.test_df,start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        self.__check_data_in_date_range(dataset.validation_df,start_date=\"2012-01-04\",end_date=\"2012-06-01\")\n",
    "        \n",
    "    def __check_data_in_date_range(self,df,start_date,end_date):\n",
    "        dates = df.index\n",
    "        start_timestamp = pd.to_datetime(start_date)        \n",
    "        end_timestamp = pd.to_datetime(end_date)\n",
    "        \n",
    "        for date in dates:\n",
    "            self.assertTrue(date >= start_timestamp)\n",
    "            self.assertTrue(date <= end_timestamp)\n",
    "            \n",
    "    def test_get_batches(self):\n",
    "        dataset = CreateDataSet('SPY')\n",
    "        dataset.set_training_dates(start_date=\"2010-01-04\",end_date=\"2010-06-01\")\n",
    "        dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "        dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\")        \n",
    "        dataset.create_dataset()\n",
    "        \n",
    "        dataset.set_batch_size(32)\n",
    "        for x_batch, y_batch in dataset.get_batches():\n",
    "            self.assertEqual(x_batch.shape,(32,10))\n",
    "            self.assertEqual(y_batch.shape,(32,1))\n",
    "            \n",
    "    \n",
    "        \n",
    "run_unit_tests(CreateDataSetTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your model\n",
    "Setup whatever model you plan to use for price prediction. i.e. neural network, linear regression, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.learning_rate = None\n",
    "        self.number_of_inputs = None\n",
    "        self.keep_prob = None\n",
    "        self.epochs = None\n",
    "        self.dataset = None\n",
    "        self.optimizer = None\n",
    "        self.cost = None\n",
    "        self.hidden_layers = []\n",
    "        \n",
    "    def add_inputs(self,number_of_inputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        \n",
    "    def add_hidden_layer(self,number_of_nodes,dropout=False):\n",
    "        self.hidden_layers.append((number_of_nodes,dropout))\n",
    "            \n",
    "    def set_learning_rate(self,learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def set_keep_probability(self,keep_prob):\n",
    "        self.keep_probability = keep_prob\n",
    "        \n",
    "    def set_epochs(self,epochs):\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def set_dataset(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        inputs = tf.placeholder(tf.float32,[None,self.number_of_inputs],name=\"inputs\")\n",
    "        targets = tf.placeholder(tf.float32,[None,1],name=\"targets\")\n",
    "        keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "        \n",
    "        # add hidden layers\n",
    "        layer = inputs\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            num_nodes, dropout = self.hidden_layers[i]\n",
    "\n",
    "            layer = tf.layers.dense(layer,num_nodes,activation=tf.nn.relu)\n",
    "            \n",
    "            if dropout:\n",
    "                layer = tf.nn.dropout(layer,keep_prob)\n",
    "            \n",
    "            \n",
    "        # last layer predict the price\n",
    "        output = tf.layers.dense(layer,1,activation=None,name=\"output\")\n",
    "        \n",
    "        # setup loss function and optimizer\n",
    "        loss = tf.nn.l2_loss(targets-output)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(cost)\n",
    "        \n",
    "        # save variables to be used later\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.keep_prob = keep_prob\n",
    "        self.cost = cost\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        self.__reset_stats()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                \n",
    "                batch_i = 0\n",
    "                for batch_x, batch_y in self.dataset.get_batches():\n",
    "                    \n",
    "                    sess.run(self.optimizer,feed_dict={self.inputs:batch_x, self.targets:batch_y, self.keep_prob:self.keep_probability})\n",
    "                    train_loss, val_loss = self.__save_stats(sess,batch_x,batch_y)\n",
    "                    \n",
    "                    print(\"Epoch %d Batch %d:  Training Loss: %0.4f, Validation Loss %0.4f\" % (epoch,batch_i,train_loss,val_loss))\n",
    "                    \n",
    "                    batch_i += 1\n",
    "            \n",
    "            # run final model on the test set\n",
    "            test_loss = self.__calculate_test_loss(sess)\n",
    "            print(\"Training Complete.  Test Set Loss: %0.4f\" % (test_loss))\n",
    "\n",
    "                    \n",
    "    def __reset_stats(self):\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "        \n",
    "    def __calculate_train_loss(self,session,batch_x,batch_y):\n",
    "        train_loss = session.run(self.cost, feed_dict={self.inputs:batch_x,self.targets:batch_y,self.keep_prob:1})\n",
    "        return train_loss\n",
    "    \n",
    "    def __calculate_validation_loss(self,session):\n",
    "        validation_loss = session.run(self.cost, feed_dict={self.inputs:self.dataset.validation_x,self.targets:self.dataset.validation_y,self.keep_prob:1})\n",
    "        return validation_loss\n",
    "    \n",
    "    def __calculate_test_loss(self,session):\n",
    "        test_loss = session.run(self.cost, feed_dict={self.inputs:self.dataset.test_x,self.targets:self.dataset.test_y,self.keep_prob:1})\n",
    "        return test_loss\n",
    "    \n",
    "                    \n",
    "    def __save_stats(self,session,batch_x,batch_y):\n",
    "        train_loss = self.__calculate_train_loss(session,batch_x,batch_y)\n",
    "        validation_loss = self.__calculate_validation_loss(session)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.validation_losses.append(validation_loss)\n",
    "        return train_loss, validation_loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brent/anaconda/envs/stocks/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0:  Training Loss: 52.7879, Validation Loss 175.0898\n",
      "Epoch 0 Batch 1:  Training Loss: 40.3882, Validation Loss 166.3738\n",
      "Epoch 0 Batch 2:  Training Loss: 69.3737, Validation Loss 157.8967\n",
      "Epoch 0 Batch 3:  Training Loss: 30.5589, Validation Loss 149.8934\n",
      "Epoch 0 Batch 4:  Training Loss: 14.1596, Validation Loss 142.7126\n",
      "Epoch 0 Batch 5:  Training Loss: 8.4950, Validation Loss 136.5005\n",
      "Epoch 0 Batch 6:  Training Loss: 14.0729, Validation Loss 131.2132\n",
      "Epoch 0 Batch 7:  Training Loss: 13.2434, Validation Loss 126.6611\n",
      "Epoch 0 Batch 8:  Training Loss: 21.7701, Validation Loss 122.7163\n",
      "Epoch 0 Batch 9:  Training Loss: 14.2971, Validation Loss 119.2700\n",
      "Epoch 0 Batch 10:  Training Loss: 6.9695, Validation Loss 116.2368\n",
      "Epoch 0 Batch 11:  Training Loss: 5.9472, Validation Loss 113.5542\n",
      "Epoch 0 Batch 12:  Training Loss: 1.9646, Validation Loss 111.1694\n",
      "Epoch 0 Batch 13:  Training Loss: 0.6522, Validation Loss 109.0402\n",
      "Epoch 0 Batch 14:  Training Loss: 0.5113, Validation Loss 107.1337\n",
      "Epoch 1 Batch 0:  Training Loss: 30.9055, Validation Loss 104.3376\n",
      "Epoch 1 Batch 1:  Training Loss: 24.2925, Validation Loss 101.0290\n",
      "Epoch 1 Batch 2:  Training Loss: 43.0471, Validation Loss 96.9555\n",
      "Epoch 1 Batch 3:  Training Loss: 17.8867, Validation Loss 92.7384\n",
      "Epoch 1 Batch 4:  Training Loss: 8.6086, Validation Loss 88.7020\n",
      "Epoch 1 Batch 5:  Training Loss: 5.4565, Validation Loss 85.0312\n",
      "Epoch 1 Batch 6:  Training Loss: 4.1348, Validation Loss 81.7619\n",
      "Epoch 1 Batch 7:  Training Loss: 2.7277, Validation Loss 78.8453\n",
      "Epoch 1 Batch 8:  Training Loss: 4.8986, Validation Loss 76.2416\n",
      "Epoch 1 Batch 9:  Training Loss: 1.4467, Validation Loss 73.9108\n",
      "Epoch 1 Batch 10:  Training Loss: 0.7280, Validation Loss 71.8188\n",
      "Epoch 1 Batch 11:  Training Loss: 0.8939, Validation Loss 69.9409\n",
      "Epoch 1 Batch 12:  Training Loss: 0.1903, Validation Loss 68.2513\n",
      "Epoch 1 Batch 13:  Training Loss: 0.1476, Validation Loss 66.7277\n",
      "Epoch 1 Batch 14:  Training Loss: 0.2117, Validation Loss 65.3524\n",
      "Epoch 2 Batch 0:  Training Loss: 18.4333, Validation Loss 63.4392\n",
      "Epoch 2 Batch 1:  Training Loss: 14.5824, Validation Loss 61.2047\n",
      "Epoch 2 Batch 2:  Training Loss: 26.2887, Validation Loss 58.3904\n",
      "Epoch 2 Batch 3:  Training Loss: 9.9659, Validation Loss 55.4948\n",
      "Epoch 2 Batch 4:  Training Loss: 4.9749, Validation Loss 52.7346\n",
      "Epoch 2 Batch 5:  Training Loss: 3.8590, Validation Loss 50.2406\n",
      "Epoch 2 Batch 6:  Training Loss: 1.6297, Validation Loss 48.0224\n",
      "Epoch 2 Batch 7:  Training Loss: 0.5846, Validation Loss 46.0456\n",
      "Epoch 2 Batch 8:  Training Loss: 0.7555, Validation Loss 44.2819\n",
      "Epoch 2 Batch 9:  Training Loss: 0.4194, Validation Loss 42.7063\n",
      "Epoch 2 Batch 10:  Training Loss: 0.3025, Validation Loss 41.2969\n",
      "Epoch 2 Batch 11:  Training Loss: 0.3742, Validation Loss 40.0347\n",
      "Epoch 2 Batch 12:  Training Loss: 0.2256, Validation Loss 38.9028\n",
      "Epoch 2 Batch 13:  Training Loss: 0.2855, Validation Loss 37.8859\n",
      "Epoch 2 Batch 14:  Training Loss: 0.2496, Validation Loss 36.9724\n",
      "Epoch 3 Batch 0:  Training Loss: 10.1144, Validation Loss 35.7321\n",
      "Epoch 3 Batch 1:  Training Loss: 8.0876, Validation Loss 34.2970\n",
      "Epoch 3 Batch 2:  Training Loss: 14.8991, Validation Loss 32.4679\n",
      "Epoch 3 Batch 3:  Training Loss: 4.9448, Validation Loss 30.6114\n",
      "Epoch 3 Batch 4:  Training Loss: 2.6220, Validation Loss 28.8571\n",
      "Epoch 3 Batch 5:  Training Loss: 3.2418, Validation Loss 27.2907\n",
      "Epoch 3 Batch 6:  Training Loss: 1.4659, Validation Loss 25.9058\n",
      "Epoch 3 Batch 7:  Training Loss: 0.6054, Validation Loss 24.6796\n",
      "Epoch 3 Batch 8:  Training Loss: 0.4392, Validation Loss 23.5922\n",
      "Epoch 3 Batch 9:  Training Loss: 0.8960, Validation Loss 22.6286\n",
      "Epoch 3 Batch 10:  Training Loss: 0.4947, Validation Loss 21.7722\n",
      "Epoch 3 Batch 11:  Training Loss: 0.4382, Validation Loss 21.0094\n",
      "Epoch 3 Batch 12:  Training Loss: 0.2768, Validation Loss 20.3289\n",
      "Epoch 3 Batch 13:  Training Loss: 0.3061, Validation Loss 19.7210\n",
      "Epoch 3 Batch 14:  Training Loss: 0.2511, Validation Loss 19.1771\n",
      "Epoch 4 Batch 0:  Training Loss: 5.0403, Validation Loss 18.4527\n",
      "Epoch 4 Batch 1:  Training Loss: 4.1392, Validation Loss 17.6219\n",
      "Epoch 4 Batch 2:  Training Loss: 7.8244, Validation Loss 16.5530\n",
      "Epoch 4 Batch 3:  Training Loss: 2.1299, Validation Loss 15.4886\n",
      "Epoch 4 Batch 4:  Training Loss: 1.2627, Validation Loss 14.4917\n",
      "Epoch 4 Batch 5:  Training Loss: 3.1430, Validation Loss 13.6139\n",
      "Epoch 4 Batch 6:  Training Loss: 1.5013, Validation Loss 12.8421\n",
      "Epoch 4 Batch 7:  Training Loss: 0.5690, Validation Loss 12.1626\n",
      "Epoch 4 Batch 8:  Training Loss: 0.5665, Validation Loss 11.5632\n",
      "Epoch 4 Batch 9:  Training Loss: 0.5921, Validation Loss 11.0354\n",
      "Epoch 4 Batch 10:  Training Loss: 0.3428, Validation Loss 10.5688\n",
      "Epoch 4 Batch 11:  Training Loss: 0.3763, Validation Loss 10.1553\n",
      "Epoch 4 Batch 12:  Training Loss: 0.2040, Validation Loss 9.7881\n",
      "Epoch 4 Batch 13:  Training Loss: 0.2493, Validation Loss 9.4614\n",
      "Epoch 4 Batch 14:  Training Loss: 0.2305, Validation Loss 9.1704\n",
      "Epoch 5 Batch 0:  Training Loss: 2.3021, Validation Loss 8.7908\n",
      "Epoch 5 Batch 1:  Training Loss: 2.0049, Validation Loss 8.3599\n",
      "Epoch 5 Batch 2:  Training Loss: 3.8542, Validation Loss 7.7998\n",
      "Epoch 5 Batch 3:  Training Loss: 0.8057, Validation Loss 7.2574\n",
      "Epoch 5 Batch 4:  Training Loss: 0.5830, Validation Loss 6.7551\n",
      "Epoch 5 Batch 5:  Training Loss: 3.2119, Validation Loss 6.3204\n",
      "Epoch 5 Batch 6:  Training Loss: 1.5772, Validation Loss 5.9398\n",
      "Epoch 5 Batch 7:  Training Loss: 0.5771, Validation Loss 5.6064\n",
      "Epoch 5 Batch 8:  Training Loss: 0.7450, Validation Loss 5.3138\n",
      "Epoch 5 Batch 9:  Training Loss: 0.4081, Validation Loss 5.0573\n",
      "Epoch 5 Batch 10:  Training Loss: 0.2652, Validation Loss 4.8316\n",
      "Epoch 5 Batch 11:  Training Loss: 0.3596, Validation Loss 4.6326\n",
      "Epoch 5 Batch 12:  Training Loss: 0.1730, Validation Loss 4.4565\n",
      "Epoch 5 Batch 13:  Training Loss: 0.2245, Validation Loss 4.3005\n",
      "Epoch 5 Batch 14:  Training Loss: 0.2230, Validation Loss 4.1619\n",
      "Epoch 6 Batch 0:  Training Loss: 1.0181, Validation Loss 3.9859\n",
      "Epoch 6 Batch 1:  Training Loss: 0.9973, Validation Loss 3.7880\n",
      "Epoch 6 Batch 2:  Training Loss: 1.8417, Validation Loss 3.5241\n",
      "Epoch 6 Batch 3:  Training Loss: 0.3280, Validation Loss 3.2787\n",
      "Epoch 6 Batch 4:  Training Loss: 0.2992, Validation Loss 3.0541\n",
      "Epoch 6 Batch 5:  Training Loss: 3.2840, Validation Loss 2.8647\n",
      "Epoch 6 Batch 6:  Training Loss: 1.5896, Validation Loss 2.6994\n",
      "Epoch 6 Batch 7:  Training Loss: 0.5797, Validation Loss 2.5551\n",
      "Epoch 6 Batch 8:  Training Loss: 0.7562, Validation Loss 2.4289\n",
      "Epoch 6 Batch 9:  Training Loss: 0.4054, Validation Loss 2.3189\n",
      "Epoch 6 Batch 10:  Training Loss: 0.2659, Validation Loss 2.2225\n",
      "Epoch 6 Batch 11:  Training Loss: 0.3592, Validation Loss 2.1378\n",
      "Epoch 6 Batch 12:  Training Loss: 0.1746, Validation Loss 2.0631\n",
      "Epoch 6 Batch 13:  Training Loss: 0.2252, Validation Loss 1.9971\n",
      "Epoch 6 Batch 14:  Training Loss: 0.2222, Validation Loss 1.9385\n",
      "Epoch 7 Batch 0:  Training Loss: 0.5080, Validation Loss 1.8660\n",
      "Epoch 7 Batch 1:  Training Loss: 0.5852, Validation Loss 1.7848\n",
      "Epoch 7 Batch 2:  Training Loss: 0.9183, Validation Loss 1.6707\n",
      "Epoch 7 Batch 3:  Training Loss: 0.2352, Validation Loss 1.5710\n",
      "Epoch 7 Batch 4:  Training Loss: 0.2059, Validation Loss 1.4808\n",
      "Epoch 7 Batch 5:  Training Loss: 3.3266, Validation Loss 1.4077\n",
      "Epoch 7 Batch 6:  Training Loss: 1.5697, Validation Loss 1.3441\n",
      "Epoch 7 Batch 7:  Training Loss: 0.5709, Validation Loss 1.2887\n",
      "Epoch 7 Batch 8:  Training Loss: 0.7062, Validation Loss 1.2402\n",
      "Epoch 7 Batch 9:  Training Loss: 0.4422, Validation Loss 1.1983\n",
      "Epoch 7 Batch 10:  Training Loss: 0.2810, Validation Loss 1.1618\n",
      "Epoch 7 Batch 11:  Training Loss: 0.3595, Validation Loss 1.1299\n",
      "Epoch 7 Batch 12:  Training Loss: 0.1803, Validation Loss 1.1018\n",
      "Epoch 7 Batch 13:  Training Loss: 0.2281, Validation Loss 1.0769\n",
      "Epoch 7 Batch 14:  Training Loss: 0.2217, Validation Loss 1.0549\n",
      "Epoch 8 Batch 0:  Training Loss: 0.3389, Validation Loss 1.0279\n",
      "Epoch 8 Batch 1:  Training Loss: 0.4409, Validation Loss 0.9972\n",
      "Epoch 8 Batch 2:  Training Loss: 0.5279, Validation Loss 0.9487\n",
      "Epoch 8 Batch 3:  Training Loss: 0.2652, Validation Loss 0.9103\n",
      "Epoch 8 Batch 4:  Training Loss: 0.1849, Validation Loss 0.8762\n",
      "Epoch 8 Batch 5:  Training Loss: 3.3462, Validation Loss 0.8508\n",
      "Epoch 8 Batch 6:  Training Loss: 1.5618, Validation Loss 0.8286\n",
      "Epoch 8 Batch 7:  Training Loss: 0.5654, Validation Loss 0.8092\n",
      "Epoch 8 Batch 8:  Training Loss: 0.6892, Validation Loss 0.7922\n",
      "Epoch 8 Batch 9:  Training Loss: 0.4458, Validation Loss 0.7778\n",
      "Epoch 8 Batch 10:  Training Loss: 0.2813, Validation Loss 0.7655\n",
      "Epoch 8 Batch 11:  Training Loss: 0.3560, Validation Loss 0.7547\n",
      "Epoch 8 Batch 12:  Training Loss: 0.1794, Validation Loss 0.7452\n",
      "Epoch 8 Batch 13:  Training Loss: 0.2259, Validation Loss 0.7368\n",
      "Epoch 8 Batch 14:  Training Loss: 0.2197, Validation Loss 0.7294\n",
      "Epoch 9 Batch 0:  Training Loss: 0.2934, Validation Loss 0.7201\n",
      "Epoch 9 Batch 1:  Training Loss: 0.3966, Validation Loss 0.7088\n",
      "Epoch 9 Batch 2:  Training Loss: 0.3706, Validation Loss 0.6860\n",
      "Epoch 9 Batch 3:  Training Loss: 0.3073, Validation Loss 0.6708\n",
      "Epoch 9 Batch 4:  Training Loss: 0.1843, Validation Loss 0.6577\n",
      "Epoch 9 Batch 5:  Training Loss: 3.3406, Validation Loss 0.6497\n",
      "Epoch 9 Batch 6:  Training Loss: 1.5606, Validation Loss 0.6428\n",
      "Epoch 9 Batch 7:  Training Loss: 0.5614, Validation Loss 0.6368\n",
      "Epoch 9 Batch 8:  Training Loss: 0.6894, Validation Loss 0.6316\n",
      "Epoch 9 Batch 9:  Training Loss: 0.4312, Validation Loss 0.6275\n",
      "Epoch 9 Batch 10:  Training Loss: 0.2746, Validation Loss 0.6241\n",
      "Epoch 9 Batch 11:  Training Loss: 0.3506, Validation Loss 0.6212\n",
      "Epoch 9 Batch 12:  Training Loss: 0.1762, Validation Loss 0.6188\n",
      "Epoch 9 Batch 13:  Training Loss: 0.2222, Validation Loss 0.6167\n",
      "Epoch 9 Batch 14:  Training Loss: 0.2174, Validation Loss 0.6148\n",
      "Epoch 10 Batch 0:  Training Loss: 0.2842, Validation Loss 0.6121\n",
      "Epoch 10 Batch 1:  Training Loss: 0.3841, Validation Loss 0.6080\n",
      "Epoch 10 Batch 2:  Training Loss: 0.3102, Validation Loss 0.5948\n",
      "Epoch 10 Batch 3:  Training Loss: 0.3323, Validation Loss 0.5879\n",
      "Epoch 10 Batch 4:  Training Loss: 0.1860, Validation Loss 0.5823\n",
      "Epoch 10 Batch 5:  Training Loss: 3.3118, Validation Loss 0.5806\n",
      "Epoch 10 Batch 6:  Training Loss: 1.5554, Validation Loss 0.5793\n",
      "Epoch 10 Batch 7:  Training Loss: 0.5561, Validation Loss 0.5782\n",
      "Epoch 10 Batch 8:  Training Loss: 0.6815, Validation Loss 0.5774\n",
      "Epoch 10 Batch 9:  Training Loss: 0.4210, Validation Loss 0.5772\n",
      "Epoch 10 Batch 10:  Training Loss: 0.2697, Validation Loss 0.5773\n",
      "Epoch 10 Batch 11:  Training Loss: 0.3451, Validation Loss 0.5775\n",
      "Epoch 10 Batch 12:  Training Loss: 0.1742, Validation Loss 0.5779\n",
      "Epoch 10 Batch 13:  Training Loss: 0.2195, Validation Loss 0.5783\n",
      "Epoch 10 Batch 14:  Training Loss: 0.2154, Validation Loss 0.5787\n",
      "Epoch 11 Batch 0:  Training Loss: 0.2830, Validation Loss 0.5785\n",
      "Epoch 11 Batch 1:  Training Loss: 0.3807, Validation Loss 0.5771\n",
      "Epoch 11 Batch 2:  Training Loss: 0.2902, Validation Loss 0.5671\n",
      "Epoch 11 Batch 3:  Training Loss: 0.3416, Validation Loss 0.5631\n",
      "Epoch 11 Batch 4:  Training Loss: 0.1865, Validation Loss 0.5601\n",
      "Epoch 11 Batch 5:  Training Loss: 3.2684, Validation Loss 0.5606\n",
      "Epoch 11 Batch 6:  Training Loss: 1.5458, Validation Loss 0.5614\n",
      "Epoch 11 Batch 7:  Training Loss: 0.5495, Validation Loss 0.5622\n",
      "Epoch 11 Batch 8:  Training Loss: 0.6668, Validation Loss 0.5630\n",
      "Epoch 11 Batch 9:  Training Loss: 0.4125, Validation Loss 0.5643\n",
      "Epoch 11 Batch 10:  Training Loss: 0.2657, Validation Loss 0.5659\n",
      "Epoch 11 Batch 11:  Training Loss: 0.3401, Validation Loss 0.5674\n",
      "Epoch 11 Batch 12:  Training Loss: 0.1728, Validation Loss 0.5689\n",
      "Epoch 11 Batch 13:  Training Loss: 0.2172, Validation Loss 0.5704\n",
      "Epoch 11 Batch 14:  Training Loss: 0.2135, Validation Loss 0.5717\n",
      "Epoch 12 Batch 0:  Training Loss: 0.2834, Validation Loss 0.5725\n",
      "Epoch 12 Batch 1:  Training Loss: 0.3802, Validation Loss 0.5719\n",
      "Epoch 12 Batch 2:  Training Loss: 0.2861, Validation Loss 0.5628\n",
      "Epoch 12 Batch 3:  Training Loss: 0.3427, Validation Loss 0.5596\n",
      "Epoch 12 Batch 4:  Training Loss: 0.1861, Validation Loss 0.5574\n",
      "Epoch 12 Batch 5:  Training Loss: 3.2188, Validation Loss 0.5587\n",
      "Epoch 12 Batch 6:  Training Loss: 1.5336, Validation Loss 0.5603\n",
      "Epoch 12 Batch 7:  Training Loss: 0.5417, Validation Loss 0.5619\n",
      "Epoch 12 Batch 8:  Training Loss: 0.6524, Validation Loss 0.5633\n",
      "Epoch 12 Batch 9:  Training Loss: 0.4021, Validation Loss 0.5653\n",
      "Epoch 12 Batch 10:  Training Loss: 0.2607, Validation Loss 0.5674\n",
      "Epoch 12 Batch 11:  Training Loss: 0.3347, Validation Loss 0.5694\n",
      "Epoch 12 Batch 12:  Training Loss: 0.1714, Validation Loss 0.5713\n",
      "Epoch 12 Batch 13:  Training Loss: 0.2150, Validation Loss 0.5732\n",
      "Epoch 12 Batch 14:  Training Loss: 0.2116, Validation Loss 0.5748\n",
      "Epoch 13 Batch 0:  Training Loss: 0.2843, Validation Loss 0.5759\n",
      "Epoch 13 Batch 1:  Training Loss: 0.3805, Validation Loss 0.5756\n",
      "Epoch 13 Batch 2:  Training Loss: 0.2858, Validation Loss 0.5662\n",
      "Epoch 13 Batch 3:  Training Loss: 0.3412, Validation Loss 0.5632\n",
      "Epoch 13 Batch 4:  Training Loss: 0.1854, Validation Loss 0.5611\n",
      "Epoch 13 Batch 5:  Training Loss: 3.1667, Validation Loss 0.5625\n",
      "Epoch 13 Batch 6:  Training Loss: 1.5197, Validation Loss 0.5642\n",
      "Epoch 13 Batch 7:  Training Loss: 0.5341, Validation Loss 0.5659\n",
      "Epoch 13 Batch 8:  Training Loss: 0.6374, Validation Loss 0.5673\n",
      "Epoch 13 Batch 9:  Training Loss: 0.3923, Validation Loss 0.5692\n",
      "Epoch 13 Batch 10:  Training Loss: 0.2558, Validation Loss 0.5712\n",
      "Epoch 13 Batch 11:  Training Loss: 0.3297, Validation Loss 0.5732\n",
      "Epoch 13 Batch 12:  Training Loss: 0.1702, Validation Loss 0.5751\n",
      "Epoch 13 Batch 13:  Training Loss: 0.2131, Validation Loss 0.5768\n",
      "Epoch 13 Batch 14:  Training Loss: 0.2100, Validation Loss 0.5783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 0:  Training Loss: 0.2852, Validation Loss 0.5792\n",
      "Epoch 14 Batch 1:  Training Loss: 0.3809, Validation Loss 0.5786\n",
      "Epoch 14 Batch 2:  Training Loss: 0.2850, Validation Loss 0.5687\n",
      "Epoch 14 Batch 3:  Training Loss: 0.3395, Validation Loss 0.5655\n",
      "Epoch 14 Batch 4:  Training Loss: 0.1847, Validation Loss 0.5632\n",
      "Epoch 14 Batch 5:  Training Loss: 3.1134, Validation Loss 0.5646\n",
      "Epoch 14 Batch 6:  Training Loss: 1.5033, Validation Loss 0.5663\n",
      "Epoch 14 Batch 7:  Training Loss: 0.5272, Validation Loss 0.5678\n",
      "Epoch 14 Batch 8:  Training Loss: 0.6220, Validation Loss 0.5692\n",
      "Epoch 14 Batch 9:  Training Loss: 0.3842, Validation Loss 0.5709\n",
      "Epoch 14 Batch 10:  Training Loss: 0.2514, Validation Loss 0.5727\n",
      "Epoch 14 Batch 11:  Training Loss: 0.3259, Validation Loss 0.5743\n",
      "Epoch 14 Batch 12:  Training Loss: 0.1692, Validation Loss 0.5759\n",
      "Epoch 14 Batch 13:  Training Loss: 0.2117, Validation Loss 0.5773\n",
      "Epoch 14 Batch 14:  Training Loss: 0.2081, Validation Loss 0.5787\n",
      "Epoch 15 Batch 0:  Training Loss: 0.2854, Validation Loss 0.5795\n",
      "Epoch 15 Batch 1:  Training Loss: 0.3814, Validation Loss 0.5788\n",
      "Epoch 15 Batch 2:  Training Loss: 0.2833, Validation Loss 0.5685\n",
      "Epoch 15 Batch 3:  Training Loss: 0.3380, Validation Loss 0.5651\n",
      "Epoch 15 Batch 4:  Training Loss: 0.1838, Validation Loss 0.5626\n",
      "Epoch 15 Batch 5:  Training Loss: 3.0599, Validation Loss 0.5640\n",
      "Epoch 15 Batch 6:  Training Loss: 1.4868, Validation Loss 0.5656\n",
      "Epoch 15 Batch 7:  Training Loss: 0.5205, Validation Loss 0.5672\n",
      "Epoch 15 Batch 8:  Training Loss: 0.6075, Validation Loss 0.5685\n",
      "Epoch 15 Batch 9:  Training Loss: 0.3765, Validation Loss 0.5702\n",
      "Epoch 15 Batch 10:  Training Loss: 0.2470, Validation Loss 0.5720\n",
      "Epoch 15 Batch 11:  Training Loss: 0.3223, Validation Loss 0.5737\n",
      "Epoch 15 Batch 12:  Training Loss: 0.1678, Validation Loss 0.5752\n",
      "Epoch 15 Batch 13:  Training Loss: 0.2101, Validation Loss 0.5765\n",
      "Epoch 15 Batch 14:  Training Loss: 0.2057, Validation Loss 0.5777\n",
      "Epoch 16 Batch 0:  Training Loss: 0.2854, Validation Loss 0.5784\n",
      "Epoch 16 Batch 1:  Training Loss: 0.3821, Validation Loss 0.5776\n",
      "Epoch 16 Batch 2:  Training Loss: 0.2812, Validation Loss 0.5670\n",
      "Epoch 16 Batch 3:  Training Loss: 0.3366, Validation Loss 0.5636\n",
      "Epoch 16 Batch 4:  Training Loss: 0.1828, Validation Loss 0.5610\n",
      "Epoch 16 Batch 5:  Training Loss: 3.0065, Validation Loss 0.5623\n",
      "Epoch 16 Batch 6:  Training Loss: 1.4702, Validation Loss 0.5640\n",
      "Epoch 16 Batch 7:  Training Loss: 0.5133, Validation Loss 0.5655\n",
      "Epoch 16 Batch 8:  Training Loss: 0.5960, Validation Loss 0.5667\n",
      "Epoch 16 Batch 9:  Training Loss: 0.3665, Validation Loss 0.5682\n",
      "Epoch 16 Batch 10:  Training Loss: 0.2426, Validation Loss 0.5697\n",
      "Epoch 16 Batch 11:  Training Loss: 0.3187, Validation Loss 0.5710\n",
      "Epoch 16 Batch 12:  Training Loss: 0.1667, Validation Loss 0.5722\n",
      "Epoch 16 Batch 13:  Training Loss: 0.2090, Validation Loss 0.5733\n",
      "Epoch 16 Batch 14:  Training Loss: 0.2038, Validation Loss 0.5742\n",
      "Epoch 17 Batch 0:  Training Loss: 0.2849, Validation Loss 0.5748\n",
      "Epoch 17 Batch 1:  Training Loss: 0.3825, Validation Loss 0.5738\n",
      "Epoch 17 Batch 2:  Training Loss: 0.2786, Validation Loss 0.5630\n",
      "Epoch 17 Batch 3:  Training Loss: 0.3355, Validation Loss 0.5595\n",
      "Epoch 17 Batch 4:  Training Loss: 0.1820, Validation Loss 0.5568\n",
      "Epoch 17 Batch 5:  Training Loss: 2.9523, Validation Loss 0.5581\n",
      "Epoch 17 Batch 6:  Training Loss: 1.4526, Validation Loss 0.5598\n",
      "Epoch 17 Batch 7:  Training Loss: 0.5058, Validation Loss 0.5613\n",
      "Epoch 17 Batch 8:  Training Loss: 0.5835, Validation Loss 0.5625\n",
      "Epoch 17 Batch 9:  Training Loss: 0.3612, Validation Loss 0.5640\n",
      "Epoch 17 Batch 10:  Training Loss: 0.2402, Validation Loss 0.5653\n",
      "Epoch 17 Batch 11:  Training Loss: 0.3158, Validation Loss 0.5666\n",
      "Epoch 17 Batch 12:  Training Loss: 0.1662, Validation Loss 0.5678\n",
      "Epoch 17 Batch 13:  Training Loss: 0.2085, Validation Loss 0.5689\n",
      "Epoch 17 Batch 14:  Training Loss: 0.2023, Validation Loss 0.5698\n",
      "Epoch 18 Batch 0:  Training Loss: 0.2844, Validation Loss 0.5703\n",
      "Epoch 18 Batch 1:  Training Loss: 0.3824, Validation Loss 0.5692\n",
      "Epoch 18 Batch 2:  Training Loss: 0.2749, Validation Loss 0.5583\n",
      "Epoch 18 Batch 3:  Training Loss: 0.3343, Validation Loss 0.5548\n",
      "Epoch 18 Batch 4:  Training Loss: 0.1814, Validation Loss 0.5522\n",
      "Epoch 18 Batch 5:  Training Loss: 2.8989, Validation Loss 0.5536\n",
      "Epoch 18 Batch 6:  Training Loss: 1.4367, Validation Loss 0.5553\n",
      "Epoch 18 Batch 7:  Training Loss: 0.4990, Validation Loss 0.5568\n",
      "Epoch 18 Batch 8:  Training Loss: 0.5742, Validation Loss 0.5580\n",
      "Epoch 18 Batch 9:  Training Loss: 0.3552, Validation Loss 0.5594\n",
      "Epoch 18 Batch 10:  Training Loss: 0.2375, Validation Loss 0.5607\n",
      "Epoch 18 Batch 11:  Training Loss: 0.3130, Validation Loss 0.5620\n",
      "Epoch 18 Batch 12:  Training Loss: 0.1656, Validation Loss 0.5631\n",
      "Epoch 18 Batch 13:  Training Loss: 0.2080, Validation Loss 0.5640\n",
      "Epoch 18 Batch 14:  Training Loss: 0.2010, Validation Loss 0.5649\n",
      "Epoch 19 Batch 0:  Training Loss: 0.2836, Validation Loss 0.5654\n",
      "Epoch 19 Batch 1:  Training Loss: 0.3823, Validation Loss 0.5643\n",
      "Epoch 19 Batch 2:  Training Loss: 0.2712, Validation Loss 0.5533\n",
      "Epoch 19 Batch 3:  Training Loss: 0.3327, Validation Loss 0.5500\n",
      "Epoch 19 Batch 4:  Training Loss: 0.1808, Validation Loss 0.5474\n",
      "Epoch 19 Batch 5:  Training Loss: 2.8463, Validation Loss 0.5488\n",
      "Epoch 19 Batch 6:  Training Loss: 1.4225, Validation Loss 0.5505\n",
      "Epoch 19 Batch 7:  Training Loss: 0.4933, Validation Loss 0.5521\n",
      "Epoch 19 Batch 8:  Training Loss: 0.5673, Validation Loss 0.5533\n",
      "Epoch 19 Batch 9:  Training Loss: 0.3492, Validation Loss 0.5546\n",
      "Epoch 19 Batch 10:  Training Loss: 0.2351, Validation Loss 0.5560\n",
      "Epoch 19 Batch 11:  Training Loss: 0.3102, Validation Loss 0.5572\n",
      "Epoch 19 Batch 12:  Training Loss: 0.1650, Validation Loss 0.5583\n",
      "Epoch 19 Batch 13:  Training Loss: 0.2075, Validation Loss 0.5592\n",
      "Epoch 19 Batch 14:  Training Loss: 0.1999, Validation Loss 0.5600\n",
      "Epoch 20 Batch 0:  Training Loss: 0.2825, Validation Loss 0.5605\n",
      "Epoch 20 Batch 1:  Training Loss: 0.3821, Validation Loss 0.5594\n",
      "Epoch 20 Batch 2:  Training Loss: 0.2676, Validation Loss 0.5484\n",
      "Epoch 20 Batch 3:  Training Loss: 0.3308, Validation Loss 0.5452\n",
      "Epoch 20 Batch 4:  Training Loss: 0.1802, Validation Loss 0.5426\n",
      "Epoch 20 Batch 5:  Training Loss: 2.7942, Validation Loss 0.5441\n",
      "Epoch 20 Batch 6:  Training Loss: 1.4093, Validation Loss 0.5458\n",
      "Epoch 20 Batch 7:  Training Loss: 0.4882, Validation Loss 0.5474\n",
      "Epoch 20 Batch 8:  Training Loss: 0.5613, Validation Loss 0.5487\n",
      "Epoch 20 Batch 9:  Training Loss: 0.3435, Validation Loss 0.5500\n",
      "Epoch 20 Batch 10:  Training Loss: 0.2330, Validation Loss 0.5513\n",
      "Epoch 20 Batch 11:  Training Loss: 0.3075, Validation Loss 0.5524\n",
      "Epoch 20 Batch 12:  Training Loss: 0.1646, Validation Loss 0.5535\n",
      "Epoch 20 Batch 13:  Training Loss: 0.2072, Validation Loss 0.5544\n",
      "Epoch 20 Batch 14:  Training Loss: 0.1989, Validation Loss 0.5551\n",
      "Epoch 21 Batch 0:  Training Loss: 0.2815, Validation Loss 0.5556\n",
      "Epoch 21 Batch 1:  Training Loss: 0.3815, Validation Loss 0.5545\n",
      "Epoch 21 Batch 2:  Training Loss: 0.2639, Validation Loss 0.5435\n",
      "Epoch 21 Batch 3:  Training Loss: 0.3285, Validation Loss 0.5403\n",
      "Epoch 21 Batch 4:  Training Loss: 0.1797, Validation Loss 0.5378\n",
      "Epoch 21 Batch 5:  Training Loss: 2.7425, Validation Loss 0.5393\n",
      "Epoch 21 Batch 6:  Training Loss: 1.3961, Validation Loss 0.5411\n",
      "Epoch 21 Batch 7:  Training Loss: 0.4832, Validation Loss 0.5427\n",
      "Epoch 21 Batch 8:  Training Loss: 0.5558, Validation Loss 0.5440\n",
      "Epoch 21 Batch 9:  Training Loss: 0.3390, Validation Loss 0.5453\n",
      "Epoch 21 Batch 10:  Training Loss: 0.2313, Validation Loss 0.5465\n",
      "Epoch 21 Batch 11:  Training Loss: 0.3047, Validation Loss 0.5477\n",
      "Epoch 21 Batch 12:  Training Loss: 0.1644, Validation Loss 0.5487\n",
      "Epoch 21 Batch 13:  Training Loss: 0.2069, Validation Loss 0.5495\n",
      "Epoch 21 Batch 14:  Training Loss: 0.1982, Validation Loss 0.5502\n",
      "Epoch 22 Batch 0:  Training Loss: 0.2805, Validation Loss 0.5508\n",
      "Epoch 22 Batch 1:  Training Loss: 0.3808, Validation Loss 0.5496\n",
      "Epoch 22 Batch 2:  Training Loss: 0.2601, Validation Loss 0.5386\n",
      "Epoch 22 Batch 3:  Training Loss: 0.3259, Validation Loss 0.5355\n",
      "Epoch 22 Batch 4:  Training Loss: 0.1792, Validation Loss 0.5331\n",
      "Epoch 22 Batch 5:  Training Loss: 2.6917, Validation Loss 0.5347\n",
      "Epoch 22 Batch 6:  Training Loss: 1.3838, Validation Loss 0.5364\n",
      "Epoch 22 Batch 7:  Training Loss: 0.4786, Validation Loss 0.5379\n",
      "Epoch 22 Batch 8:  Training Loss: 0.5521, Validation Loss 0.5392\n",
      "Epoch 22 Batch 9:  Training Loss: 0.3338, Validation Loss 0.5405\n",
      "Epoch 22 Batch 10:  Training Loss: 0.2292, Validation Loss 0.5417\n",
      "Epoch 22 Batch 11:  Training Loss: 0.3017, Validation Loss 0.5427\n",
      "Epoch 22 Batch 12:  Training Loss: 0.1638, Validation Loss 0.5437\n",
      "Epoch 22 Batch 13:  Training Loss: 0.2064, Validation Loss 0.5445\n",
      "Epoch 22 Batch 14:  Training Loss: 0.1975, Validation Loss 0.5452\n",
      "Epoch 23 Batch 0:  Training Loss: 0.2797, Validation Loss 0.5457\n",
      "Epoch 23 Batch 1:  Training Loss: 0.3801, Validation Loss 0.5446\n",
      "Epoch 23 Batch 2:  Training Loss: 0.2560, Validation Loss 0.5335\n",
      "Epoch 23 Batch 3:  Training Loss: 0.3234, Validation Loss 0.5305\n",
      "Epoch 23 Batch 4:  Training Loss: 0.1788, Validation Loss 0.5280\n",
      "Epoch 23 Batch 5:  Training Loss: 2.6414, Validation Loss 0.5296\n",
      "Epoch 23 Batch 6:  Training Loss: 1.3732, Validation Loss 0.5313\n",
      "Epoch 23 Batch 7:  Training Loss: 0.4740, Validation Loss 0.5329\n",
      "Epoch 23 Batch 8:  Training Loss: 0.5478, Validation Loss 0.5342\n",
      "Epoch 23 Batch 9:  Training Loss: 0.3292, Validation Loss 0.5355\n",
      "Epoch 23 Batch 10:  Training Loss: 0.2275, Validation Loss 0.5367\n",
      "Epoch 23 Batch 11:  Training Loss: 0.2988, Validation Loss 0.5378\n",
      "Epoch 23 Batch 12:  Training Loss: 0.1635, Validation Loss 0.5387\n",
      "Epoch 23 Batch 13:  Training Loss: 0.2060, Validation Loss 0.5395\n",
      "Epoch 23 Batch 14:  Training Loss: 0.1969, Validation Loss 0.5401\n",
      "Epoch 24 Batch 0:  Training Loss: 0.2789, Validation Loss 0.5406\n",
      "Epoch 24 Batch 1:  Training Loss: 0.3794, Validation Loss 0.5394\n",
      "Epoch 24 Batch 2:  Training Loss: 0.2522, Validation Loss 0.5284\n",
      "Epoch 24 Batch 3:  Training Loss: 0.3206, Validation Loss 0.5254\n",
      "Epoch 24 Batch 4:  Training Loss: 0.1786, Validation Loss 0.5230\n",
      "Epoch 24 Batch 5:  Training Loss: 2.5917, Validation Loss 0.5246\n",
      "Epoch 24 Batch 6:  Training Loss: 1.3631, Validation Loss 0.5263\n",
      "Epoch 24 Batch 7:  Training Loss: 0.4695, Validation Loss 0.5279\n",
      "Epoch 24 Batch 8:  Training Loss: 0.5428, Validation Loss 0.5294\n",
      "Epoch 24 Batch 9:  Training Loss: 0.3251, Validation Loss 0.5307\n",
      "Epoch 24 Batch 10:  Training Loss: 0.2258, Validation Loss 0.5319\n",
      "Epoch 24 Batch 11:  Training Loss: 0.2959, Validation Loss 0.5329\n",
      "Epoch 24 Batch 12:  Training Loss: 0.1631, Validation Loss 0.5339\n",
      "Epoch 24 Batch 13:  Training Loss: 0.2054, Validation Loss 0.5346\n",
      "Epoch 24 Batch 14:  Training Loss: 0.1963, Validation Loss 0.5352\n",
      "Epoch 25 Batch 0:  Training Loss: 0.2782, Validation Loss 0.5358\n",
      "Epoch 25 Batch 1:  Training Loss: 0.3787, Validation Loss 0.5346\n",
      "Epoch 25 Batch 2:  Training Loss: 0.2487, Validation Loss 0.5236\n",
      "Epoch 25 Batch 3:  Training Loss: 0.3175, Validation Loss 0.5206\n",
      "Epoch 25 Batch 4:  Training Loss: 0.1784, Validation Loss 0.5182\n",
      "Epoch 25 Batch 5:  Training Loss: 2.5429, Validation Loss 0.5199\n",
      "Epoch 25 Batch 6:  Training Loss: 1.3541, Validation Loss 0.5216\n",
      "Epoch 25 Batch 7:  Training Loss: 0.4654, Validation Loss 0.5232\n",
      "Epoch 25 Batch 8:  Training Loss: 0.5393, Validation Loss 0.5247\n",
      "Epoch 25 Batch 9:  Training Loss: 0.3200, Validation Loss 0.5261\n",
      "Epoch 25 Batch 10:  Training Loss: 0.2237, Validation Loss 0.5273\n",
      "Epoch 25 Batch 11:  Training Loss: 0.2929, Validation Loss 0.5284\n",
      "Epoch 25 Batch 12:  Training Loss: 0.1625, Validation Loss 0.5293\n",
      "Epoch 25 Batch 13:  Training Loss: 0.2046, Validation Loss 0.5301\n",
      "Epoch 25 Batch 14:  Training Loss: 0.1957, Validation Loss 0.5307\n",
      "Epoch 26 Batch 0:  Training Loss: 0.2775, Validation Loss 0.5312\n",
      "Epoch 26 Batch 1:  Training Loss: 0.3779, Validation Loss 0.5300\n",
      "Epoch 26 Batch 2:  Training Loss: 0.2454, Validation Loss 0.5190\n",
      "Epoch 26 Batch 3:  Training Loss: 0.3143, Validation Loss 0.5161\n",
      "Epoch 26 Batch 4:  Training Loss: 0.1782, Validation Loss 0.5137\n",
      "Epoch 26 Batch 5:  Training Loss: 2.4954, Validation Loss 0.5154\n",
      "Epoch 26 Batch 6:  Training Loss: 1.3451, Validation Loss 0.5173\n",
      "Epoch 26 Batch 7:  Training Loss: 0.4616, Validation Loss 0.5190\n",
      "Epoch 26 Batch 8:  Training Loss: 0.5350, Validation Loss 0.5206\n",
      "Epoch 26 Batch 9:  Training Loss: 0.3157, Validation Loss 0.5221\n",
      "Epoch 26 Batch 10:  Training Loss: 0.2219, Validation Loss 0.5234\n",
      "Epoch 26 Batch 11:  Training Loss: 0.2900, Validation Loss 0.5245\n",
      "Epoch 26 Batch 12:  Training Loss: 0.1621, Validation Loss 0.5255\n",
      "Epoch 26 Batch 13:  Training Loss: 0.2040, Validation Loss 0.5263\n",
      "Epoch 26 Batch 14:  Training Loss: 0.1951, Validation Loss 0.5270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 0:  Training Loss: 0.2766, Validation Loss 0.5275\n",
      "Epoch 27 Batch 1:  Training Loss: 0.3773, Validation Loss 0.5264\n",
      "Epoch 27 Batch 2:  Training Loss: 0.2428, Validation Loss 0.5153\n",
      "Epoch 27 Batch 3:  Training Loss: 0.3108, Validation Loss 0.5124\n",
      "Epoch 27 Batch 4:  Training Loss: 0.1780, Validation Loss 0.5099\n",
      "Epoch 27 Batch 5:  Training Loss: 2.4491, Validation Loss 0.5117\n",
      "Epoch 27 Batch 6:  Training Loss: 1.3362, Validation Loss 0.5136\n",
      "Epoch 27 Batch 7:  Training Loss: 0.4579, Validation Loss 0.5153\n",
      "Epoch 27 Batch 8:  Training Loss: 0.5299, Validation Loss 0.5169\n",
      "Epoch 27 Batch 9:  Training Loss: 0.3120, Validation Loss 0.5184\n",
      "Epoch 27 Batch 10:  Training Loss: 0.2203, Validation Loss 0.5196\n",
      "Epoch 27 Batch 11:  Training Loss: 0.2872, Validation Loss 0.5208\n",
      "Epoch 27 Batch 12:  Training Loss: 0.1617, Validation Loss 0.5218\n",
      "Epoch 27 Batch 13:  Training Loss: 0.2034, Validation Loss 0.5226\n",
      "Epoch 27 Batch 14:  Training Loss: 0.1946, Validation Loss 0.5232\n",
      "Epoch 28 Batch 0:  Training Loss: 0.2758, Validation Loss 0.5238\n",
      "Epoch 28 Batch 1:  Training Loss: 0.3766, Validation Loss 0.5226\n",
      "Epoch 28 Batch 2:  Training Loss: 0.2401, Validation Loss 0.5114\n",
      "Epoch 28 Batch 3:  Training Loss: 0.3074, Validation Loss 0.5085\n",
      "Epoch 28 Batch 4:  Training Loss: 0.1779, Validation Loss 0.5060\n",
      "Epoch 28 Batch 5:  Training Loss: 2.4041, Validation Loss 0.5078\n",
      "Epoch 28 Batch 6:  Training Loss: 1.3281, Validation Loss 0.5096\n",
      "Epoch 28 Batch 7:  Training Loss: 0.4545, Validation Loss 0.5114\n",
      "Epoch 28 Batch 8:  Training Loss: 0.5259, Validation Loss 0.5130\n",
      "Epoch 28 Batch 9:  Training Loss: 0.3076, Validation Loss 0.5144\n",
      "Epoch 28 Batch 10:  Training Loss: 0.2183, Validation Loss 0.5157\n",
      "Epoch 28 Batch 11:  Training Loss: 0.2843, Validation Loss 0.5168\n",
      "Epoch 28 Batch 12:  Training Loss: 0.1612, Validation Loss 0.5178\n",
      "Epoch 28 Batch 13:  Training Loss: 0.2026, Validation Loss 0.5186\n",
      "Epoch 28 Batch 14:  Training Loss: 0.1941, Validation Loss 0.5192\n",
      "Epoch 29 Batch 0:  Training Loss: 0.2750, Validation Loss 0.5198\n",
      "Epoch 29 Batch 1:  Training Loss: 0.3759, Validation Loss 0.5186\n",
      "Epoch 29 Batch 2:  Training Loss: 0.2373, Validation Loss 0.5073\n",
      "Epoch 29 Batch 3:  Training Loss: 0.3041, Validation Loss 0.5045\n",
      "Epoch 29 Batch 4:  Training Loss: 0.1778, Validation Loss 0.5020\n",
      "Epoch 29 Batch 5:  Training Loss: 2.3600, Validation Loss 0.5037\n",
      "Epoch 29 Batch 6:  Training Loss: 1.3202, Validation Loss 0.5055\n",
      "Epoch 29 Batch 7:  Training Loss: 0.4510, Validation Loss 0.5073\n",
      "Epoch 29 Batch 8:  Training Loss: 0.5216, Validation Loss 0.5088\n",
      "Epoch 29 Batch 9:  Training Loss: 0.3035, Validation Loss 0.5103\n",
      "Epoch 29 Batch 10:  Training Loss: 0.2165, Validation Loss 0.5115\n",
      "Epoch 29 Batch 11:  Training Loss: 0.2815, Validation Loss 0.5127\n",
      "Epoch 29 Batch 12:  Training Loss: 0.1607, Validation Loss 0.5136\n",
      "Epoch 29 Batch 13:  Training Loss: 0.2018, Validation Loss 0.5144\n",
      "Epoch 29 Batch 14:  Training Loss: 0.1935, Validation Loss 0.5151\n",
      "Epoch 30 Batch 0:  Training Loss: 0.2742, Validation Loss 0.5157\n",
      "Epoch 30 Batch 1:  Training Loss: 0.3751, Validation Loss 0.5144\n",
      "Epoch 30 Batch 2:  Training Loss: 0.2344, Validation Loss 0.5032\n",
      "Epoch 30 Batch 3:  Training Loss: 0.3009, Validation Loss 0.5003\n",
      "Epoch 30 Batch 4:  Training Loss: 0.1777, Validation Loss 0.4978\n",
      "Epoch 30 Batch 5:  Training Loss: 2.3167, Validation Loss 0.4995\n",
      "Epoch 30 Batch 6:  Training Loss: 1.3121, Validation Loss 0.5013\n",
      "Epoch 30 Batch 7:  Training Loss: 0.4476, Validation Loss 0.5030\n",
      "Epoch 30 Batch 8:  Training Loss: 0.5173, Validation Loss 0.5045\n",
      "Epoch 30 Batch 9:  Training Loss: 0.2995, Validation Loss 0.5059\n",
      "Epoch 30 Batch 10:  Training Loss: 0.2147, Validation Loss 0.5072\n",
      "Epoch 30 Batch 11:  Training Loss: 0.2787, Validation Loss 0.5083\n",
      "Epoch 30 Batch 12:  Training Loss: 0.1602, Validation Loss 0.5092\n",
      "Epoch 30 Batch 13:  Training Loss: 0.2009, Validation Loss 0.5100\n",
      "Epoch 30 Batch 14:  Training Loss: 0.1930, Validation Loss 0.5106\n",
      "Epoch 31 Batch 0:  Training Loss: 0.2734, Validation Loss 0.5112\n",
      "Epoch 31 Batch 1:  Training Loss: 0.3742, Validation Loss 0.5099\n",
      "Epoch 31 Batch 2:  Training Loss: 0.2314, Validation Loss 0.4987\n",
      "Epoch 31 Batch 3:  Training Loss: 0.2979, Validation Loss 0.4959\n",
      "Epoch 31 Batch 4:  Training Loss: 0.1775, Validation Loss 0.4934\n",
      "Epoch 31 Batch 5:  Training Loss: 2.2738, Validation Loss 0.4951\n",
      "Epoch 31 Batch 6:  Training Loss: 1.3042, Validation Loss 0.4970\n",
      "Epoch 31 Batch 7:  Training Loss: 0.4444, Validation Loss 0.4986\n",
      "Epoch 31 Batch 8:  Training Loss: 0.5137, Validation Loss 0.5002\n",
      "Epoch 31 Batch 9:  Training Loss: 0.2951, Validation Loss 0.5017\n",
      "Epoch 31 Batch 10:  Training Loss: 0.2127, Validation Loss 0.5029\n",
      "Epoch 31 Batch 11:  Training Loss: 0.2758, Validation Loss 0.5040\n",
      "Epoch 31 Batch 12:  Training Loss: 0.1596, Validation Loss 0.5050\n",
      "Epoch 31 Batch 13:  Training Loss: 0.2001, Validation Loss 0.5057\n",
      "Epoch 31 Batch 14:  Training Loss: 0.1926, Validation Loss 0.5064\n",
      "Epoch 32 Batch 0:  Training Loss: 0.2726, Validation Loss 0.5069\n",
      "Epoch 32 Batch 1:  Training Loss: 0.3732, Validation Loss 0.5056\n",
      "Epoch 32 Batch 2:  Training Loss: 0.2286, Validation Loss 0.4945\n",
      "Epoch 32 Batch 3:  Training Loss: 0.2948, Validation Loss 0.4917\n",
      "Epoch 32 Batch 4:  Training Loss: 0.1774, Validation Loss 0.4892\n",
      "Epoch 32 Batch 5:  Training Loss: 2.2313, Validation Loss 0.4909\n",
      "Epoch 32 Batch 6:  Training Loss: 1.2961, Validation Loss 0.4927\n",
      "Epoch 32 Batch 7:  Training Loss: 0.4408, Validation Loss 0.4944\n",
      "Epoch 32 Batch 8:  Training Loss: 0.5094, Validation Loss 0.4961\n",
      "Epoch 32 Batch 9:  Training Loss: 0.2914, Validation Loss 0.4975\n",
      "Epoch 32 Batch 10:  Training Loss: 0.2109, Validation Loss 0.4987\n",
      "Epoch 32 Batch 11:  Training Loss: 0.2731, Validation Loss 0.4998\n",
      "Epoch 32 Batch 12:  Training Loss: 0.1591, Validation Loss 0.5007\n",
      "Epoch 32 Batch 13:  Training Loss: 0.1993, Validation Loss 0.5014\n",
      "Epoch 32 Batch 14:  Training Loss: 0.1922, Validation Loss 0.5019\n",
      "Epoch 33 Batch 0:  Training Loss: 0.2717, Validation Loss 0.5025\n",
      "Epoch 33 Batch 1:  Training Loss: 0.3723, Validation Loss 0.5012\n",
      "Epoch 33 Batch 2:  Training Loss: 0.2258, Validation Loss 0.4900\n",
      "Epoch 33 Batch 3:  Training Loss: 0.2919, Validation Loss 0.4873\n",
      "Epoch 33 Batch 4:  Training Loss: 0.1773, Validation Loss 0.4849\n",
      "Epoch 33 Batch 5:  Training Loss: 2.1897, Validation Loss 0.4866\n",
      "Epoch 33 Batch 6:  Training Loss: 1.2881, Validation Loss 0.4884\n",
      "Epoch 33 Batch 7:  Training Loss: 0.4370, Validation Loss 0.4900\n",
      "Epoch 33 Batch 8:  Training Loss: 0.5049, Validation Loss 0.4917\n",
      "Epoch 33 Batch 9:  Training Loss: 0.2880, Validation Loss 0.4932\n",
      "Epoch 33 Batch 10:  Training Loss: 0.2093, Validation Loss 0.4944\n",
      "Epoch 33 Batch 11:  Training Loss: 0.2704, Validation Loss 0.4955\n",
      "Epoch 33 Batch 12:  Training Loss: 0.1586, Validation Loss 0.4964\n",
      "Epoch 33 Batch 13:  Training Loss: 0.1985, Validation Loss 0.4971\n",
      "Epoch 33 Batch 14:  Training Loss: 0.1918, Validation Loss 0.4976\n",
      "Epoch 34 Batch 0:  Training Loss: 0.2710, Validation Loss 0.4982\n",
      "Epoch 34 Batch 1:  Training Loss: 0.3714, Validation Loss 0.4969\n",
      "Epoch 34 Batch 2:  Training Loss: 0.2232, Validation Loss 0.4858\n",
      "Epoch 34 Batch 3:  Training Loss: 0.2888, Validation Loss 0.4832\n",
      "Epoch 34 Batch 4:  Training Loss: 0.1772, Validation Loss 0.4807\n",
      "Epoch 34 Batch 5:  Training Loss: 2.1492, Validation Loss 0.4824\n",
      "Epoch 34 Batch 6:  Training Loss: 1.2804, Validation Loss 0.4842\n",
      "Epoch 34 Batch 7:  Training Loss: 0.4333, Validation Loss 0.4859\n",
      "Epoch 34 Batch 8:  Training Loss: 0.5007, Validation Loss 0.4876\n",
      "Epoch 34 Batch 9:  Training Loss: 0.2844, Validation Loss 0.4890\n",
      "Epoch 34 Batch 10:  Training Loss: 0.2075, Validation Loss 0.4903\n",
      "Epoch 34 Batch 11:  Training Loss: 0.2677, Validation Loss 0.4914\n",
      "Epoch 34 Batch 12:  Training Loss: 0.1580, Validation Loss 0.4923\n",
      "Epoch 34 Batch 13:  Training Loss: 0.1975, Validation Loss 0.4930\n",
      "Epoch 34 Batch 14:  Training Loss: 0.1913, Validation Loss 0.4935\n",
      "Epoch 35 Batch 0:  Training Loss: 0.2702, Validation Loss 0.4941\n",
      "Epoch 35 Batch 1:  Training Loss: 0.3706, Validation Loss 0.4928\n",
      "Epoch 35 Batch 2:  Training Loss: 0.2205, Validation Loss 0.4818\n",
      "Epoch 35 Batch 3:  Training Loss: 0.2858, Validation Loss 0.4792\n",
      "Epoch 35 Batch 4:  Training Loss: 0.1772, Validation Loss 0.4768\n",
      "Epoch 35 Batch 5:  Training Loss: 2.1096, Validation Loss 0.4785\n",
      "Epoch 35 Batch 6:  Training Loss: 1.2734, Validation Loss 0.4803\n",
      "Epoch 35 Batch 7:  Training Loss: 0.4296, Validation Loss 0.4820\n",
      "Epoch 35 Batch 8:  Training Loss: 0.4968, Validation Loss 0.4837\n",
      "Epoch 35 Batch 9:  Training Loss: 0.2807, Validation Loss 0.4852\n",
      "Epoch 35 Batch 10:  Training Loss: 0.2057, Validation Loss 0.4864\n",
      "Epoch 35 Batch 11:  Training Loss: 0.2650, Validation Loss 0.4875\n",
      "Epoch 35 Batch 12:  Training Loss: 0.1574, Validation Loss 0.4884\n",
      "Epoch 35 Batch 13:  Training Loss: 0.1964, Validation Loss 0.4891\n",
      "Epoch 35 Batch 14:  Training Loss: 0.1908, Validation Loss 0.4897\n",
      "Epoch 36 Batch 0:  Training Loss: 0.2694, Validation Loss 0.4903\n",
      "Epoch 36 Batch 1:  Training Loss: 0.3698, Validation Loss 0.4890\n",
      "Epoch 36 Batch 2:  Training Loss: 0.2180, Validation Loss 0.4780\n",
      "Epoch 36 Batch 3:  Training Loss: 0.2827, Validation Loss 0.4755\n",
      "Epoch 36 Batch 4:  Training Loss: 0.1771, Validation Loss 0.4730\n",
      "Epoch 36 Batch 5:  Training Loss: 2.0709, Validation Loss 0.4747\n",
      "Epoch 36 Batch 6:  Training Loss: 1.2666, Validation Loss 0.4765\n",
      "Epoch 36 Batch 7:  Training Loss: 0.4259, Validation Loss 0.4782\n",
      "Epoch 36 Batch 8:  Training Loss: 0.4924, Validation Loss 0.4799\n",
      "Epoch 36 Batch 9:  Training Loss: 0.2775, Validation Loss 0.4813\n",
      "Epoch 36 Batch 10:  Training Loss: 0.2040, Validation Loss 0.4826\n",
      "Epoch 36 Batch 11:  Training Loss: 0.2623, Validation Loss 0.4837\n",
      "Epoch 36 Batch 12:  Training Loss: 0.1568, Validation Loss 0.4846\n",
      "Epoch 36 Batch 13:  Training Loss: 0.1953, Validation Loss 0.4853\n",
      "Epoch 36 Batch 14:  Training Loss: 0.1902, Validation Loss 0.4858\n",
      "Epoch 37 Batch 0:  Training Loss: 0.2687, Validation Loss 0.4865\n",
      "Epoch 37 Batch 1:  Training Loss: 0.3689, Validation Loss 0.4851\n",
      "Epoch 37 Batch 2:  Training Loss: 0.2155, Validation Loss 0.4742\n",
      "Epoch 37 Batch 3:  Training Loss: 0.2797, Validation Loss 0.4717\n",
      "Epoch 37 Batch 4:  Training Loss: 0.1771, Validation Loss 0.4692\n",
      "Epoch 37 Batch 5:  Training Loss: 2.0333, Validation Loss 0.4709\n",
      "Epoch 37 Batch 6:  Training Loss: 1.2601, Validation Loss 0.4727\n",
      "Epoch 37 Batch 7:  Training Loss: 0.4224, Validation Loss 0.4744\n",
      "Epoch 37 Batch 8:  Training Loss: 0.4885, Validation Loss 0.4760\n",
      "Epoch 37 Batch 9:  Training Loss: 0.2743, Validation Loss 0.4775\n",
      "Epoch 37 Batch 10:  Training Loss: 0.2022, Validation Loss 0.4787\n",
      "Epoch 37 Batch 11:  Training Loss: 0.2597, Validation Loss 0.4798\n",
      "Epoch 37 Batch 12:  Training Loss: 0.1562, Validation Loss 0.4807\n",
      "Epoch 37 Batch 13:  Training Loss: 0.1941, Validation Loss 0.4814\n",
      "Epoch 37 Batch 14:  Training Loss: 0.1896, Validation Loss 0.4820\n",
      "Epoch 38 Batch 0:  Training Loss: 0.2680, Validation Loss 0.4826\n",
      "Epoch 38 Batch 1:  Training Loss: 0.3682, Validation Loss 0.4812\n",
      "Epoch 38 Batch 2:  Training Loss: 0.2130, Validation Loss 0.4703\n",
      "Epoch 38 Batch 3:  Training Loss: 0.2767, Validation Loss 0.4678\n",
      "Epoch 38 Batch 4:  Training Loss: 0.1771, Validation Loss 0.4654\n",
      "Epoch 38 Batch 5:  Training Loss: 1.9965, Validation Loss 0.4670\n",
      "Epoch 38 Batch 6:  Training Loss: 1.2537, Validation Loss 0.4688\n",
      "Epoch 38 Batch 7:  Training Loss: 0.4189, Validation Loss 0.4704\n",
      "Epoch 38 Batch 8:  Training Loss: 0.4840, Validation Loss 0.4720\n",
      "Epoch 38 Batch 9:  Training Loss: 0.2715, Validation Loss 0.4734\n",
      "Epoch 38 Batch 10:  Training Loss: 0.2005, Validation Loss 0.4746\n",
      "Epoch 38 Batch 11:  Training Loss: 0.2571, Validation Loss 0.4757\n",
      "Epoch 38 Batch 12:  Training Loss: 0.1555, Validation Loss 0.4765\n",
      "Epoch 38 Batch 13:  Training Loss: 0.1929, Validation Loss 0.4772\n",
      "Epoch 38 Batch 14:  Training Loss: 0.1890, Validation Loss 0.4777\n",
      "Epoch 39 Batch 0:  Training Loss: 0.2674, Validation Loss 0.4783\n",
      "Epoch 39 Batch 1:  Training Loss: 0.3673, Validation Loss 0.4769\n",
      "Epoch 39 Batch 2:  Training Loss: 0.2101, Validation Loss 0.4661\n",
      "Epoch 39 Batch 3:  Training Loss: 0.2741, Validation Loss 0.4637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 4:  Training Loss: 0.1771, Validation Loss 0.4613\n",
      "Epoch 39 Batch 5:  Training Loss: 1.9609, Validation Loss 0.4629\n",
      "Epoch 39 Batch 6:  Training Loss: 1.2475, Validation Loss 0.4647\n",
      "Epoch 39 Batch 7:  Training Loss: 0.4156, Validation Loss 0.4664\n",
      "Epoch 39 Batch 8:  Training Loss: 0.4797, Validation Loss 0.4679\n",
      "Epoch 39 Batch 9:  Training Loss: 0.2683, Validation Loss 0.4693\n",
      "Epoch 39 Batch 10:  Training Loss: 0.1985, Validation Loss 0.4705\n",
      "Epoch 39 Batch 11:  Training Loss: 0.2544, Validation Loss 0.4715\n",
      "Epoch 39 Batch 12:  Training Loss: 0.1546, Validation Loss 0.4723\n",
      "Epoch 39 Batch 13:  Training Loss: 0.1914, Validation Loss 0.4730\n",
      "Epoch 39 Batch 14:  Training Loss: 0.1883, Validation Loss 0.4734\n",
      "Epoch 40 Batch 0:  Training Loss: 0.2668, Validation Loss 0.4741\n",
      "Epoch 40 Batch 1:  Training Loss: 0.3666, Validation Loss 0.4727\n",
      "Epoch 40 Batch 2:  Training Loss: 0.2074, Validation Loss 0.4620\n",
      "Epoch 40 Batch 3:  Training Loss: 0.2716, Validation Loss 0.4597\n",
      "Epoch 40 Batch 4:  Training Loss: 0.1770, Validation Loss 0.4574\n",
      "Epoch 40 Batch 5:  Training Loss: 1.9266, Validation Loss 0.4590\n",
      "Epoch 40 Batch 6:  Training Loss: 1.2412, Validation Loss 0.4608\n",
      "Epoch 40 Batch 7:  Training Loss: 0.4121, Validation Loss 0.4625\n",
      "Epoch 40 Batch 8:  Training Loss: 0.4742, Validation Loss 0.4641\n",
      "Epoch 40 Batch 9:  Training Loss: 0.2657, Validation Loss 0.4654\n",
      "Epoch 40 Batch 10:  Training Loss: 0.1968, Validation Loss 0.4666\n",
      "Epoch 40 Batch 11:  Training Loss: 0.2519, Validation Loss 0.4676\n",
      "Epoch 40 Batch 12:  Training Loss: 0.1538, Validation Loss 0.4684\n",
      "Epoch 40 Batch 13:  Training Loss: 0.1900, Validation Loss 0.4691\n",
      "Epoch 40 Batch 14:  Training Loss: 0.1876, Validation Loss 0.4696\n",
      "Epoch 41 Batch 0:  Training Loss: 0.2661, Validation Loss 0.4702\n",
      "Epoch 41 Batch 1:  Training Loss: 0.3659, Validation Loss 0.4689\n",
      "Epoch 41 Batch 2:  Training Loss: 0.2048, Validation Loss 0.4583\n",
      "Epoch 41 Batch 3:  Training Loss: 0.2690, Validation Loss 0.4561\n",
      "Epoch 41 Batch 4:  Training Loss: 0.1770, Validation Loss 0.4538\n",
      "Epoch 41 Batch 5:  Training Loss: 1.8931, Validation Loss 0.4554\n",
      "Epoch 41 Batch 6:  Training Loss: 1.2349, Validation Loss 0.4573\n",
      "Epoch 41 Batch 7:  Training Loss: 0.4087, Validation Loss 0.4589\n",
      "Epoch 41 Batch 8:  Training Loss: 0.4684, Validation Loss 0.4605\n",
      "Epoch 41 Batch 9:  Training Loss: 0.2632, Validation Loss 0.4619\n",
      "Epoch 41 Batch 10:  Training Loss: 0.1950, Validation Loss 0.4630\n",
      "Epoch 41 Batch 11:  Training Loss: 0.2495, Validation Loss 0.4641\n",
      "Epoch 41 Batch 12:  Training Loss: 0.1530, Validation Loss 0.4649\n",
      "Epoch 41 Batch 13:  Training Loss: 0.1885, Validation Loss 0.4656\n",
      "Epoch 41 Batch 14:  Training Loss: 0.1868, Validation Loss 0.4661\n",
      "Epoch 42 Batch 0:  Training Loss: 0.2655, Validation Loss 0.4668\n",
      "Epoch 42 Batch 1:  Training Loss: 0.3653, Validation Loss 0.4654\n",
      "Epoch 42 Batch 2:  Training Loss: 0.2025, Validation Loss 0.4549\n",
      "Epoch 42 Batch 3:  Training Loss: 0.2662, Validation Loss 0.4528\n",
      "Epoch 42 Batch 4:  Training Loss: 0.1770, Validation Loss 0.4505\n",
      "Epoch 42 Batch 5:  Training Loss: 1.8604, Validation Loss 0.4522\n",
      "Epoch 42 Batch 6:  Training Loss: 1.2289, Validation Loss 0.4541\n",
      "Epoch 42 Batch 7:  Training Loss: 0.4053, Validation Loss 0.4558\n",
      "Epoch 42 Batch 8:  Training Loss: 0.4629, Validation Loss 0.4574\n",
      "Epoch 42 Batch 9:  Training Loss: 0.2607, Validation Loss 0.4588\n",
      "Epoch 42 Batch 10:  Training Loss: 0.1932, Validation Loss 0.4600\n",
      "Epoch 42 Batch 11:  Training Loss: 0.2472, Validation Loss 0.4610\n",
      "Epoch 42 Batch 12:  Training Loss: 0.1521, Validation Loss 0.4619\n",
      "Epoch 42 Batch 13:  Training Loss: 0.1870, Validation Loss 0.4626\n",
      "Epoch 42 Batch 14:  Training Loss: 0.1860, Validation Loss 0.4631\n",
      "Epoch 43 Batch 0:  Training Loss: 0.2649, Validation Loss 0.4638\n",
      "Epoch 43 Batch 1:  Training Loss: 0.3648, Validation Loss 0.4625\n",
      "Epoch 43 Batch 2:  Training Loss: 0.2005, Validation Loss 0.4521\n",
      "Epoch 43 Batch 3:  Training Loss: 0.2632, Validation Loss 0.4500\n",
      "Epoch 43 Batch 4:  Training Loss: 0.1770, Validation Loss 0.4477\n",
      "Epoch 43 Batch 5:  Training Loss: 1.8285, Validation Loss 0.4493\n",
      "Epoch 43 Batch 6:  Training Loss: 1.2230, Validation Loss 0.4511\n",
      "Epoch 43 Batch 7:  Training Loss: 0.4021, Validation Loss 0.4527\n",
      "Epoch 43 Batch 8:  Training Loss: 0.4574, Validation Loss 0.4542\n",
      "Epoch 43 Batch 9:  Training Loss: 0.2583, Validation Loss 0.4555\n",
      "Epoch 43 Batch 10:  Training Loss: 0.1915, Validation Loss 0.4567\n",
      "Epoch 43 Batch 11:  Training Loss: 0.2449, Validation Loss 0.4577\n",
      "Epoch 43 Batch 12:  Training Loss: 0.1513, Validation Loss 0.4585\n",
      "Epoch 43 Batch 13:  Training Loss: 0.1854, Validation Loss 0.4591\n",
      "Epoch 43 Batch 14:  Training Loss: 0.1852, Validation Loss 0.4596\n",
      "Epoch 44 Batch 0:  Training Loss: 0.2643, Validation Loss 0.4603\n",
      "Epoch 44 Batch 1:  Training Loss: 0.3642, Validation Loss 0.4589\n",
      "Epoch 44 Batch 2:  Training Loss: 0.1983, Validation Loss 0.4486\n",
      "Epoch 44 Batch 3:  Training Loss: 0.2606, Validation Loss 0.4466\n",
      "Epoch 44 Batch 4:  Training Loss: 0.1770, Validation Loss 0.4443\n",
      "Epoch 44 Batch 5:  Training Loss: 1.7974, Validation Loss 0.4459\n",
      "Epoch 44 Batch 6:  Training Loss: 1.2174, Validation Loss 0.4476\n",
      "Epoch 44 Batch 7:  Training Loss: 0.3990, Validation Loss 0.4493\n",
      "Epoch 44 Batch 8:  Training Loss: 0.4512, Validation Loss 0.4507\n",
      "Epoch 44 Batch 9:  Training Loss: 0.2560, Validation Loss 0.4520\n",
      "Epoch 44 Batch 10:  Training Loss: 0.1897, Validation Loss 0.4531\n",
      "Epoch 44 Batch 11:  Training Loss: 0.2427, Validation Loss 0.4540\n",
      "Epoch 44 Batch 12:  Training Loss: 0.1504, Validation Loss 0.4548\n",
      "Epoch 44 Batch 13:  Training Loss: 0.1838, Validation Loss 0.4554\n",
      "Epoch 44 Batch 14:  Training Loss: 0.1844, Validation Loss 0.4558\n",
      "Epoch 45 Batch 0:  Training Loss: 0.2638, Validation Loss 0.4565\n",
      "Epoch 45 Batch 1:  Training Loss: 0.3635, Validation Loss 0.4552\n",
      "Epoch 45 Batch 2:  Training Loss: 0.1960, Validation Loss 0.4450\n",
      "Epoch 45 Batch 3:  Training Loss: 0.2581, Validation Loss 0.4430\n",
      "Epoch 45 Batch 4:  Training Loss: 0.1770, Validation Loss 0.4408\n",
      "Epoch 45 Batch 5:  Training Loss: 1.7671, Validation Loss 0.4424\n",
      "Epoch 45 Batch 6:  Training Loss: 1.2120, Validation Loss 0.4442\n",
      "Epoch 45 Batch 7:  Training Loss: 0.3960, Validation Loss 0.4458\n",
      "Epoch 45 Batch 8:  Training Loss: 0.4447, Validation Loss 0.4472\n",
      "Epoch 45 Batch 9:  Training Loss: 0.2536, Validation Loss 0.4484\n",
      "Epoch 45 Batch 10:  Training Loss: 0.1877, Validation Loss 0.4494\n",
      "Epoch 45 Batch 11:  Training Loss: 0.2404, Validation Loss 0.4504\n",
      "Epoch 45 Batch 12:  Training Loss: 0.1493, Validation Loss 0.4511\n",
      "Epoch 45 Batch 13:  Training Loss: 0.1821, Validation Loss 0.4517\n",
      "Epoch 45 Batch 14:  Training Loss: 0.1835, Validation Loss 0.4521\n",
      "Epoch 46 Batch 0:  Training Loss: 0.2634, Validation Loss 0.4528\n",
      "Epoch 46 Batch 1:  Training Loss: 0.3628, Validation Loss 0.4514\n",
      "Epoch 46 Batch 2:  Training Loss: 0.1935, Validation Loss 0.4414\n",
      "Epoch 46 Batch 3:  Training Loss: 0.2560, Validation Loss 0.4396\n",
      "Epoch 46 Batch 4:  Training Loss: 0.1770, Validation Loss 0.4374\n",
      "Epoch 46 Batch 5:  Training Loss: 1.7375, Validation Loss 0.4390\n",
      "Epoch 46 Batch 6:  Training Loss: 1.2063, Validation Loss 0.4408\n",
      "Epoch 46 Batch 7:  Training Loss: 0.3928, Validation Loss 0.4425\n",
      "Epoch 46 Batch 8:  Training Loss: 0.4364, Validation Loss 0.4439\n",
      "Epoch 46 Batch 9:  Training Loss: 0.2515, Validation Loss 0.4451\n",
      "Epoch 46 Batch 10:  Training Loss: 0.1857, Validation Loss 0.4462\n",
      "Epoch 46 Batch 11:  Training Loss: 0.2381, Validation Loss 0.4472\n",
      "Epoch 46 Batch 12:  Training Loss: 0.1482, Validation Loss 0.4480\n",
      "Epoch 46 Batch 13:  Training Loss: 0.1804, Validation Loss 0.4486\n",
      "Epoch 46 Batch 14:  Training Loss: 0.1825, Validation Loss 0.4490\n",
      "Epoch 47 Batch 0:  Training Loss: 0.2631, Validation Loss 0.4498\n",
      "Epoch 47 Batch 1:  Training Loss: 0.3622, Validation Loss 0.4484\n",
      "Epoch 47 Batch 2:  Training Loss: 0.1915, Validation Loss 0.4386\n",
      "Epoch 47 Batch 3:  Training Loss: 0.2536, Validation Loss 0.4368\n",
      "Epoch 47 Batch 4:  Training Loss: 0.1771, Validation Loss 0.4348\n",
      "Epoch 47 Batch 5:  Training Loss: 1.7087, Validation Loss 0.4364\n",
      "Epoch 47 Batch 6:  Training Loss: 1.2009, Validation Loss 0.4381\n",
      "Epoch 47 Batch 7:  Training Loss: 0.3900, Validation Loss 0.4398\n",
      "Epoch 47 Batch 8:  Training Loss: 0.4293, Validation Loss 0.4412\n",
      "Epoch 47 Batch 9:  Training Loss: 0.2491, Validation Loss 0.4425\n",
      "Epoch 47 Batch 10:  Training Loss: 0.1836, Validation Loss 0.4436\n",
      "Epoch 47 Batch 11:  Training Loss: 0.2359, Validation Loss 0.4446\n",
      "Epoch 47 Batch 12:  Training Loss: 0.1471, Validation Loss 0.4454\n",
      "Epoch 47 Batch 13:  Training Loss: 0.1786, Validation Loss 0.4460\n",
      "Epoch 47 Batch 14:  Training Loss: 0.1816, Validation Loss 0.4465\n",
      "Epoch 48 Batch 0:  Training Loss: 0.2628, Validation Loss 0.4473\n",
      "Epoch 48 Batch 1:  Training Loss: 0.3617, Validation Loss 0.4460\n",
      "Epoch 48 Batch 2:  Training Loss: 0.1896, Validation Loss 0.4363\n",
      "Epoch 48 Batch 3:  Training Loss: 0.2512, Validation Loss 0.4346\n",
      "Epoch 48 Batch 4:  Training Loss: 0.1772, Validation Loss 0.4325\n",
      "Epoch 48 Batch 5:  Training Loss: 1.6807, Validation Loss 0.4341\n",
      "Epoch 48 Batch 6:  Training Loss: 1.1952, Validation Loss 0.4358\n",
      "Epoch 48 Batch 7:  Training Loss: 0.3870, Validation Loss 0.4375\n",
      "Epoch 48 Batch 8:  Training Loss: 0.4215, Validation Loss 0.4390\n",
      "Epoch 48 Batch 9:  Training Loss: 0.2471, Validation Loss 0.4403\n",
      "Epoch 48 Batch 10:  Training Loss: 0.1817, Validation Loss 0.4415\n",
      "Epoch 48 Batch 11:  Training Loss: 0.2337, Validation Loss 0.4425\n",
      "Epoch 48 Batch 12:  Training Loss: 0.1460, Validation Loss 0.4433\n",
      "Epoch 48 Batch 13:  Training Loss: 0.1769, Validation Loss 0.4440\n",
      "Epoch 48 Batch 14:  Training Loss: 0.1808, Validation Loss 0.4444\n",
      "Epoch 49 Batch 0:  Training Loss: 0.2626, Validation Loss 0.4453\n",
      "Epoch 49 Batch 1:  Training Loss: 0.3612, Validation Loss 0.4440\n",
      "Epoch 49 Batch 2:  Training Loss: 0.1879, Validation Loss 0.4343\n",
      "Epoch 49 Batch 3:  Training Loss: 0.2487, Validation Loss 0.4326\n",
      "Epoch 49 Batch 4:  Training Loss: 0.1774, Validation Loss 0.4306\n",
      "Epoch 49 Batch 5:  Training Loss: 1.6538, Validation Loss 0.4321\n",
      "Epoch 49 Batch 6:  Training Loss: 1.1907, Validation Loss 0.4339\n",
      "Epoch 49 Batch 7:  Training Loss: 0.3843, Validation Loss 0.4356\n",
      "Epoch 49 Batch 8:  Training Loss: 0.4158, Validation Loss 0.4371\n",
      "Epoch 49 Batch 9:  Training Loss: 0.2448, Validation Loss 0.4384\n",
      "Epoch 49 Batch 10:  Training Loss: 0.1797, Validation Loss 0.4396\n",
      "Epoch 49 Batch 11:  Training Loss: 0.2319, Validation Loss 0.4406\n",
      "Epoch 49 Batch 12:  Training Loss: 0.1450, Validation Loss 0.4415\n",
      "Epoch 49 Batch 13:  Training Loss: 0.1752, Validation Loss 0.4421\n",
      "Epoch 49 Batch 14:  Training Loss: 0.1801, Validation Loss 0.4427\n",
      "Epoch 50 Batch 0:  Training Loss: 0.2622, Validation Loss 0.4435\n",
      "Epoch 50 Batch 1:  Training Loss: 0.3608, Validation Loss 0.4421\n",
      "Epoch 50 Batch 2:  Training Loss: 0.1865, Validation Loss 0.4324\n",
      "Epoch 50 Batch 3:  Training Loss: 0.2456, Validation Loss 0.4307\n",
      "Epoch 50 Batch 4:  Training Loss: 0.1777, Validation Loss 0.4286\n",
      "Epoch 50 Batch 5:  Training Loss: 1.6278, Validation Loss 0.4301\n",
      "Epoch 50 Batch 6:  Training Loss: 1.1865, Validation Loss 0.4318\n",
      "Epoch 50 Batch 7:  Training Loss: 0.3818, Validation Loss 0.4334\n",
      "Epoch 50 Batch 8:  Training Loss: 0.4106, Validation Loss 0.4348\n",
      "Epoch 50 Batch 9:  Training Loss: 0.2431, Validation Loss 0.4361\n",
      "Epoch 50 Batch 10:  Training Loss: 0.1781, Validation Loss 0.4372\n",
      "Epoch 50 Batch 11:  Training Loss: 0.2302, Validation Loss 0.4382\n",
      "Epoch 50 Batch 12:  Training Loss: 0.1441, Validation Loss 0.4390\n",
      "Epoch 50 Batch 13:  Training Loss: 0.1737, Validation Loss 0.4397\n",
      "Epoch 50 Batch 14:  Training Loss: 0.1793, Validation Loss 0.4401\n",
      "Epoch 51 Batch 0:  Training Loss: 0.2618, Validation Loss 0.4409\n",
      "Epoch 51 Batch 1:  Training Loss: 0.3602, Validation Loss 0.4395\n",
      "Epoch 51 Batch 2:  Training Loss: 0.1849, Validation Loss 0.4298\n",
      "Epoch 51 Batch 3:  Training Loss: 0.2428, Validation Loss 0.4281\n",
      "Epoch 51 Batch 4:  Training Loss: 0.1779, Validation Loss 0.4259\n",
      "Epoch 51 Batch 5:  Training Loss: 1.6028, Validation Loss 0.4273\n",
      "Epoch 51 Batch 6:  Training Loss: 1.1823, Validation Loss 0.4289\n",
      "Epoch 51 Batch 7:  Training Loss: 0.3794, Validation Loss 0.4304\n",
      "Epoch 51 Batch 8:  Training Loss: 0.4059, Validation Loss 0.4318\n",
      "Epoch 51 Batch 9:  Training Loss: 0.2415, Validation Loss 0.4330\n",
      "Epoch 51 Batch 10:  Training Loss: 0.1765, Validation Loss 0.4341\n",
      "Epoch 51 Batch 11:  Training Loss: 0.2283, Validation Loss 0.4351\n",
      "Epoch 51 Batch 12:  Training Loss: 0.1433, Validation Loss 0.4358\n",
      "Epoch 51 Batch 13:  Training Loss: 0.1722, Validation Loss 0.4364\n",
      "Epoch 51 Batch 14:  Training Loss: 0.1785, Validation Loss 0.4369\n",
      "Epoch 52 Batch 0:  Training Loss: 0.2613, Validation Loss 0.4376\n",
      "Epoch 52 Batch 1:  Training Loss: 0.3594, Validation Loss 0.4362\n",
      "Epoch 52 Batch 2:  Training Loss: 0.1828, Validation Loss 0.4266\n",
      "Epoch 52 Batch 3:  Training Loss: 0.2403, Validation Loss 0.4249\n",
      "Epoch 52 Batch 4:  Training Loss: 0.1781, Validation Loss 0.4227\n",
      "Epoch 52 Batch 5:  Training Loss: 1.5787, Validation Loss 0.4241\n",
      "Epoch 52 Batch 6:  Training Loss: 1.1780, Validation Loss 0.4257\n",
      "Epoch 52 Batch 7:  Training Loss: 0.3769, Validation Loss 0.4272\n",
      "Epoch 52 Batch 8:  Training Loss: 0.4012, Validation Loss 0.4286\n",
      "Epoch 52 Batch 9:  Training Loss: 0.2400, Validation Loss 0.4298\n",
      "Epoch 52 Batch 10:  Training Loss: 0.1750, Validation Loss 0.4309\n",
      "Epoch 52 Batch 11:  Training Loss: 0.2265, Validation Loss 0.4318\n",
      "Epoch 52 Batch 12:  Training Loss: 0.1426, Validation Loss 0.4326\n",
      "Epoch 52 Batch 13:  Training Loss: 0.1707, Validation Loss 0.4332\n",
      "Epoch 52 Batch 14:  Training Loss: 0.1777, Validation Loss 0.4336\n",
      "Epoch 53 Batch 0:  Training Loss: 0.2609, Validation Loss 0.4344\n",
      "Epoch 53 Batch 1:  Training Loss: 0.3587, Validation Loss 0.4330\n",
      "Epoch 53 Batch 2:  Training Loss: 0.1807, Validation Loss 0.4235\n",
      "Epoch 53 Batch 3:  Training Loss: 0.2376, Validation Loss 0.4219\n",
      "Epoch 53 Batch 4:  Training Loss: 0.1783, Validation Loss 0.4197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 5:  Training Loss: 1.5552, Validation Loss 0.4210\n",
      "Epoch 53 Batch 6:  Training Loss: 1.1734, Validation Loss 0.4226\n",
      "Epoch 53 Batch 7:  Training Loss: 0.3747, Validation Loss 0.4241\n",
      "Epoch 53 Batch 8:  Training Loss: 0.3965, Validation Loss 0.4254\n",
      "Epoch 53 Batch 9:  Training Loss: 0.2385, Validation Loss 0.4266\n",
      "Epoch 53 Batch 10:  Training Loss: 0.1736, Validation Loss 0.4277\n",
      "Epoch 53 Batch 11:  Training Loss: 0.2245, Validation Loss 0.4286\n",
      "Epoch 53 Batch 12:  Training Loss: 0.1418, Validation Loss 0.4293\n",
      "Epoch 53 Batch 13:  Training Loss: 0.1693, Validation Loss 0.4299\n",
      "Epoch 53 Batch 14:  Training Loss: 0.1769, Validation Loss 0.4304\n",
      "Epoch 54 Batch 0:  Training Loss: 0.2605, Validation Loss 0.4312\n",
      "Epoch 54 Batch 1:  Training Loss: 0.3579, Validation Loss 0.4298\n",
      "Epoch 54 Batch 2:  Training Loss: 0.1787, Validation Loss 0.4204\n",
      "Epoch 54 Batch 3:  Training Loss: 0.2350, Validation Loss 0.4189\n",
      "Epoch 54 Batch 4:  Training Loss: 0.1786, Validation Loss 0.4167\n",
      "Epoch 54 Batch 5:  Training Loss: 1.5324, Validation Loss 0.4181\n",
      "Epoch 54 Batch 6:  Training Loss: 1.1689, Validation Loss 0.4196\n",
      "Epoch 54 Batch 7:  Training Loss: 0.3726, Validation Loss 0.4211\n",
      "Epoch 54 Batch 8:  Training Loss: 0.3918, Validation Loss 0.4224\n",
      "Epoch 54 Batch 9:  Training Loss: 0.2370, Validation Loss 0.4236\n",
      "Epoch 54 Batch 10:  Training Loss: 0.1722, Validation Loss 0.4246\n",
      "Epoch 54 Batch 11:  Training Loss: 0.2226, Validation Loss 0.4255\n",
      "Epoch 54 Batch 12:  Training Loss: 0.1410, Validation Loss 0.4263\n",
      "Epoch 54 Batch 13:  Training Loss: 0.1679, Validation Loss 0.4269\n",
      "Epoch 54 Batch 14:  Training Loss: 0.1760, Validation Loss 0.4273\n",
      "Epoch 55 Batch 0:  Training Loss: 0.2601, Validation Loss 0.4282\n",
      "Epoch 55 Batch 1:  Training Loss: 0.3572, Validation Loss 0.4268\n",
      "Epoch 55 Batch 2:  Training Loss: 0.1768, Validation Loss 0.4175\n",
      "Epoch 55 Batch 3:  Training Loss: 0.2322, Validation Loss 0.4160\n",
      "Epoch 55 Batch 4:  Training Loss: 0.1789, Validation Loss 0.4139\n",
      "Epoch 55 Batch 5:  Training Loss: 1.5107, Validation Loss 0.4152\n",
      "Epoch 55 Batch 6:  Training Loss: 1.1643, Validation Loss 0.4167\n",
      "Epoch 55 Batch 7:  Training Loss: 0.3705, Validation Loss 0.4181\n",
      "Epoch 55 Batch 8:  Training Loss: 0.3876, Validation Loss 0.4194\n",
      "Epoch 55 Batch 9:  Training Loss: 0.2356, Validation Loss 0.4205\n",
      "Epoch 55 Batch 10:  Training Loss: 0.1707, Validation Loss 0.4215\n",
      "Epoch 55 Batch 11:  Training Loss: 0.2207, Validation Loss 0.4224\n",
      "Epoch 55 Batch 12:  Training Loss: 0.1401, Validation Loss 0.4232\n",
      "Epoch 55 Batch 13:  Training Loss: 0.1663, Validation Loss 0.4237\n",
      "Epoch 55 Batch 14:  Training Loss: 0.1750, Validation Loss 0.4242\n",
      "Epoch 56 Batch 0:  Training Loss: 0.2599, Validation Loss 0.4250\n",
      "Epoch 56 Batch 1:  Training Loss: 0.3564, Validation Loss 0.4236\n",
      "Epoch 56 Batch 2:  Training Loss: 0.1749, Validation Loss 0.4144\n",
      "Epoch 56 Batch 3:  Training Loss: 0.2295, Validation Loss 0.4130\n",
      "Epoch 56 Batch 4:  Training Loss: 0.1792, Validation Loss 0.4109\n",
      "Epoch 56 Batch 5:  Training Loss: 1.4900, Validation Loss 0.4122\n",
      "Epoch 56 Batch 6:  Training Loss: 1.1595, Validation Loss 0.4137\n",
      "Epoch 56 Batch 7:  Training Loss: 0.3684, Validation Loss 0.4151\n",
      "Epoch 56 Batch 8:  Training Loss: 0.3829, Validation Loss 0.4163\n",
      "Epoch 56 Batch 9:  Training Loss: 0.2347, Validation Loss 0.4174\n",
      "Epoch 56 Batch 10:  Training Loss: 0.1695, Validation Loss 0.4183\n",
      "Epoch 56 Batch 11:  Training Loss: 0.2190, Validation Loss 0.4192\n",
      "Epoch 56 Batch 12:  Training Loss: 0.1393, Validation Loss 0.4200\n",
      "Epoch 56 Batch 13:  Training Loss: 0.1648, Validation Loss 0.4205\n",
      "Epoch 56 Batch 14:  Training Loss: 0.1740, Validation Loss 0.4210\n",
      "Epoch 57 Batch 0:  Training Loss: 0.2597, Validation Loss 0.4218\n",
      "Epoch 57 Batch 1:  Training Loss: 0.3554, Validation Loss 0.4205\n",
      "Epoch 57 Batch 2:  Training Loss: 0.1731, Validation Loss 0.4115\n",
      "Epoch 57 Batch 3:  Training Loss: 0.2268, Validation Loss 0.4101\n",
      "Epoch 57 Batch 4:  Training Loss: 0.1795, Validation Loss 0.4081\n",
      "Epoch 57 Batch 5:  Training Loss: 1.4701, Validation Loss 0.4093\n",
      "Epoch 57 Batch 6:  Training Loss: 1.1548, Validation Loss 0.4107\n",
      "Epoch 57 Batch 7:  Training Loss: 0.3663, Validation Loss 0.4121\n",
      "Epoch 57 Batch 8:  Training Loss: 0.3788, Validation Loss 0.4132\n",
      "Epoch 57 Batch 9:  Training Loss: 0.2336, Validation Loss 0.4142\n",
      "Epoch 57 Batch 10:  Training Loss: 0.1682, Validation Loss 0.4152\n",
      "Epoch 57 Batch 11:  Training Loss: 0.2173, Validation Loss 0.4160\n",
      "Epoch 57 Batch 12:  Training Loss: 0.1385, Validation Loss 0.4167\n",
      "Epoch 57 Batch 13:  Training Loss: 0.1633, Validation Loss 0.4173\n",
      "Epoch 57 Batch 14:  Training Loss: 0.1730, Validation Loss 0.4178\n",
      "Epoch 58 Batch 0:  Training Loss: 0.2596, Validation Loss 0.4186\n",
      "Epoch 58 Batch 1:  Training Loss: 0.3544, Validation Loss 0.4173\n",
      "Epoch 58 Batch 2:  Training Loss: 0.1714, Validation Loss 0.4084\n",
      "Epoch 58 Batch 3:  Training Loss: 0.2241, Validation Loss 0.4071\n",
      "Epoch 58 Batch 4:  Training Loss: 0.1799, Validation Loss 0.4050\n",
      "Epoch 58 Batch 5:  Training Loss: 1.4507, Validation Loss 0.4062\n",
      "Epoch 58 Batch 6:  Training Loss: 1.1504, Validation Loss 0.4075\n",
      "Epoch 58 Batch 7:  Training Loss: 0.3643, Validation Loss 0.4086\n",
      "Epoch 58 Batch 8:  Training Loss: 0.3747, Validation Loss 0.4095\n",
      "Epoch 58 Batch 9:  Training Loss: 0.2326, Validation Loss 0.4104\n",
      "Epoch 58 Batch 10:  Training Loss: 0.1669, Validation Loss 0.4112\n",
      "Epoch 58 Batch 11:  Training Loss: 0.2156, Validation Loss 0.4119\n",
      "Epoch 58 Batch 12:  Training Loss: 0.1376, Validation Loss 0.4125\n",
      "Epoch 58 Batch 13:  Training Loss: 0.1618, Validation Loss 0.4130\n",
      "Epoch 58 Batch 14:  Training Loss: 0.1720, Validation Loss 0.4133\n",
      "Epoch 59 Batch 0:  Training Loss: 0.2592, Validation Loss 0.4141\n",
      "Epoch 59 Batch 1:  Training Loss: 0.3530, Validation Loss 0.4127\n",
      "Epoch 59 Batch 2:  Training Loss: 0.1690, Validation Loss 0.4041\n",
      "Epoch 59 Batch 3:  Training Loss: 0.2217, Validation Loss 0.4029\n",
      "Epoch 59 Batch 4:  Training Loss: 0.1803, Validation Loss 0.4009\n",
      "Epoch 59 Batch 5:  Training Loss: 1.4317, Validation Loss 0.4021\n",
      "Epoch 59 Batch 6:  Training Loss: 1.1461, Validation Loss 0.4034\n",
      "Epoch 59 Batch 7:  Training Loss: 0.3625, Validation Loss 0.4045\n",
      "Epoch 59 Batch 8:  Training Loss: 0.3715, Validation Loss 0.4053\n",
      "Epoch 59 Batch 9:  Training Loss: 0.2314, Validation Loss 0.4062\n",
      "Epoch 59 Batch 10:  Training Loss: 0.1657, Validation Loss 0.4070\n",
      "Epoch 59 Batch 11:  Training Loss: 0.2140, Validation Loss 0.4077\n",
      "Epoch 59 Batch 12:  Training Loss: 0.1368, Validation Loss 0.4083\n",
      "Epoch 59 Batch 13:  Training Loss: 0.1604, Validation Loss 0.4088\n",
      "Epoch 59 Batch 14:  Training Loss: 0.1710, Validation Loss 0.4092\n",
      "Epoch 60 Batch 0:  Training Loss: 0.2586, Validation Loss 0.4100\n",
      "Epoch 60 Batch 1:  Training Loss: 0.3513, Validation Loss 0.4086\n",
      "Epoch 60 Batch 2:  Training Loss: 0.1669, Validation Loss 0.4002\n",
      "Epoch 60 Batch 3:  Training Loss: 0.2188, Validation Loss 0.3990\n",
      "Epoch 60 Batch 4:  Training Loss: 0.1808, Validation Loss 0.3972\n",
      "Epoch 60 Batch 5:  Training Loss: 1.4130, Validation Loss 0.3984\n",
      "Epoch 60 Batch 6:  Training Loss: 1.1411, Validation Loss 0.3996\n",
      "Epoch 60 Batch 7:  Training Loss: 0.3602, Validation Loss 0.4007\n",
      "Epoch 60 Batch 8:  Training Loss: 0.3665, Validation Loss 0.4015\n",
      "Epoch 60 Batch 9:  Training Loss: 0.2313, Validation Loss 0.4023\n",
      "Epoch 60 Batch 10:  Training Loss: 0.1647, Validation Loss 0.4030\n",
      "Epoch 60 Batch 11:  Training Loss: 0.2125, Validation Loss 0.4037\n",
      "Epoch 60 Batch 12:  Training Loss: 0.1360, Validation Loss 0.4044\n",
      "Epoch 60 Batch 13:  Training Loss: 0.1587, Validation Loss 0.4049\n",
      "Epoch 60 Batch 14:  Training Loss: 0.1698, Validation Loss 0.4053\n",
      "Epoch 61 Batch 0:  Training Loss: 0.2578, Validation Loss 0.4061\n",
      "Epoch 61 Batch 1:  Training Loss: 0.3494, Validation Loss 0.4048\n",
      "Epoch 61 Batch 2:  Training Loss: 0.1653, Validation Loss 0.3965\n",
      "Epoch 61 Batch 3:  Training Loss: 0.2154, Validation Loss 0.3954\n",
      "Epoch 61 Batch 4:  Training Loss: 0.1814, Validation Loss 0.3935\n",
      "Epoch 61 Batch 5:  Training Loss: 1.3951, Validation Loss 0.3946\n",
      "Epoch 61 Batch 6:  Training Loss: 1.1371, Validation Loss 0.3958\n",
      "Epoch 61 Batch 7:  Training Loss: 0.3590, Validation Loss 0.3968\n",
      "Epoch 61 Batch 8:  Training Loss: 0.3653, Validation Loss 0.3975\n",
      "Epoch 61 Batch 9:  Training Loss: 0.2297, Validation Loss 0.3981\n",
      "Epoch 61 Batch 10:  Training Loss: 0.1634, Validation Loss 0.3988\n",
      "Epoch 61 Batch 11:  Training Loss: 0.2109, Validation Loss 0.3994\n",
      "Epoch 61 Batch 12:  Training Loss: 0.1353, Validation Loss 0.4000\n",
      "Epoch 61 Batch 13:  Training Loss: 0.1573, Validation Loss 0.4005\n",
      "Epoch 61 Batch 14:  Training Loss: 0.1688, Validation Loss 0.4009\n",
      "Epoch 62 Batch 0:  Training Loss: 0.2570, Validation Loss 0.4017\n",
      "Epoch 62 Batch 1:  Training Loss: 0.3476, Validation Loss 0.4003\n",
      "Epoch 62 Batch 2:  Training Loss: 0.1635, Validation Loss 0.3921\n",
      "Epoch 62 Batch 3:  Training Loss: 0.2121, Validation Loss 0.3910\n",
      "Epoch 62 Batch 4:  Training Loss: 0.1820, Validation Loss 0.3891\n",
      "Epoch 62 Batch 5:  Training Loss: 1.3771, Validation Loss 0.3901\n",
      "Epoch 62 Batch 6:  Training Loss: 1.1322, Validation Loss 0.3914\n",
      "Epoch 62 Batch 7:  Training Loss: 0.3566, Validation Loss 0.3925\n",
      "Epoch 62 Batch 8:  Training Loss: 0.3603, Validation Loss 0.3931\n",
      "Epoch 62 Batch 9:  Training Loss: 0.2298, Validation Loss 0.3938\n",
      "Epoch 62 Batch 10:  Training Loss: 0.1628, Validation Loss 0.3945\n",
      "Epoch 62 Batch 11:  Training Loss: 0.2094, Validation Loss 0.3952\n",
      "Epoch 62 Batch 12:  Training Loss: 0.1344, Validation Loss 0.3959\n",
      "Epoch 62 Batch 13:  Training Loss: 0.1555, Validation Loss 0.3964\n",
      "Epoch 62 Batch 14:  Training Loss: 0.1676, Validation Loss 0.3968\n",
      "Epoch 63 Batch 0:  Training Loss: 0.2562, Validation Loss 0.3976\n",
      "Epoch 63 Batch 1:  Training Loss: 0.3455, Validation Loss 0.3963\n",
      "Epoch 63 Batch 2:  Training Loss: 0.1621, Validation Loss 0.3882\n",
      "Epoch 63 Batch 3:  Training Loss: 0.2082, Validation Loss 0.3870\n",
      "Epoch 63 Batch 4:  Training Loss: 0.1826, Validation Loss 0.3850\n",
      "Epoch 63 Batch 5:  Training Loss: 1.3596, Validation Loss 0.3860\n",
      "Epoch 63 Batch 6:  Training Loss: 1.1284, Validation Loss 0.3869\n",
      "Epoch 63 Batch 7:  Training Loss: 0.3556, Validation Loss 0.3877\n",
      "Epoch 63 Batch 8:  Training Loss: 0.3596, Validation Loss 0.3880\n",
      "Epoch 63 Batch 9:  Training Loss: 0.2283, Validation Loss 0.3885\n",
      "Epoch 63 Batch 10:  Training Loss: 0.1617, Validation Loss 0.3890\n",
      "Epoch 63 Batch 11:  Training Loss: 0.2080, Validation Loss 0.3894\n",
      "Epoch 63 Batch 12:  Training Loss: 0.1339, Validation Loss 0.3899\n",
      "Epoch 63 Batch 13:  Training Loss: 0.1544, Validation Loss 0.3903\n",
      "Epoch 63 Batch 14:  Training Loss: 0.1668, Validation Loss 0.3906\n",
      "Epoch 64 Batch 0:  Training Loss: 0.2558, Validation Loss 0.3913\n",
      "Epoch 64 Batch 1:  Training Loss: 0.3429, Validation Loss 0.3899\n",
      "Epoch 64 Batch 2:  Training Loss: 0.1594, Validation Loss 0.3821\n",
      "Epoch 64 Batch 3:  Training Loss: 0.2054, Validation Loss 0.3810\n",
      "Epoch 64 Batch 4:  Training Loss: 0.1831, Validation Loss 0.3792\n",
      "Epoch 64 Batch 5:  Training Loss: 1.3427, Validation Loss 0.3802\n",
      "Epoch 64 Batch 6:  Training Loss: 1.1232, Validation Loss 0.3812\n",
      "Epoch 64 Batch 7:  Training Loss: 0.3527, Validation Loss 0.3820\n",
      "Epoch 64 Batch 8:  Training Loss: 0.3534, Validation Loss 0.3823\n",
      "Epoch 64 Batch 9:  Training Loss: 0.2290, Validation Loss 0.3828\n",
      "Epoch 64 Batch 10:  Training Loss: 0.1612, Validation Loss 0.3833\n",
      "Epoch 64 Batch 11:  Training Loss: 0.2066, Validation Loss 0.3838\n",
      "Epoch 64 Batch 12:  Training Loss: 0.1331, Validation Loss 0.3843\n",
      "Epoch 64 Batch 13:  Training Loss: 0.1527, Validation Loss 0.3847\n",
      "Epoch 64 Batch 14:  Training Loss: 0.1655, Validation Loss 0.3850\n",
      "Epoch 65 Batch 0:  Training Loss: 0.2554, Validation Loss 0.3858\n",
      "Epoch 65 Batch 1:  Training Loss: 0.3405, Validation Loss 0.3844\n",
      "Epoch 65 Batch 2:  Training Loss: 0.1571, Validation Loss 0.3770\n",
      "Epoch 65 Batch 3:  Training Loss: 0.2024, Validation Loss 0.3759\n",
      "Epoch 65 Batch 4:  Training Loss: 0.1838, Validation Loss 0.3742\n",
      "Epoch 65 Batch 5:  Training Loss: 1.3270, Validation Loss 0.3751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 6:  Training Loss: 1.1203, Validation Loss 0.3761\n",
      "Epoch 65 Batch 7:  Training Loss: 0.3516, Validation Loss 0.3769\n",
      "Epoch 65 Batch 8:  Training Loss: 0.3524, Validation Loss 0.3772\n",
      "Epoch 65 Batch 9:  Training Loss: 0.2277, Validation Loss 0.3776\n",
      "Epoch 65 Batch 10:  Training Loss: 0.1602, Validation Loss 0.3781\n",
      "Epoch 65 Batch 11:  Training Loss: 0.2054, Validation Loss 0.3786\n",
      "Epoch 65 Batch 12:  Training Loss: 0.1328, Validation Loss 0.3790\n",
      "Epoch 65 Batch 13:  Training Loss: 0.1519, Validation Loss 0.3794\n",
      "Epoch 65 Batch 14:  Training Loss: 0.1650, Validation Loss 0.3797\n",
      "Epoch 66 Batch 0:  Training Loss: 0.2553, Validation Loss 0.3804\n",
      "Epoch 66 Batch 1:  Training Loss: 0.3383, Validation Loss 0.3792\n",
      "Epoch 66 Batch 2:  Training Loss: 0.1546, Validation Loss 0.3719\n",
      "Epoch 66 Batch 3:  Training Loss: 0.1996, Validation Loss 0.3710\n",
      "Epoch 66 Batch 4:  Training Loss: 0.1845, Validation Loss 0.3693\n",
      "Epoch 66 Batch 5:  Training Loss: 1.3119, Validation Loss 0.3703\n",
      "Epoch 66 Batch 6:  Training Loss: 1.1147, Validation Loss 0.3712\n",
      "Epoch 66 Batch 7:  Training Loss: 0.3480, Validation Loss 0.3720\n",
      "Epoch 66 Batch 8:  Training Loss: 0.3437, Validation Loss 0.3723\n",
      "Epoch 66 Batch 9:  Training Loss: 0.2293, Validation Loss 0.3727\n",
      "Epoch 66 Batch 10:  Training Loss: 0.1598, Validation Loss 0.3732\n",
      "Epoch 66 Batch 11:  Training Loss: 0.2040, Validation Loss 0.3737\n",
      "Epoch 66 Batch 12:  Training Loss: 0.1316, Validation Loss 0.3741\n",
      "Epoch 66 Batch 13:  Training Loss: 0.1496, Validation Loss 0.3745\n",
      "Epoch 66 Batch 14:  Training Loss: 0.1631, Validation Loss 0.3748\n",
      "Epoch 67 Batch 0:  Training Loss: 0.2552, Validation Loss 0.3756\n",
      "Epoch 67 Batch 1:  Training Loss: 0.3365, Validation Loss 0.3744\n",
      "Epoch 67 Batch 2:  Training Loss: 0.1524, Validation Loss 0.3674\n",
      "Epoch 67 Batch 3:  Training Loss: 0.1967, Validation Loss 0.3665\n",
      "Epoch 67 Batch 4:  Training Loss: 0.1853, Validation Loss 0.3649\n",
      "Epoch 67 Batch 5:  Training Loss: 1.2979, Validation Loss 0.3659\n",
      "Epoch 67 Batch 6:  Training Loss: 1.1129, Validation Loss 0.3668\n",
      "Epoch 67 Batch 7:  Training Loss: 0.3488, Validation Loss 0.3676\n",
      "Epoch 67 Batch 8:  Training Loss: 0.3468, Validation Loss 0.3678\n",
      "Epoch 67 Batch 9:  Training Loss: 0.2263, Validation Loss 0.3682\n",
      "Epoch 67 Batch 10:  Training Loss: 0.1579, Validation Loss 0.3686\n",
      "Epoch 67 Batch 11:  Training Loss: 0.2025, Validation Loss 0.3690\n",
      "Epoch 67 Batch 12:  Training Loss: 0.1312, Validation Loss 0.3695\n",
      "Epoch 67 Batch 13:  Training Loss: 0.1490, Validation Loss 0.3699\n",
      "Epoch 67 Batch 14:  Training Loss: 0.1628, Validation Loss 0.3702\n",
      "Epoch 68 Batch 0:  Training Loss: 0.2552, Validation Loss 0.3710\n",
      "Epoch 68 Batch 1:  Training Loss: 0.3349, Validation Loss 0.3698\n",
      "Epoch 68 Batch 2:  Training Loss: 0.1503, Validation Loss 0.3631\n",
      "Epoch 68 Batch 3:  Training Loss: 0.1942, Validation Loss 0.3623\n",
      "Epoch 68 Batch 4:  Training Loss: 0.1860, Validation Loss 0.3608\n",
      "Epoch 68 Batch 5:  Training Loss: 1.2835, Validation Loss 0.3618\n",
      "Epoch 68 Batch 6:  Training Loss: 1.1066, Validation Loss 0.3627\n",
      "Epoch 68 Batch 7:  Training Loss: 0.3442, Validation Loss 0.3635\n",
      "Epoch 68 Batch 8:  Training Loss: 0.3346, Validation Loss 0.3637\n",
      "Epoch 68 Batch 9:  Training Loss: 0.2294, Validation Loss 0.3641\n",
      "Epoch 68 Batch 10:  Training Loss: 0.1580, Validation Loss 0.3646\n",
      "Epoch 68 Batch 11:  Training Loss: 0.2012, Validation Loss 0.3651\n",
      "Epoch 68 Batch 12:  Training Loss: 0.1298, Validation Loss 0.3656\n",
      "Epoch 68 Batch 13:  Training Loss: 0.1462, Validation Loss 0.3660\n",
      "Epoch 68 Batch 14:  Training Loss: 0.1605, Validation Loss 0.3664\n",
      "Epoch 69 Batch 0:  Training Loss: 0.2551, Validation Loss 0.3672\n",
      "Epoch 69 Batch 1:  Training Loss: 0.3335, Validation Loss 0.3660\n",
      "Epoch 69 Batch 2:  Training Loss: 0.1486, Validation Loss 0.3595\n",
      "Epoch 69 Batch 3:  Training Loss: 0.1914, Validation Loss 0.3586\n",
      "Epoch 69 Batch 4:  Training Loss: 0.1866, Validation Loss 0.3571\n",
      "Epoch 69 Batch 5:  Training Loss: 1.2705, Validation Loss 0.3579\n",
      "Epoch 69 Batch 6:  Training Loss: 1.1066, Validation Loss 0.3588\n",
      "Epoch 69 Batch 7:  Training Loss: 0.3467, Validation Loss 0.3595\n",
      "Epoch 69 Batch 8:  Training Loss: 0.3424, Validation Loss 0.3596\n",
      "Epoch 69 Batch 9:  Training Loss: 0.2249, Validation Loss 0.3599\n",
      "Epoch 69 Batch 10:  Training Loss: 0.1559, Validation Loss 0.3603\n",
      "Epoch 69 Batch 11:  Training Loss: 0.1999, Validation Loss 0.3606\n",
      "Epoch 69 Batch 12:  Training Loss: 0.1300, Validation Loss 0.3610\n",
      "Epoch 69 Batch 13:  Training Loss: 0.1464, Validation Loss 0.3614\n",
      "Epoch 69 Batch 14:  Training Loss: 0.1610, Validation Loss 0.3617\n",
      "Epoch 70 Batch 0:  Training Loss: 0.2548, Validation Loss 0.3624\n",
      "Epoch 70 Batch 1:  Training Loss: 0.3320, Validation Loss 0.3612\n",
      "Epoch 70 Batch 2:  Training Loss: 0.1464, Validation Loss 0.3549\n",
      "Epoch 70 Batch 3:  Training Loss: 0.1891, Validation Loss 0.3541\n",
      "Epoch 70 Batch 4:  Training Loss: 0.1872, Validation Loss 0.3526\n",
      "Epoch 70 Batch 5:  Training Loss: 1.2569, Validation Loss 0.3534\n",
      "Epoch 70 Batch 6:  Training Loss: 1.0988, Validation Loss 0.3545\n",
      "Epoch 70 Batch 7:  Training Loss: 0.3401, Validation Loss 0.3554\n",
      "Epoch 70 Batch 8:  Training Loss: 0.3255, Validation Loss 0.3558\n",
      "Epoch 70 Batch 9:  Training Loss: 0.2296, Validation Loss 0.3564\n",
      "Epoch 70 Batch 10:  Training Loss: 0.1564, Validation Loss 0.3570\n",
      "Epoch 70 Batch 11:  Training Loss: 0.1982, Validation Loss 0.3576\n",
      "Epoch 70 Batch 12:  Training Loss: 0.1277, Validation Loss 0.3582\n",
      "Epoch 70 Batch 13:  Training Loss: 0.1423, Validation Loss 0.3587\n",
      "Epoch 70 Batch 14:  Training Loss: 0.1577, Validation Loss 0.3591\n",
      "Epoch 71 Batch 0:  Training Loss: 0.2541, Validation Loss 0.3598\n",
      "Epoch 71 Batch 1:  Training Loss: 0.3310, Validation Loss 0.3586\n",
      "Epoch 71 Batch 2:  Training Loss: 0.1458, Validation Loss 0.3522\n",
      "Epoch 71 Batch 3:  Training Loss: 0.1853, Validation Loss 0.3513\n",
      "Epoch 71 Batch 4:  Training Loss: 0.1879, Validation Loss 0.3496\n",
      "Epoch 71 Batch 5:  Training Loss: 1.2447, Validation Loss 0.3503\n",
      "Epoch 71 Batch 6:  Training Loss: 1.1017, Validation Loss 0.3510\n",
      "Epoch 71 Batch 7:  Training Loss: 0.3460, Validation Loss 0.3516\n",
      "Epoch 71 Batch 8:  Training Loss: 0.3427, Validation Loss 0.3517\n",
      "Epoch 71 Batch 9:  Training Loss: 0.2220, Validation Loss 0.3519\n",
      "Epoch 71 Batch 10:  Training Loss: 0.1535, Validation Loss 0.3522\n",
      "Epoch 71 Batch 11:  Training Loss: 0.1971, Validation Loss 0.3525\n",
      "Epoch 71 Batch 12:  Training Loss: 0.1288, Validation Loss 0.3528\n",
      "Epoch 71 Batch 13:  Training Loss: 0.1441, Validation Loss 0.3531\n",
      "Epoch 71 Batch 14:  Training Loss: 0.1596, Validation Loss 0.3533\n",
      "Epoch 72 Batch 0:  Training Loss: 0.2541, Validation Loss 0.3538\n",
      "Epoch 72 Batch 1:  Training Loss: 0.3293, Validation Loss 0.3526\n",
      "Epoch 72 Batch 2:  Training Loss: 0.1431, Validation Loss 0.3467\n",
      "Epoch 72 Batch 3:  Training Loss: 0.1844, Validation Loss 0.3459\n",
      "Epoch 72 Batch 4:  Training Loss: 0.1882, Validation Loss 0.3445\n",
      "Epoch 72 Batch 5:  Training Loss: 1.2320, Validation Loss 0.3452\n",
      "Epoch 72 Batch 6:  Training Loss: 1.0905, Validation Loss 0.3459\n",
      "Epoch 72 Batch 7:  Training Loss: 0.3344, Validation Loss 0.3466\n",
      "Epoch 72 Batch 8:  Training Loss: 0.3122, Validation Loss 0.3469\n",
      "Epoch 72 Batch 9:  Training Loss: 0.2322, Validation Loss 0.3473\n",
      "Epoch 72 Batch 10:  Training Loss: 0.1555, Validation Loss 0.3477\n",
      "Epoch 72 Batch 11:  Training Loss: 0.1954, Validation Loss 0.3482\n",
      "Epoch 72 Batch 12:  Training Loss: 0.1252, Validation Loss 0.3487\n",
      "Epoch 72 Batch 13:  Training Loss: 0.1376, Validation Loss 0.3490\n",
      "Epoch 72 Batch 14:  Training Loss: 0.1542, Validation Loss 0.3493\n",
      "Epoch 73 Batch 0:  Training Loss: 0.2536, Validation Loss 0.3499\n",
      "Epoch 73 Batch 1:  Training Loss: 0.3285, Validation Loss 0.3487\n",
      "Epoch 73 Batch 2:  Training Loss: 0.1420, Validation Loss 0.3429\n",
      "Epoch 73 Batch 3:  Training Loss: 0.1813, Validation Loss 0.3420\n",
      "Epoch 73 Batch 4:  Training Loss: 0.1888, Validation Loss 0.3404\n",
      "Epoch 73 Batch 5:  Training Loss: 1.2215, Validation Loss 0.3409\n",
      "Epoch 73 Batch 6:  Training Loss: 1.0988, Validation Loss 0.3416\n",
      "Epoch 73 Batch 7:  Training Loss: 0.3480, Validation Loss 0.3422\n",
      "Epoch 73 Batch 8:  Training Loss: 0.3482, Validation Loss 0.3425\n",
      "Epoch 73 Batch 9:  Training Loss: 0.2183, Validation Loss 0.3428\n",
      "Epoch 73 Batch 10:  Training Loss: 0.1512, Validation Loss 0.3431\n",
      "Epoch 73 Batch 11:  Training Loss: 0.1949, Validation Loss 0.3434\n",
      "Epoch 73 Batch 12:  Training Loss: 0.1285, Validation Loss 0.3436\n",
      "Epoch 73 Batch 13:  Training Loss: 0.1431, Validation Loss 0.3438\n",
      "Epoch 73 Batch 14:  Training Loss: 0.1593, Validation Loss 0.3439\n",
      "Epoch 74 Batch 0:  Training Loss: 0.2539, Validation Loss 0.3445\n",
      "Epoch 74 Batch 1:  Training Loss: 0.3271, Validation Loss 0.3434\n",
      "Epoch 74 Batch 2:  Training Loss: 0.1394, Validation Loss 0.3382\n",
      "Epoch 74 Batch 3:  Training Loss: 0.1811, Validation Loss 0.3375\n",
      "Epoch 74 Batch 4:  Training Loss: 0.1890, Validation Loss 0.3363\n",
      "Epoch 74 Batch 5:  Training Loss: 1.2095, Validation Loss 0.3370\n",
      "Epoch 74 Batch 6:  Training Loss: 1.0809, Validation Loss 0.3378\n",
      "Epoch 74 Batch 7:  Training Loss: 0.3272, Validation Loss 0.3386\n",
      "Epoch 74 Batch 8:  Training Loss: 0.2926, Validation Loss 0.3392\n",
      "Epoch 74 Batch 9:  Training Loss: 0.2384, Validation Loss 0.3398\n",
      "Epoch 74 Batch 10:  Training Loss: 0.1554, Validation Loss 0.3403\n",
      "Epoch 74 Batch 11:  Training Loss: 0.1924, Validation Loss 0.3408\n",
      "Epoch 74 Batch 12:  Training Loss: 0.1216, Validation Loss 0.3413\n",
      "Epoch 74 Batch 13:  Training Loss: 0.1313, Validation Loss 0.3417\n",
      "Epoch 74 Batch 14:  Training Loss: 0.1492, Validation Loss 0.3420\n",
      "Epoch 75 Batch 0:  Training Loss: 0.2532, Validation Loss 0.3427\n",
      "Epoch 75 Batch 1:  Training Loss: 0.3276, Validation Loss 0.3415\n",
      "Epoch 75 Batch 2:  Training Loss: 0.1402, Validation Loss 0.3358\n",
      "Epoch 75 Batch 3:  Training Loss: 0.1766, Validation Loss 0.3349\n",
      "Epoch 75 Batch 4:  Training Loss: 0.1896, Validation Loss 0.3333\n",
      "Epoch 75 Batch 5:  Training Loss: 1.1999, Validation Loss 0.3337\n",
      "Epoch 75 Batch 6:  Training Loss: 1.1025, Validation Loss 0.3342\n",
      "Epoch 75 Batch 7:  Training Loss: 0.3582, Validation Loss 0.3347\n",
      "Epoch 75 Batch 8:  Training Loss: 0.3697, Validation Loss 0.3346\n",
      "Epoch 75 Batch 9:  Training Loss: 0.2116, Validation Loss 0.3348\n",
      "Epoch 75 Batch 10:  Training Loss: 0.1481, Validation Loss 0.3350\n",
      "Epoch 75 Batch 11:  Training Loss: 0.1931, Validation Loss 0.3352\n",
      "Epoch 75 Batch 12:  Training Loss: 0.1301, Validation Loss 0.3353\n",
      "Epoch 75 Batch 13:  Training Loss: 0.1451, Validation Loss 0.3354\n",
      "Epoch 75 Batch 14:  Training Loss: 0.1614, Validation Loss 0.3354\n",
      "Epoch 76 Batch 0:  Training Loss: 0.2539, Validation Loss 0.3359\n",
      "Epoch 76 Batch 1:  Training Loss: 0.3254, Validation Loss 0.3349\n",
      "Epoch 76 Batch 2:  Training Loss: 0.1359, Validation Loss 0.3303\n",
      "Epoch 76 Batch 3:  Training Loss: 0.1786, Validation Loss 0.3299\n",
      "Epoch 76 Batch 4:  Training Loss: 0.1897, Validation Loss 0.3290\n",
      "Epoch 76 Batch 5:  Training Loss: 1.1885, Validation Loss 0.3298\n",
      "Epoch 76 Batch 6:  Training Loss: 1.0725, Validation Loss 0.3307\n",
      "Epoch 76 Batch 7:  Training Loss: 0.3200, Validation Loss 0.3315\n",
      "Epoch 76 Batch 8:  Training Loss: 0.2627, Validation Loss 0.3322\n",
      "Epoch 76 Batch 9:  Training Loss: 0.2537, Validation Loss 0.3329\n",
      "Epoch 76 Batch 10:  Training Loss: 0.1567, Validation Loss 0.3337\n",
      "Epoch 76 Batch 11:  Training Loss: 0.1886, Validation Loss 0.3344\n",
      "Epoch 76 Batch 12:  Training Loss: 0.1159, Validation Loss 0.3352\n",
      "Epoch 76 Batch 13:  Training Loss: 0.1216, Validation Loss 0.3359\n",
      "Epoch 76 Batch 14:  Training Loss: 0.1413, Validation Loss 0.3364\n",
      "Epoch 77 Batch 0:  Training Loss: 0.2529, Validation Loss 0.3373\n",
      "Epoch 77 Batch 1:  Training Loss: 0.3275, Validation Loss 0.3362\n",
      "Epoch 77 Batch 2:  Training Loss: 0.1400, Validation Loss 0.3304\n",
      "Epoch 77 Batch 3:  Training Loss: 0.1716, Validation Loss 0.3294\n",
      "Epoch 77 Batch 4:  Training Loss: 0.1901, Validation Loss 0.3275\n",
      "Epoch 77 Batch 5:  Training Loss: 1.1809, Validation Loss 0.3277\n",
      "Epoch 77 Batch 6:  Training Loss: 1.1283, Validation Loss 0.3280\n",
      "Epoch 77 Batch 7:  Training Loss: 0.3947, Validation Loss 0.3281\n",
      "Epoch 77 Batch 8:  Training Loss: 0.4324, Validation Loss 0.3276\n",
      "Epoch 77 Batch 9:  Training Loss: 0.2021, Validation Loss 0.3273\n",
      "Epoch 77 Batch 10:  Training Loss: 0.1438, Validation Loss 0.3270\n",
      "Epoch 77 Batch 11:  Training Loss: 0.1929, Validation Loss 0.3269\n",
      "Epoch 77 Batch 12:  Training Loss: 0.1366, Validation Loss 0.3268\n",
      "Epoch 77 Batch 13:  Training Loss: 0.1542, Validation Loss 0.3267\n",
      "Epoch 77 Batch 14:  Training Loss: 0.1698, Validation Loss 0.3266\n",
      "Epoch 78 Batch 0:  Training Loss: 0.2545, Validation Loss 0.3271\n",
      "Epoch 78 Batch 1:  Training Loss: 0.3235, Validation Loss 0.3264\n",
      "Epoch 78 Batch 2:  Training Loss: 0.1319, Validation Loss 0.3230\n",
      "Epoch 78 Batch 3:  Training Loss: 0.1788, Validation Loss 0.3232\n",
      "Epoch 78 Batch 4:  Training Loss: 0.1901, Validation Loss 0.3228\n",
      "Epoch 78 Batch 5:  Training Loss: 1.1716, Validation Loss 0.3240\n",
      "Epoch 78 Batch 6:  Training Loss: 1.0848, Validation Loss 0.3252\n",
      "Epoch 78 Batch 7:  Training Loss: 0.3313, Validation Loss 0.3263\n",
      "Epoch 78 Batch 8:  Training Loss: 0.2265, Validation Loss 0.3274\n",
      "Epoch 78 Batch 9:  Training Loss: 0.2962, Validation Loss 0.3287\n",
      "Epoch 78 Batch 10:  Training Loss: 0.1613, Validation Loss 0.3301\n",
      "Epoch 78 Batch 11:  Training Loss: 0.1837, Validation Loss 0.3316\n",
      "Epoch 78 Batch 12:  Training Loss: 0.1066, Validation Loss 0.3332\n",
      "Epoch 78 Batch 13:  Training Loss: 0.1058, Validation Loss 0.3348\n",
      "Epoch 78 Batch 14:  Training Loss: 0.1283, Validation Loss 0.3362\n",
      "Epoch 79 Batch 0:  Training Loss: 0.2525, Validation Loss 0.3374\n",
      "Epoch 79 Batch 1:  Training Loss: 0.3302, Validation Loss 0.3361\n",
      "Epoch 79 Batch 2:  Training Loss: 0.1437, Validation Loss 0.3292\n",
      "Epoch 79 Batch 3:  Training Loss: 0.1651, Validation Loss 0.3273\n",
      "Epoch 79 Batch 4:  Training Loss: 0.1910, Validation Loss 0.3245\n",
      "Epoch 79 Batch 5:  Training Loss: 1.1685, Validation Loss 0.3240\n",
      "Epoch 79 Batch 6:  Training Loss: 1.2452, Validation Loss 0.3234\n",
      "Epoch 79 Batch 7:  Training Loss: 0.5306, Validation Loss 0.3226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 8:  Training Loss: 0.6288, Validation Loss 0.3214\n",
      "Epoch 79 Batch 9:  Training Loss: 0.2023, Validation Loss 0.3203\n",
      "Epoch 79 Batch 10:  Training Loss: 0.1368, Validation Loss 0.3193\n",
      "Epoch 79 Batch 11:  Training Loss: 0.1959, Validation Loss 0.3186\n",
      "Epoch 79 Batch 12:  Training Loss: 0.1552, Validation Loss 0.3180\n",
      "Epoch 79 Batch 13:  Training Loss: 0.1813, Validation Loss 0.3174\n",
      "Epoch 79 Batch 14:  Training Loss: 0.1943, Validation Loss 0.3169\n",
      "Epoch 80 Batch 0:  Training Loss: 0.2558, Validation Loss 0.3171\n",
      "Epoch 80 Batch 1:  Training Loss: 0.3210, Validation Loss 0.3166\n",
      "Epoch 80 Batch 2:  Training Loss: 0.1271, Validation Loss 0.3145\n",
      "Epoch 80 Batch 3:  Training Loss: 0.1830, Validation Loss 0.3149\n",
      "Epoch 80 Batch 4:  Training Loss: 0.1901, Validation Loss 0.3150\n",
      "Epoch 80 Batch 5:  Training Loss: 1.1674, Validation Loss 0.3162\n",
      "Epoch 80 Batch 6:  Training Loss: 1.2288, Validation Loss 0.3175\n",
      "Epoch 80 Batch 7:  Training Loss: 0.4786, Validation Loss 0.3193\n",
      "Epoch 80 Batch 8:  Training Loss: 0.2605, Validation Loss 0.3213\n",
      "Epoch 80 Batch 9:  Training Loss: 0.4463, Validation Loss 0.3237\n",
      "Epoch 80 Batch 10:  Training Loss: 0.1803, Validation Loss 0.3268\n",
      "Epoch 80 Batch 11:  Training Loss: 0.1790, Validation Loss 0.3306\n",
      "Epoch 80 Batch 12:  Training Loss: 0.0966, Validation Loss 0.3340\n",
      "Epoch 80 Batch 13:  Training Loss: 0.0855, Validation Loss 0.3374\n",
      "Epoch 80 Batch 14:  Training Loss: 0.1116, Validation Loss 0.3411\n",
      "Epoch 81 Batch 0:  Training Loss: 0.2528, Validation Loss 0.3443\n",
      "Epoch 81 Batch 1:  Training Loss: 0.3373, Validation Loss 0.3437\n",
      "Epoch 81 Batch 2:  Training Loss: 0.1551, Validation Loss 0.3350\n",
      "Epoch 81 Batch 3:  Training Loss: 0.1535, Validation Loss 0.3318\n",
      "Epoch 81 Batch 4:  Training Loss: 0.1934, Validation Loss 0.3275\n",
      "Epoch 81 Batch 5:  Training Loss: 1.1846, Validation Loss 0.3258\n",
      "Epoch 81 Batch 6:  Training Loss: 1.7334, Validation Loss 0.3231\n",
      "Epoch 81 Batch 7:  Training Loss: 1.0681, Validation Loss 0.3189\n",
      "Epoch 81 Batch 8:  Training Loss: 1.3178, Validation Loss 0.3127\n",
      "Epoch 81 Batch 9:  Training Loss: 0.3173, Validation Loss 0.3083\n",
      "Epoch 81 Batch 10:  Training Loss: 0.1304, Validation Loss 0.3064\n",
      "Epoch 81 Batch 11:  Training Loss: 0.1964, Validation Loss 0.3047\n",
      "Epoch 81 Batch 12:  Training Loss: 0.1930, Validation Loss 0.3036\n",
      "Epoch 81 Batch 13:  Training Loss: 0.2419, Validation Loss 0.3031\n",
      "Epoch 81 Batch 14:  Training Loss: 0.2518, Validation Loss 0.3035\n",
      "Epoch 82 Batch 0:  Training Loss: 0.2598, Validation Loss 0.3038\n",
      "Epoch 82 Batch 1:  Training Loss: 0.3199, Validation Loss 0.3040\n",
      "Epoch 82 Batch 2:  Training Loss: 0.1203, Validation Loss 0.3042\n",
      "Epoch 82 Batch 3:  Training Loss: 0.1921, Validation Loss 0.3037\n",
      "Epoch 82 Batch 4:  Training Loss: 0.1908, Validation Loss 0.3031\n",
      "Epoch 82 Batch 5:  Training Loss: 1.2031, Validation Loss 0.3028\n",
      "Epoch 82 Batch 6:  Training Loss: 1.9283, Validation Loss 0.3029\n",
      "Epoch 82 Batch 7:  Training Loss: 1.2585, Validation Loss 0.3042\n",
      "Epoch 82 Batch 8:  Training Loss: 0.7790, Validation Loss 0.3064\n",
      "Epoch 82 Batch 9:  Training Loss: 1.0879, Validation Loss 0.3109\n",
      "Epoch 82 Batch 10:  Training Loss: 0.3014, Validation Loss 0.3162\n",
      "Epoch 82 Batch 11:  Training Loss: 0.1815, Validation Loss 0.3246\n",
      "Epoch 82 Batch 12:  Training Loss: 0.0938, Validation Loss 0.3354\n",
      "Epoch 82 Batch 13:  Training Loss: 0.0767, Validation Loss 0.3497\n",
      "Epoch 82 Batch 14:  Training Loss: 0.1062, Validation Loss 0.3658\n",
      "Epoch 83 Batch 0:  Training Loss: 0.2578, Validation Loss 0.3795\n",
      "Epoch 83 Batch 1:  Training Loss: 0.3534, Validation Loss 0.3855\n",
      "Epoch 83 Batch 2:  Training Loss: 0.1853, Validation Loss 0.3752\n",
      "Epoch 83 Batch 3:  Training Loss: 0.1388, Validation Loss 0.3700\n",
      "Epoch 83 Batch 4:  Training Loss: 0.1987, Validation Loss 0.3619\n",
      "Epoch 83 Batch 5:  Training Loss: 1.2855, Validation Loss 0.3579\n",
      "Epoch 83 Batch 6:  Training Loss: 3.1763, Validation Loss 0.3502\n",
      "Epoch 83 Batch 7:  Training Loss: 2.6391, Validation Loss 0.3384\n",
      "Epoch 83 Batch 8:  Training Loss: 3.4762, Validation Loss 0.3186\n",
      "Epoch 83 Batch 9:  Training Loss: 1.1126, Validation Loss 0.3008\n",
      "Epoch 83 Batch 10:  Training Loss: 0.2640, Validation Loss 0.2878\n",
      "Epoch 83 Batch 11:  Training Loss: 0.1861, Validation Loss 0.2827\n",
      "Epoch 83 Batch 12:  Training Loss: 0.1492, Validation Loss 0.2817\n",
      "Epoch 83 Batch 13:  Training Loss: 0.2410, Validation Loss 0.2842\n",
      "Epoch 83 Batch 14:  Training Loss: 0.2728, Validation Loss 0.2897\n",
      "Epoch 84 Batch 0:  Training Loss: 0.2678, Validation Loss 0.2953\n",
      "Epoch 84 Batch 1:  Training Loss: 0.3289, Validation Loss 0.3007\n",
      "Epoch 84 Batch 2:  Training Loss: 0.1270, Validation Loss 0.3050\n",
      "Epoch 84 Batch 3:  Training Loss: 0.1979, Validation Loss 0.3036\n",
      "Epoch 84 Batch 4:  Training Loss: 0.1930, Validation Loss 0.3024\n",
      "Epoch 84 Batch 5:  Training Loss: 1.2518, Validation Loss 0.2993\n",
      "Epoch 84 Batch 6:  Training Loss: 2.9595, Validation Loss 0.2954\n",
      "Epoch 84 Batch 7:  Training Loss: 2.5185, Validation Loss 0.2902\n",
      "Epoch 84 Batch 8:  Training Loss: 1.8417, Validation Loss 0.2843\n",
      "Epoch 84 Batch 9:  Training Loss: 2.8250, Validation Loss 0.2786\n",
      "Epoch 84 Batch 10:  Training Loss: 0.8938, Validation Loss 0.2774\n",
      "Epoch 84 Batch 11:  Training Loss: 0.3678, Validation Loss 0.2820\n",
      "Epoch 84 Batch 12:  Training Loss: 0.1464, Validation Loss 0.2933\n",
      "Epoch 84 Batch 13:  Training Loss: 0.1149, Validation Loss 0.3103\n",
      "Epoch 84 Batch 14:  Training Loss: 0.1172, Validation Loss 0.3314\n",
      "Epoch 85 Batch 0:  Training Loss: 0.2528, Validation Loss 0.3511\n",
      "Epoch 85 Batch 1:  Training Loss: 0.3702, Validation Loss 0.3628\n",
      "Epoch 85 Batch 2:  Training Loss: 0.1875, Validation Loss 0.3586\n",
      "Epoch 85 Batch 3:  Training Loss: 0.1370, Validation Loss 0.3555\n",
      "Epoch 85 Batch 4:  Training Loss: 0.2007, Validation Loss 0.3496\n",
      "Epoch 85 Batch 5:  Training Loss: 1.3242, Validation Loss 0.3471\n",
      "Epoch 85 Batch 6:  Training Loss: 3.2338, Validation Loss 0.3408\n",
      "Epoch 85 Batch 7:  Training Loss: 2.6285, Validation Loss 0.3331\n",
      "Epoch 85 Batch 8:  Training Loss: 4.3604, Validation Loss 0.3235\n",
      "Epoch 85 Batch 9:  Training Loss: 1.3216, Validation Loss 0.3129\n",
      "Epoch 85 Batch 10:  Training Loss: 0.4848, Validation Loss 0.3040\n",
      "Epoch 85 Batch 11:  Training Loss: 0.4356, Validation Loss 0.2967\n",
      "Epoch 85 Batch 12:  Training Loss: 0.0914, Validation Loss 0.2909\n",
      "Epoch 85 Batch 13:  Training Loss: 0.1315, Validation Loss 0.2863\n",
      "Epoch 85 Batch 14:  Training Loss: 0.1612, Validation Loss 0.2827\n",
      "Epoch 86 Batch 0:  Training Loss: 0.2428, Validation Loss 0.2797\n",
      "Epoch 86 Batch 1:  Training Loss: 0.3483, Validation Loss 0.2763\n",
      "Epoch 86 Batch 2:  Training Loss: 0.1409, Validation Loss 0.2729\n",
      "Epoch 86 Batch 3:  Training Loss: 0.1574, Validation Loss 0.2716\n",
      "Epoch 86 Batch 4:  Training Loss: 0.1885, Validation Loss 0.2706\n",
      "Epoch 86 Batch 5:  Training Loss: 1.1536, Validation Loss 0.2704\n",
      "Epoch 86 Batch 6:  Training Loss: 1.5176, Validation Loss 0.2704\n",
      "Epoch 86 Batch 7:  Training Loss: 1.0035, Validation Loss 0.2704\n",
      "Epoch 86 Batch 8:  Training Loss: 0.5675, Validation Loss 0.2705\n",
      "Epoch 86 Batch 9:  Training Loss: 1.8282, Validation Loss 0.2706\n",
      "Epoch 86 Batch 10:  Training Loss: 0.7729, Validation Loss 0.2707\n",
      "Epoch 86 Batch 11:  Training Loss: 0.4566, Validation Loss 0.2708\n",
      "Epoch 86 Batch 12:  Training Loss: 0.2568, Validation Loss 0.2709\n",
      "Epoch 86 Batch 13:  Training Loss: 0.2134, Validation Loss 0.2710\n",
      "Epoch 86 Batch 14:  Training Loss: 0.1659, Validation Loss 0.2711\n",
      "Epoch 87 Batch 0:  Training Loss: 0.2444, Validation Loss 0.2714\n",
      "Epoch 87 Batch 1:  Training Loss: 0.3466, Validation Loss 0.2715\n",
      "Epoch 87 Batch 2:  Training Loss: 0.1394, Validation Loss 0.2709\n",
      "Epoch 87 Batch 3:  Training Loss: 0.1565, Validation Loss 0.2714\n",
      "Epoch 87 Batch 4:  Training Loss: 0.1894, Validation Loss 0.2717\n",
      "Epoch 87 Batch 5:  Training Loss: 1.1740, Validation Loss 0.2729\n",
      "Epoch 87 Batch 6:  Training Loss: 1.4107, Validation Loss 0.2737\n",
      "Epoch 87 Batch 7:  Training Loss: 0.6990, Validation Loss 0.2744\n",
      "Epoch 87 Batch 8:  Training Loss: 1.3039, Validation Loss 0.2747\n",
      "Epoch 87 Batch 9:  Training Loss: 0.2057, Validation Loss 0.2750\n",
      "Epoch 87 Batch 10:  Training Loss: 0.1591, Validation Loss 0.2753\n",
      "Epoch 87 Batch 11:  Training Loss: 0.2685, Validation Loss 0.2755\n",
      "Epoch 87 Batch 12:  Training Loss: 0.0970, Validation Loss 0.2756\n",
      "Epoch 87 Batch 13:  Training Loss: 0.1323, Validation Loss 0.2758\n",
      "Epoch 87 Batch 14:  Training Loss: 0.1450, Validation Loss 0.2759\n",
      "Epoch 88 Batch 0:  Training Loss: 0.2421, Validation Loss 0.2762\n",
      "Epoch 88 Batch 1:  Training Loss: 0.3485, Validation Loss 0.2755\n",
      "Epoch 88 Batch 2:  Training Loss: 0.1413, Validation Loss 0.2736\n",
      "Epoch 88 Batch 3:  Training Loss: 0.1541, Validation Loss 0.2734\n",
      "Epoch 88 Batch 4:  Training Loss: 0.1885, Validation Loss 0.2729\n",
      "Epoch 88 Batch 5:  Training Loss: 1.1260, Validation Loss 0.2734\n",
      "Epoch 88 Batch 6:  Training Loss: 1.0729, Validation Loss 0.2738\n",
      "Epoch 88 Batch 7:  Training Loss: 0.3712, Validation Loss 0.2741\n",
      "Epoch 88 Batch 8:  Training Loss: 0.3161, Validation Loss 0.2744\n",
      "Epoch 88 Batch 9:  Training Loss: 0.4373, Validation Loss 0.2746\n",
      "Epoch 88 Batch 10:  Training Loss: 0.2574, Validation Loss 0.2748\n",
      "Epoch 88 Batch 11:  Training Loss: 0.2587, Validation Loss 0.2751\n",
      "Epoch 88 Batch 12:  Training Loss: 0.1529, Validation Loss 0.2754\n",
      "Epoch 88 Batch 13:  Training Loss: 0.1719, Validation Loss 0.2757\n",
      "Epoch 88 Batch 14:  Training Loss: 0.1583, Validation Loss 0.2760\n",
      "Epoch 89 Batch 0:  Training Loss: 0.2410, Validation Loss 0.2765\n",
      "Epoch 89 Batch 1:  Training Loss: 0.3481, Validation Loss 0.2758\n",
      "Epoch 89 Batch 2:  Training Loss: 0.1404, Validation Loss 0.2737\n",
      "Epoch 89 Batch 3:  Training Loss: 0.1536, Validation Loss 0.2734\n",
      "Epoch 89 Batch 4:  Training Loss: 0.1878, Validation Loss 0.2728\n",
      "Epoch 89 Batch 5:  Training Loss: 1.1217, Validation Loss 0.2732\n",
      "Epoch 89 Batch 6:  Training Loss: 1.1038, Validation Loss 0.2735\n",
      "Epoch 89 Batch 7:  Training Loss: 0.3866, Validation Loss 0.2736\n",
      "Epoch 89 Batch 8:  Training Loss: 0.5222, Validation Loss 0.2734\n",
      "Epoch 89 Batch 9:  Training Loss: 0.2186, Validation Loss 0.2734\n",
      "Epoch 89 Batch 10:  Training Loss: 0.1625, Validation Loss 0.2735\n",
      "Epoch 89 Batch 11:  Training Loss: 0.2236, Validation Loss 0.2735\n",
      "Epoch 89 Batch 12:  Training Loss: 0.1154, Validation Loss 0.2736\n",
      "Epoch 89 Batch 13:  Training Loss: 0.1449, Validation Loss 0.2738\n",
      "Epoch 89 Batch 14:  Training Loss: 0.1471, Validation Loss 0.2739\n",
      "Epoch 90 Batch 0:  Training Loss: 0.2398, Validation Loss 0.2744\n",
      "Epoch 90 Batch 1:  Training Loss: 0.3469, Validation Loss 0.2737\n",
      "Epoch 90 Batch 2:  Training Loss: 0.1382, Validation Loss 0.2719\n",
      "Epoch 90 Batch 3:  Training Loss: 0.1544, Validation Loss 0.2719\n",
      "Epoch 90 Batch 4:  Training Loss: 0.1870, Validation Loss 0.2714\n",
      "Epoch 90 Batch 5:  Training Loss: 1.1100, Validation Loss 0.2720\n",
      "Epoch 90 Batch 6:  Training Loss: 1.0815, Validation Loss 0.2725\n",
      "Epoch 90 Batch 7:  Training Loss: 0.3620, Validation Loss 0.2728\n",
      "Epoch 90 Batch 8:  Training Loss: 0.4273, Validation Loss 0.2729\n",
      "Epoch 90 Batch 9:  Training Loss: 0.2544, Validation Loss 0.2731\n",
      "Epoch 90 Batch 10:  Training Loss: 0.1754, Validation Loss 0.2734\n",
      "Epoch 90 Batch 11:  Training Loss: 0.2171, Validation Loss 0.2737\n",
      "Epoch 90 Batch 12:  Training Loss: 0.1200, Validation Loss 0.2740\n",
      "Epoch 90 Batch 13:  Training Loss: 0.1465, Validation Loss 0.2743\n",
      "Epoch 90 Batch 14:  Training Loss: 0.1482, Validation Loss 0.2746\n",
      "Epoch 91 Batch 0:  Training Loss: 0.2393, Validation Loss 0.2752\n",
      "Epoch 91 Batch 1:  Training Loss: 0.3459, Validation Loss 0.2745\n",
      "Epoch 91 Batch 2:  Training Loss: 0.1374, Validation Loss 0.2724\n",
      "Epoch 91 Batch 3:  Training Loss: 0.1533, Validation Loss 0.2724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 4:  Training Loss: 0.1870, Validation Loss 0.2719\n",
      "Epoch 91 Batch 5:  Training Loss: 1.1001, Validation Loss 0.2725\n",
      "Epoch 91 Batch 6:  Training Loss: 1.0778, Validation Loss 0.2729\n",
      "Epoch 91 Batch 7:  Training Loss: 0.3546, Validation Loss 0.2733\n",
      "Epoch 91 Batch 8:  Training Loss: 0.4154, Validation Loss 0.2735\n",
      "Epoch 91 Batch 9:  Training Loss: 0.2405, Validation Loss 0.2738\n",
      "Epoch 91 Batch 10:  Training Loss: 0.1670, Validation Loss 0.2742\n",
      "Epoch 91 Batch 11:  Training Loss: 0.2054, Validation Loss 0.2746\n",
      "Epoch 91 Batch 12:  Training Loss: 0.1153, Validation Loss 0.2751\n",
      "Epoch 91 Batch 13:  Training Loss: 0.1390, Validation Loss 0.2756\n",
      "Epoch 91 Batch 14:  Training Loss: 0.1450, Validation Loss 0.2760\n",
      "Epoch 92 Batch 0:  Training Loss: 0.2396, Validation Loss 0.2768\n",
      "Epoch 92 Batch 1:  Training Loss: 0.3439, Validation Loss 0.2762\n",
      "Epoch 92 Batch 2:  Training Loss: 0.1372, Validation Loss 0.2739\n",
      "Epoch 92 Batch 3:  Training Loss: 0.1529, Validation Loss 0.2739\n",
      "Epoch 92 Batch 4:  Training Loss: 0.1871, Validation Loss 0.2733\n",
      "Epoch 92 Batch 5:  Training Loss: 1.0912, Validation Loss 0.2740\n",
      "Epoch 92 Batch 6:  Training Loss: 1.0764, Validation Loss 0.2745\n",
      "Epoch 92 Batch 7:  Training Loss: 0.3469, Validation Loss 0.2748\n",
      "Epoch 92 Batch 8:  Training Loss: 0.4009, Validation Loss 0.2749\n",
      "Epoch 92 Batch 9:  Training Loss: 0.2327, Validation Loss 0.2750\n",
      "Epoch 92 Batch 10:  Training Loss: 0.1604, Validation Loss 0.2752\n",
      "Epoch 92 Batch 11:  Training Loss: 0.1968, Validation Loss 0.2754\n",
      "Epoch 92 Batch 12:  Training Loss: 0.1137, Validation Loss 0.2757\n",
      "Epoch 92 Batch 13:  Training Loss: 0.1352, Validation Loss 0.2759\n",
      "Epoch 92 Batch 14:  Training Loss: 0.1434, Validation Loss 0.2761\n",
      "Epoch 93 Batch 0:  Training Loss: 0.2406, Validation Loss 0.2767\n",
      "Epoch 93 Batch 1:  Training Loss: 0.3421, Validation Loss 0.2761\n",
      "Epoch 93 Batch 2:  Training Loss: 0.1361, Validation Loss 0.2741\n",
      "Epoch 93 Batch 3:  Training Loss: 0.1545, Validation Loss 0.2742\n",
      "Epoch 93 Batch 4:  Training Loss: 0.1870, Validation Loss 0.2739\n",
      "Epoch 93 Batch 5:  Training Loss: 1.0826, Validation Loss 0.2746\n",
      "Epoch 93 Batch 6:  Training Loss: 1.0674, Validation Loss 0.2752\n",
      "Epoch 93 Batch 7:  Training Loss: 0.3380, Validation Loss 0.2756\n",
      "Epoch 93 Batch 8:  Training Loss: 0.3714, Validation Loss 0.2756\n",
      "Epoch 93 Batch 9:  Training Loss: 0.2353, Validation Loss 0.2757\n",
      "Epoch 93 Batch 10:  Training Loss: 0.1587, Validation Loss 0.2759\n",
      "Epoch 93 Batch 11:  Training Loss: 0.1904, Validation Loss 0.2761\n",
      "Epoch 93 Batch 12:  Training Loss: 0.1130, Validation Loss 0.2762\n",
      "Epoch 93 Batch 13:  Training Loss: 0.1322, Validation Loss 0.2764\n",
      "Epoch 93 Batch 14:  Training Loss: 0.1420, Validation Loss 0.2767\n",
      "Epoch 94 Batch 0:  Training Loss: 0.2418, Validation Loss 0.2773\n",
      "Epoch 94 Batch 1:  Training Loss: 0.3413, Validation Loss 0.2768\n",
      "Epoch 94 Batch 2:  Training Loss: 0.1358, Validation Loss 0.2748\n",
      "Epoch 94 Batch 3:  Training Loss: 0.1552, Validation Loss 0.2750\n",
      "Epoch 94 Batch 4:  Training Loss: 0.1874, Validation Loss 0.2746\n",
      "Epoch 94 Batch 5:  Training Loss: 1.0742, Validation Loss 0.2753\n",
      "Epoch 94 Batch 6:  Training Loss: 1.0618, Validation Loss 0.2759\n",
      "Epoch 94 Batch 7:  Training Loss: 0.3319, Validation Loss 0.2763\n",
      "Epoch 94 Batch 8:  Training Loss: 0.3548, Validation Loss 0.2763\n",
      "Epoch 94 Batch 9:  Training Loss: 0.2263, Validation Loss 0.2763\n",
      "Epoch 94 Batch 10:  Training Loss: 0.1532, Validation Loss 0.2765\n",
      "Epoch 94 Batch 11:  Training Loss: 0.1834, Validation Loss 0.2766\n",
      "Epoch 94 Batch 12:  Training Loss: 0.1115, Validation Loss 0.2767\n",
      "Epoch 94 Batch 13:  Training Loss: 0.1290, Validation Loss 0.2769\n",
      "Epoch 94 Batch 14:  Training Loss: 0.1412, Validation Loss 0.2770\n",
      "Epoch 95 Batch 0:  Training Loss: 0.2431, Validation Loss 0.2777\n",
      "Epoch 95 Batch 1:  Training Loss: 0.3403, Validation Loss 0.2772\n",
      "Epoch 95 Batch 2:  Training Loss: 0.1352, Validation Loss 0.2753\n",
      "Epoch 95 Batch 3:  Training Loss: 0.1557, Validation Loss 0.2755\n",
      "Epoch 95 Batch 4:  Training Loss: 0.1879, Validation Loss 0.2751\n",
      "Epoch 95 Batch 5:  Training Loss: 1.0663, Validation Loss 0.2759\n",
      "Epoch 95 Batch 6:  Training Loss: 1.0562, Validation Loss 0.2766\n",
      "Epoch 95 Batch 7:  Training Loss: 0.3216, Validation Loss 0.2771\n",
      "Epoch 95 Batch 8:  Training Loss: 0.3317, Validation Loss 0.2770\n",
      "Epoch 95 Batch 9:  Training Loss: 0.2247, Validation Loss 0.2772\n",
      "Epoch 95 Batch 10:  Training Loss: 0.1490, Validation Loss 0.2774\n",
      "Epoch 95 Batch 11:  Training Loss: 0.1782, Validation Loss 0.2777\n",
      "Epoch 95 Batch 12:  Training Loss: 0.1107, Validation Loss 0.2780\n",
      "Epoch 95 Batch 13:  Training Loss: 0.1257, Validation Loss 0.2783\n",
      "Epoch 95 Batch 14:  Training Loss: 0.1400, Validation Loss 0.2786\n",
      "Epoch 96 Batch 0:  Training Loss: 0.2444, Validation Loss 0.2794\n",
      "Epoch 96 Batch 1:  Training Loss: 0.3390, Validation Loss 0.2788\n",
      "Epoch 96 Batch 2:  Training Loss: 0.1345, Validation Loss 0.2767\n",
      "Epoch 96 Batch 3:  Training Loss: 0.1555, Validation Loss 0.2769\n",
      "Epoch 96 Batch 4:  Training Loss: 0.1886, Validation Loss 0.2765\n",
      "Epoch 96 Batch 5:  Training Loss: 1.0591, Validation Loss 0.2772\n",
      "Epoch 96 Batch 6:  Training Loss: 1.0536, Validation Loss 0.2778\n",
      "Epoch 96 Batch 7:  Training Loss: 0.3164, Validation Loss 0.2782\n",
      "Epoch 96 Batch 8:  Training Loss: 0.3210, Validation Loss 0.2781\n",
      "Epoch 96 Batch 9:  Training Loss: 0.2207, Validation Loss 0.2781\n",
      "Epoch 96 Batch 10:  Training Loss: 0.1434, Validation Loss 0.2783\n",
      "Epoch 96 Batch 11:  Training Loss: 0.1747, Validation Loss 0.2785\n",
      "Epoch 96 Batch 12:  Training Loss: 0.1105, Validation Loss 0.2787\n",
      "Epoch 96 Batch 13:  Training Loss: 0.1238, Validation Loss 0.2790\n",
      "Epoch 96 Batch 14:  Training Loss: 0.1392, Validation Loss 0.2792\n",
      "Epoch 97 Batch 0:  Training Loss: 0.2458, Validation Loss 0.2800\n",
      "Epoch 97 Batch 1:  Training Loss: 0.3375, Validation Loss 0.2795\n",
      "Epoch 97 Batch 2:  Training Loss: 0.1331, Validation Loss 0.2774\n",
      "Epoch 97 Batch 3:  Training Loss: 0.1560, Validation Loss 0.2776\n",
      "Epoch 97 Batch 4:  Training Loss: 0.1890, Validation Loss 0.2772\n",
      "Epoch 97 Batch 5:  Training Loss: 1.0528, Validation Loss 0.2779\n",
      "Epoch 97 Batch 6:  Training Loss: 1.0466, Validation Loss 0.2786\n",
      "Epoch 97 Batch 7:  Training Loss: 0.3117, Validation Loss 0.2790\n",
      "Epoch 97 Batch 8:  Training Loss: 0.3070, Validation Loss 0.2791\n",
      "Epoch 97 Batch 9:  Training Loss: 0.2219, Validation Loss 0.2793\n",
      "Epoch 97 Batch 10:  Training Loss: 0.1404, Validation Loss 0.2796\n",
      "Epoch 97 Batch 11:  Training Loss: 0.1725, Validation Loss 0.2800\n",
      "Epoch 97 Batch 12:  Training Loss: 0.1102, Validation Loss 0.2804\n",
      "Epoch 97 Batch 13:  Training Loss: 0.1214, Validation Loss 0.2808\n",
      "Epoch 97 Batch 14:  Training Loss: 0.1378, Validation Loss 0.2811\n",
      "Epoch 98 Batch 0:  Training Loss: 0.2467, Validation Loss 0.2820\n",
      "Epoch 98 Batch 1:  Training Loss: 0.3365, Validation Loss 0.2815\n",
      "Epoch 98 Batch 2:  Training Loss: 0.1327, Validation Loss 0.2792\n",
      "Epoch 98 Batch 3:  Training Loss: 0.1555, Validation Loss 0.2793\n",
      "Epoch 98 Batch 4:  Training Loss: 0.1897, Validation Loss 0.2788\n",
      "Epoch 98 Batch 5:  Training Loss: 1.0466, Validation Loss 0.2794\n",
      "Epoch 98 Batch 6:  Training Loss: 1.0413, Validation Loss 0.2799\n",
      "Epoch 98 Batch 7:  Training Loss: 0.3115, Validation Loss 0.2803\n",
      "Epoch 98 Batch 8:  Training Loss: 0.3033, Validation Loss 0.2802\n",
      "Epoch 98 Batch 9:  Training Loss: 0.2206, Validation Loss 0.2803\n",
      "Epoch 98 Batch 10:  Training Loss: 0.1387, Validation Loss 0.2806\n",
      "Epoch 98 Batch 11:  Training Loss: 0.1704, Validation Loss 0.2808\n",
      "Epoch 98 Batch 12:  Training Loss: 0.1099, Validation Loss 0.2811\n",
      "Epoch 98 Batch 13:  Training Loss: 0.1197, Validation Loss 0.2814\n",
      "Epoch 98 Batch 14:  Training Loss: 0.1370, Validation Loss 0.2817\n",
      "Epoch 99 Batch 0:  Training Loss: 0.2478, Validation Loss 0.2824\n",
      "Epoch 99 Batch 1:  Training Loss: 0.3350, Validation Loss 0.2819\n",
      "Epoch 99 Batch 2:  Training Loss: 0.1317, Validation Loss 0.2798\n",
      "Epoch 99 Batch 3:  Training Loss: 0.1563, Validation Loss 0.2799\n",
      "Epoch 99 Batch 4:  Training Loss: 0.1900, Validation Loss 0.2794\n",
      "Epoch 99 Batch 5:  Training Loss: 1.0412, Validation Loss 0.2799\n",
      "Epoch 99 Batch 6:  Training Loss: 1.0362, Validation Loss 0.2804\n",
      "Epoch 99 Batch 7:  Training Loss: 0.3091, Validation Loss 0.2808\n",
      "Epoch 99 Batch 8:  Training Loss: 0.2937, Validation Loss 0.2809\n",
      "Epoch 99 Batch 9:  Training Loss: 0.2208, Validation Loss 0.2810\n",
      "Epoch 99 Batch 10:  Training Loss: 0.1384, Validation Loss 0.2812\n",
      "Epoch 99 Batch 11:  Training Loss: 0.1691, Validation Loss 0.2814\n",
      "Epoch 99 Batch 12:  Training Loss: 0.1098, Validation Loss 0.2817\n",
      "Epoch 99 Batch 13:  Training Loss: 0.1177, Validation Loss 0.2820\n",
      "Epoch 99 Batch 14:  Training Loss: 0.1358, Validation Loss 0.2822\n",
      "Training Complete.  Test Set Loss: 0.2301\n"
     ]
    }
   ],
   "source": [
    "dataset = CreateDataSet('SPY')\n",
    "dataset.set_training_dates(start_date=\"2008-01-01\",end_date=\"2010-01-01\")\n",
    "dataset.set_test_dates(start_date=\"2011-01-04\",end_date=\"2011-06-01\")\n",
    "dataset.set_validation_dates(start_date=\"2012-01-04\",end_date=\"2012-06-01\") \n",
    "dataset.set_batch_size(32)\n",
    "dataset.create_dataset()\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.set_learning_rate(.001)\n",
    "nn.set_keep_probability(0.5)\n",
    "nn.set_epochs(100)\n",
    "nn.set_dataset(dataset)\n",
    "\n",
    "nn.add_inputs(number_of_inputs=10)\n",
    "nn.add_hidden_layer(number_of_nodes=20)\n",
    "nn.build_model() \n",
    "\n",
    "nn.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "Define the function to train your model using the training data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,train_data,test_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model on the validation set\n",
    "Return the accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model,validation_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the results\n",
    "Show graphs and metrics of your model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate your model here\n",
    "Call the functions to get the data, train the model, and evaluate it on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
